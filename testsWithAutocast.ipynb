{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Small example to check how autocast operates in torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# small mixed precision test\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.cuda.amp import GradScaler, autocast\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import time\n",
    "# Define a simple model\n",
    "class SimpleCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleCNN, self).__init__()\n",
    "        self.conv = nn.Conv2d(1, 32, 3, 1)\n",
    "        self.fc = nn.Linear(32 * 26 * 26, 10)\n",
    "\n",
    "    def forward(self, x, use_autocast=False):\n",
    "        if use_autocast:\n",
    "            with autocast(dtype=torch.float16):\n",
    "                x = self.conv(x)\n",
    "                x = torch.relu(x)\n",
    "                x = torch.flatten(x, 1)\n",
    "                x = self.fc(x)\n",
    "        else:\n",
    "            x = self.conv(x)\n",
    "            x = torch.relu(x)\n",
    "            x = torch.flatten(x, 1)\n",
    "            x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "# Training function with mixed precision\n",
    "def train(model, device, train_loader, optimizer, criterion, scaler, epoch, use_autocast=False):\n",
    "    model.train()\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = data.to(device).half(), target.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Enables autocasting for the forward pass\n",
    "        if use_autocast:\n",
    "            with autocast(dtype=torch.float16):\n",
    "                output = model(data)\n",
    "                loss = criterion(output, target)\n",
    "            # Scales the loss, and calls backward()\n",
    "            scaler.scale(loss).backward()\n",
    "\n",
    "            # Unscales gradients and calls optimizer.step()\n",
    "            scaler.step(optimizer)\n",
    "\n",
    "            # Updates the scale for next iteration\n",
    "            scaler.update()\n",
    "        else:\n",
    "            output = model(data)\n",
    "            loss = criterion(output.float(), target)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        \n",
    "\n",
    "        if batch_idx % 100 == 0:\n",
    "            print(f'Train Epoch: {epoch} [{batch_idx * len(data)}/'\n",
    "                f'{len(train_loader.dataset)}] Loss: {loss.item():.6f}')\n",
    "            # check precision used with the auto cast\n",
    "            for name, param in model.named_parameters():\n",
    "                print(f'Parameter: {name}, dtype: {param.dtype}')\n",
    "\n",
    "\n",
    "def main():\n",
    "    # Check device\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    # Hyperparameters\n",
    "    batch_size = 64\n",
    "    epochs = 5\n",
    "    learning_rate = 1e-3\n",
    "\n",
    "    # Data loaders\n",
    "    transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.1307,), (0.3081,))\n",
    "    ])\n",
    "\n",
    "    train_dataset = datasets.MNIST('./data', train=True, download=True, transform=transform)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    # Initialize model, loss, optimizer\n",
    "    model = SimpleCNN().to(device)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    # Initialize GradScaler\n",
    "    scaler = GradScaler()\n",
    "\n",
    "    epoch = 1\n",
    "\n",
    "    # Training loop\n",
    "    # benchmark with and without autocast\n",
    "    # print(\"Benchmarking without autocast...\")\n",
    "    # start_time = time.time()\n",
    "    # for epoch in range(1):\n",
    "    #     train(model, device, train_loader, optimizer, criterion, scaler, epoch, use_autocast=False)\n",
    "    # time_without_autocast = time.time() - start_time\n",
    "    # print(f\"Average time per batch without autocast: {time_without_autocast:.6f} seconds\")\n",
    "\n",
    "    print(\"\\nBenchmarking with autocast...\")\n",
    "    start_time = time.time()\n",
    "    for epoch in range(1):\n",
    "        train(model, device, train_loader, optimizer, criterion, scaler, epoch, use_autocast=False)\n",
    "    time_with_autocast = time.time() - start_time\n",
    "    print(f\"Average time per batch with autocast: {time_with_autocast:.6f} seconds\")\n",
    "\n",
    "    # print(\"benchmarking with float16\")\n",
    "    # start_time = time.time()\n",
    "    # for epoch in range(1):\n",
    "    #     train(model.half(), device, train_loader, optimizer, criterion, scaler, epoch, use_autocast=False)\n",
    "    # time_with_autocast = time.time() - start_time\n",
    "    # print(f\"Average time per batch with autocast: {time_with_autocast:.6f} seconds\")\n",
    "\n",
    "    # Compare results\n",
    "    #for epoch in range(1, epochs + 1):\n",
    "    #    train(model, device, train_loader, optimizer, criterion, scaler, epoch, use_autocast=True)\n",
    "\n",
    "main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
