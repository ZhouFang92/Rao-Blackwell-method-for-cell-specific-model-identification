{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filtering for CRN with neural martingales\n",
    "\n",
    "### Chemical Reaction Network setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "leader species:  ['G1']\n",
      "follower species:  ['G0', 'mRNA']\n",
      "follower parameters:  ['k2', 'kp1', 'k1']\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from CRN_Simulation_Inference.RB_method_for_model_identification.RBForModelIdentification import RBForModelIdentification\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt  \n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "from ElenaDataManagement import count_samples_for_supersampling, get_X_Y_sampling_times, sample_trajectory_on_times, CRN_simulations_to_dataloaders, run_SSA_for_filtering\n",
    "\n",
    "sigma = 0.1\n",
    "\n",
    "species_names = ['G0', 'G1', 'mRNA']\n",
    "stoichiometric_matrix = [[-1, 1, 0, 0],\n",
    "                         [1, -1, 0, 0],\n",
    "                         [0, 0, 1,-1]]\n",
    "parameters_names = ['k1','k2', 'kp1']\n",
    "reaction_names = ['G Act. 1', 'G Deg. 1', 'mRNA prod.', 'mRNA deg.']\n",
    "propensities = [\n",
    "    lambda k1, G0: k1*G0,\n",
    "    lambda k2, G1: k2*G1,\n",
    "    lambda kp1, G1: kp1*G1,\n",
    "    lambda mRNA: mRNA\n",
    "]\n",
    "\n",
    "range_of_species = \\\n",
    "    pd.DataFrame([[0, 1], [0, 1], [0, 120] ], index=species_names, columns=['min', 'max'])\n",
    "range_of_parameters= \\\n",
    "    pd.DataFrame([[0, 1], [0, 1], [0, 150]],index=parameters_names,columns=['min', 'max'])\n",
    "discretization_size_parameters = \\\n",
    "    pd.DataFrame([21, 21, 151], index=parameters_names) #index=parameters_names\n",
    "\n",
    "# The observation related information\n",
    "# h_function = [\n",
    "#     lambda Protein: Protein\n",
    "# ]\n",
    "h_function = [\n",
    "    lambda mRNA: mRNA # np.where(mRNA > 5, mRNA, 0)\n",
    "]\n",
    "observation_noise_intensity = [\n",
    "    lambda : sigma\n",
    "]\n",
    "#observation_noise_intensity = {'sigma1': 0.1}\n",
    "\n",
    "\n",
    "# sigma = 0.001\n",
    "\n",
    "# species_names = ['mRNA']\n",
    "# stoichiometric_matrix = [[1, -1]]\n",
    "# parameters_names = ['k','g']\n",
    "# reaction_names = ['mRNA prod.', 'mRNA deg.']\n",
    "# propensities = [\n",
    "#     lambda k: k,\n",
    "#     lambda g, mRNA: g*mRNA,\n",
    "# ]\n",
    "\n",
    "# range_of_species = \\\n",
    "#     pd.DataFrame([[0, 120]], index=species_names, columns=['min', 'max'])\n",
    "# range_of_parameters= \\\n",
    "#     pd.DataFrame([[0, 150], [0, 150]],index=parameters_names,columns=['min', 'max'])\n",
    "# discretization_size_parameters = \\\n",
    "#     pd.DataFrame([100, 100], index=parameters_names) #index=parameters_names\n",
    "\n",
    "# # The observation related information\n",
    "# # h_function = [\n",
    "# #     lambda Protein: Protein\n",
    "# # ]\n",
    "# h_function = [\n",
    "#     lambda mRNA: mRNA # np.where(mRNA > 5, mRNA, 0)\n",
    "# ]\n",
    "# observation_noise_intensity = [\n",
    "#     lambda : sigma\n",
    "# ]\n",
    "# #observation_noise_intensity = {'sigma1': 0.1}\n",
    "\n",
    "maximum_size_of_each_follower_subsystem = 20000 #800 # 1000\n",
    "\n",
    "\n",
    "MI = RBForModelIdentification(\n",
    "    species_names=species_names,\n",
    "    stoichiometric_matrix=stoichiometric_matrix,\n",
    "    parameters_names=parameters_names,\n",
    "    reaction_names=reaction_names,\n",
    "    propensities=propensities,\n",
    "    range_of_species=range_of_species,\n",
    "    range_of_parameters=range_of_parameters,\n",
    "    observation_noise_intensity=observation_noise_intensity,\n",
    "    discretization_size_parameters=discretization_size_parameters,\n",
    "    h_function=h_function,\n",
    "    maximum_size_of_each_follower_subsystem=maximum_size_of_each_follower_subsystem)\n",
    "\n",
    "print('leader species: ', MI.leader_species_time_course_data)\n",
    "print('follower species: ', MI.get_follower_species_time_course_data())\n",
    "print('follower parameters: ', MI.get_follower_parameters_time_course_data())\n",
    "\n",
    "# import pandas as pd\n",
    "# from CRN_Simulation_Inference.RB_method_for_model_identification.RBForModelIdentification import RBForModelIdentification\n",
    "# import numpy as np\n",
    "# import matplotlib.pyplot as plt  \n",
    "# from tqdm import tqdm\n",
    "# import torch\n",
    "# from ElenaDataManagement import count_samples_for_supersampling, get_X_Y_sampling_times, sample_trajectory_on_times, CRN_simulations_to_dataloaders, run_SSA_for_filtering\n",
    "\n",
    "# species_names = ['M', 'P']\n",
    "# stoichiometric_matrix = [[1, 0, -1, 0],\n",
    "#                          [0, 1, 0, -1]]\n",
    "# parameters_names = ['b1','b2', 'd1', 'd2']\n",
    "# reaction_names = ['Birth Mrna', 'Birthe Protein', 'Degrade Mrna', 'Degrade Protein']\n",
    "# propensities = [\n",
    "#     lambda b1: b1,\n",
    "#     lambda b2, M: b2*M,\n",
    "#     lambda d1, M: d1*M,\n",
    "#     lambda d2, P: d2*P\n",
    "# ]\n",
    "\n",
    "# range_of_species = \\\n",
    "#     pd.DataFrame([[0, 100], [0, 100] ], index=species_names, columns=['min', 'max'])\n",
    "# range_of_parameters= \\\n",
    "#     pd.DataFrame([[0, 10], [0, 10], [0, 10], [0, 10]],index=parameters_names,columns=['min', 'max'])\n",
    "# discretization_size_parameters = \\\n",
    "#     pd.DataFrame([21, 21, 21, 21], index=parameters_names) #index=parameters_names\n",
    "\n",
    "# # The observation related information\n",
    "# # h_function = [\n",
    "# #     lambda Protein: Protein\n",
    "# # ]\n",
    "# h_function = [\n",
    "#     lambda P: P # np.where(mRNA > 5, mRNA, 0)\n",
    "# ]\n",
    "# observation_noise_intensity = [\n",
    "#     lambda : 0.1\n",
    "# ]\n",
    "# #observation_noise_intensity = {'sigma1': 0.1}\n",
    "\n",
    "# maximum_size_of_each_follower_subsystem = 20000 #800 # 1000\n",
    "\n",
    "\n",
    "# MI = RBForModelIdentification(\n",
    "#     species_names=species_names,\n",
    "#     stoichiometric_matrix=stoichiometric_matrix,\n",
    "#     parameters_names=parameters_names,\n",
    "#     reaction_names=reaction_names,\n",
    "#     propensities=propensities,\n",
    "#     range_of_species=range_of_species,\n",
    "#     range_of_parameters=range_of_parameters,\n",
    "#     observation_noise_intensity=observation_noise_intensity,\n",
    "#     discretization_size_parameters=discretization_size_parameters,\n",
    "#     h_function=h_function,\n",
    "#     maximum_size_of_each_follower_subsystem=maximum_size_of_each_follower_subsystem)\n",
    "\n",
    "# print('leader species: ', MI.leader_species_time_course_data)\n",
    "# print('follower species: ', MI.get_follower_species_time_course_data())\n",
    "# print('follower parameters: ', MI.get_follower_parameters_time_course_data())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "setup parameters and simulate a single trajectory (just for visualization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAiMAAAINCAYAAADsjH/3AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjguMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/SrBM8AAAACXBIWXMAAA9hAAAPYQGoP6dpAABxiUlEQVR4nO3deVxU1fsH8M8MMAMom6IsiruiICLuWKYmiksqWqbmVpmZoWaWC6VSWqLlWq65gOXPJTO1r5kbiSsugDtqLqioDLjBsCjgzP39gYyOLDI4lwvM5/16zSvn3jPnPjONj8+ce+65MkEQBBARERFJRC51AERERGTaWIwQERGRpFiMEBERkaRYjBAREZGkWIwQERGRpFiMEBERkaRYjBAREZGkWIwQERGRpMylDqA00mq1uHPnDmxsbCCTyaQOh0gUgiAgNTUVrq6ukMv5u6QkMLeQKShObmExko87d+7Azc1N6jCISkR8fDyqV68udRgmgbmFTIkhuYXFSD5sbGwA5HyQtra2EkdDJA61Wg03Nzfd953Ex9xCpqBYuUWQ0P79+4W33npLcHFxEQAIW7Zseelr9u3bJ/j4+AgKhUKoW7euEBoamqfNokWLhJo1awpKpVJo1aqVcOzYMYPiSklJEQAIKSkpBr2OqCwpz99z5hYi6RTney7pieL09HR4e3tj8eLFRWofFxeHHj16oGPHjjh16hTGjRuHjz76CLt27dK12bhxI8aPH4/g4GDExMTA29sb/v7+SEpKEuttEFEpw9xCVMaIWBwZBEX49TJx4kTB09NTb1v//v0Ff39/3fNWrVoJgYGBuucajUZwdXUVQkJCihwLf72QKTCV7zlzC1HJKs73vEzNGYmMjISfn5/eNn9/f4wbNw4AkJWVhejoaAQFBen2y+Vy+Pn5ITIysiRDLRJBEPAoWyN1GFTOWVmY8cqNlygNucXY+YD/36ksKVPFiEqlgpOTk942JycnqNVqPHr0CA8fPoRGo8m3zcWLFwvsNzMzE5mZmbrnarXauIHnQxAEvLMsEtE3Hop+LDJtsdP9Ya0oU3/VS1xpyC2PsjXwmLarwP2GalHTAZs+8WVBQmUCFxcAEBISAjs7O92jJC69e5StYSFCVM5JkVtyRd14yJFXKjPK1M8lZ2dnJCYm6m1LTEyEra0trKysYGZmBjMzs3zbODs7F9hvUFAQxo8fr3uee1lSSYma4gdrhVmJHY9Mi5UFv1svUxpyi5WFGWKn+7/Cu8iRkaVBi+/2vnI/RCWpTBUjvr6+2LFjh962PXv2wNfXFwCgUCjQvHlzhIeHIyAgAEDOiofh4eEYPXp0gf0qlUoolUrR4n4Za4UZh9GJJFQacotMJmMeIJMl6WmatLQ0nDp1CqdOnQKQc3ndqVOncPPmTQA5vyqGDh2qa//JJ5/g2rVrmDhxIi5evIglS5bg999/x+eff65rM378eKxYsQJr1qzBhQsXMGrUKKSnp+ODDz4o0fdGRNJhbiEqWwwqw7OysrB161ZERkZCpVIByBnebNu2LXr37g2FQmHQwaOiotCxY0fd89zhzGHDhiEsLAwJCQm65AEAtWvXxt9//43PP/8cCxcuRPXq1bFy5Ur4+z8b2uzfvz/u3r2LadOmQaVSoWnTpti5c2eeiWdEVDYkJiZi+fLlmDZtWpFfw9xCVLbIBEEQitLwypUr8Pf3x507d9C6dWvdX8DExEQcO3YM1atXxz///IN69eqJGnBJUKvVsLOzQ0pKimhLNmdkPdHNnOfVDiSFkvieG8Pp06fRrFkzaDRlfzImcwuZguJ8z4v8LR01ahS8vLxw8uTJPJ2r1WoMHToUgYGBeisWEhG9zJkzZwrdf+nSpRKKhIikUuRi5PDhwzh+/Hi+VY6trS1mzJiB1q1bGzU4Iir/mjZtCplMhvwGaXO3c60MovKtyMWIvb09rl+/jsaNG+e7//r167C3tzdWXERkIipVqoQffvgBnTp1ynf/+fPn0bNnzxKOiohKUpGLkY8++ghDhw7F1KlT0alTJ705I+Hh4fjuu+8wZswY0QIlovKpefPmuHPnDmrWrJnv/uTk5HxHTYio/ChyMTJ9+nRUqFABP/74I7744gvdsKkgCHB2dsakSZMwceJE0QIlovLpk08+QXp6eoH7a9SogdDQ0BKMiIhKmkHTrCdNmoRJkyYhLi5O79Le2rVrixIcEZV/ffr0KXS/g4MDhg0bVkLREJEUinXNV+3atVmAEBERkVEUuRhJTk7G+vXrMWrUKADAoEGD8OjRI91+MzMzrFixgpNYicggzC1EVOTl4FesWIFDhw7pnv/111+Qy+W6u1GePXsWCxYsECNGIirHmFuIqMjFyB9//JHnHgw//PADQkNDERoaipCQEGzbts3oARJR+cbcQkRFLkauXbsGd3d33XN3d3e9e9F4e3vj8uXLxo2OiMo95hYiKnIxkp6ejpSUFN3zqKgoVK9eXW+/Vqs1bnREVO4xtxBRkYuROnXqICYmpsD9UVFRvMKGiAzG3EJERS5G+vTpgylTpiAxMTHPPpVKheDg4JeuF0BE9CLmFiIq8qW9EydOxObNm1G/fn0MGTIEDRo0AJBzR821a9eiWrVqmDRpkmiBElH5xNxCREUuRmxsbHD48GEEBQVh/fr1SE5OBpBzA7333nsPM2fOhI2NjVhxElE5xdxCRAatwOrg4IBly5Zh6dKluHv3LgCgSpUqvL03Eb0S5hYi01as5eBlMhmqVq1q7FiIyMQxtxCZpiJPYCUiIiISA4sRIiIikhSLESIiIpJUkYqRSpUq4d69ewCADz/8EKmpqaIGRUSmgbmFiIAiFiNZWVlQq9UAgDVr1uDx48eiBkVEpoG5hYiAIl5N4+vri4CAADRv3hyCIGDs2LGwsrLKt+3q1auNGiARlV/MLUQEFLEYWbt2LebPn4+rV69CJpMhJSWFv2CI6JUxtxARAEAwUK1atYR79+4Z+rJCLVq0SKhZs6agVCqFVq1aCceOHSuwbfv27QUAeR7du3fXtRk2bFie/f7+/kWOJyUlRQAgpKSkvNL7Kkx6ZrZQc9J2oeak7UJ6ZrZoxyEqSEl8zw3B3GIczC0kteJ8zw1e9CwuLu7Vqp8XbNy4EePHj8eyZcvQunVrLFiwAP7+/rh06VK+ix/9+eefyMrK0j2/f/8+vL290a9fP712Xbt2RWhoqO65Uqk0atxEZFzMLUSmq1iX9u7fvx89e/ZEvXr1UK9ePfTq1QsHDx4sVgDz5s3DiBEj8MEHH8DDwwPLli2DtbV1geeHK1WqBGdnZ91jz549sLa2zpMwlEqlXjsHB4dixUdEJYe5hcg0GVyMrF27Fn5+frC2tsbYsWN1E846deqEdevWGdRXVlYWoqOj4efn9ywguRx+fn6IjIwsUh+rVq3CgAEDUKFCBb3tERERqFq1Ktzd3TFq1Cjcv3+/wD4yMzOhVqv1HkRUsphbiEyYoeeCGjZsKMybNy/P9rlz5woNGzY0qK/bt28LAIQjR47obZ8wYYLQqlWrl77+2LFjAoA854HXr18vbNu2TThz5oywZcsWoVGjRkLLli2FJ0+e5NtPcHBwvueKeV6XyrPSNmeEucU4mFtIasXJLQaPjFy7dg09e/bMs71Xr15GP+f7MqtWrYKXlxdatWqlt33AgAHo1asXvLy8EBAQgO3bt+PEiROIiIjIt5+goCCkpKToHvHx8SUQPRE9j7mFyHQZXIy4ubkhPDw8z/a9e/fCzc3NoL4cHR1hZmaGxMREve2JiYlwdnYu9LXp6enYsGEDhg8f/tLj1KlTB46Ojrhy5Uq++5VKJWxtbfUeRFSymFuITJfBV9N88cUXGDt2LE6dOoW2bdsCAA4fPoywsDAsXLjQoL4UCgWaN2+O8PBwBAQEAAC0Wi3Cw8MxevToQl+7adMmZGZmYvDgwS89zq1bt3D//n24uLgYFB8RlRzmFiITVpzzQX/++afw2muvCZUqVRIqVaokvPbaa8LWrVuL05WwYcMGQalUCmFhYUJsbKzw8ccfC/b29oJKpRIEQRCGDBkiTJ48Oc/rXn/9daF///55tqempgpffvmlEBkZKcTFxQl79+4VmjVrJtSvX194/PhxkWLiWgBkCkrbnBFBYG4xBuYWklqJrDMCAH369EGfPn2MUgz1798fd+/exbRp06BSqdC0aVPs3LkTTk5OAICbN29CLtc/m3Tp0iUcOnQIu3fvztOfmZkZzpw5gzVr1iA5ORmurq7o0qULZsyYwfUAiEo55hYi0yQTBEGQOojSRq1Ww87ODikpKaKd483IegKPabsAALHT/WGtKFZdSFRsJfE9J33MLWQKivM9L9aiZ0RERETGwmKEiIiIJMVihIiIiCRlUDGSnZ2NunXr4sKFC2LFQ0QmiLmFyLQZVIxYWFjg8ePHYsVCRCaKuYXItBl8miYwMBCzZ8/GkydPxIiHiEwUcwuR6TL4mq8TJ04gPDwcu3fvhpeXV547Wv75559GC46ITAdzC5HpMrgYsbe3x9tvvy1GLERkwphbiEyXwcVIaGioGHEQkYljbiEyXcW6tPfJkyfYu3cvli9fjtTUVADAnTt3kJaWZtTgiMi0MLcQmSaDR0Zu3LiBrl274ubNm8jMzETnzp1hY2OD2bNnIzMzE8uWLRMjTiIq55hbiEyXwSMjn332GVq0aIGHDx/CyspKt71Pnz4IDw83anBEZDqYW4hMl8EjIwcPHsSRI0egUCj0tteqVQu3b982WmBEZFqYW4hMl8EjI1qtFhqNJs/2W7duwcbGxihBEZHpYW4hMl0GFyNdunTBggULdM9lMhnS0tIQHByM7t27GzM2IjIhzC1Epsvg0zRz586Fv78/PDw88PjxY7z33nu4fPkyHB0dsX79ejFiJCITwNxCZLoMLkaqV6+O06dPY8OGDThz5gzS0tIwfPhwDBo0SG/SGRGRIZhbiEyXwcUIAJibm2Pw4MHGjoWITBxzC5FpKlYxcunSJfz888+62303atQIo0ePRsOGDY0aHBGZFuYWItNk8ATWzZs3o3HjxoiOjoa3tze8vb0RExMDLy8vbN68WYwYicgEMLcQmS6DR0YmTpyIoKAgTJ8+XW97cHAwJk6cyBtdEVGxMLcQmS6DR0YSEhIwdOjQPNsHDx6MhIQEowRFRKaHuYXIdBlcjHTo0AEHDx7Ms/3QoUNo166dUYIiItPD3EJkuopUjPz111+6R69evTBp0iSMHj0aa9euxdq1azF69GhMnjwZffr0KVYQixcvRq1atWBpaYnWrVvj+PHjBbYNCwuDTCbTe1haWuq1EQQB06ZNg4uLC6ysrODn54fLly8XKzYiEg9zCxEBAIQikMlkRXrI5fKidKdnw4YNgkKhEFavXi2cP39eGDFihGBvby8kJibm2z40NFSwtbUVEhISdA+VSqXXZtasWYKdnZ2wdetW4fTp00KvXr2E2rVrC48ePSpSTCkpKQIAISUlxeD3U1TpmdlCzUnbhZqTtgvpmdmiHYeoICXxPX8Z5hbjY24hqRXne16kYkRMrVq1EgIDA3XPNRqN4OrqKoSEhOTbPjQ0VLCzsyuwP61WKzg7Ows//vijbltycrKgVCqF9evXFykmJgwyBaWhGBETcwtzC0mjON9zg+eMGFNWVhaio6Ph5+en2yaXy+Hn54fIyMgCX5eWloaaNWvCzc0NvXv3xvnz53X74uLioFKp9Pq0s7ND69atC+2TiMoP5haisqVYi56dOHEC+/btQ1JSErRard6+efPmFbmfe/fuQaPRwMnJSW+7k5MTLl68mO9r3N3dsXr1ajRp0gQpKSmYM2cO2rZti/Pnz6N69epQqVS6Pl7sM3ffizIzM5GZmal7rlari/weiMh4mFuITJPBxcjMmTMxZcoUuLu7w8nJCTKZTLfv+T+LxdfXF76+vrrnbdu2RaNGjbB8+XLMmDGjWH2GhITg22+/NVaIRFQMzC1EpsvgYmThwoVYvXo13n///Vc+uKOjI8zMzJCYmKi3PTExEc7OzkXqw8LCAj4+Prhy5QoA6F6XmJgIFxcXvT6bNm2abx9BQUEYP3687rlarYabm5shb4WIXhFzC5HpMnjOiFwux2uvvWaUgysUCjRv3hzh4eG6bVqtFuHh4Xq/UAqj0Whw9uxZXXKoXbs2nJ2d9fpUq9U4duxYgX0qlUrY2trqPYioZDG3EJkug4uRzz//HIsXLzZaAOPHj8eKFSuwZs0aXLhwAaNGjUJ6ejo++OADAMDQoUMRFBSkaz99+nTs3r0b165dQ0xMDAYPHowbN27go48+ApAznDtu3Dh89913+Ouvv3D27FkMHToUrq6uCAgIMFrcRGRczC1Epsvg0zRffvklevTogbp168LDwwMWFhZ6+//880+D+uvfvz/u3r2LadOmQaVSoWnTpti5c6duktjNmzchlz+rmR4+fIgRI0ZApVLBwcEBzZs3x5EjR+Dh4aFrM3HiRKSnp+Pjjz9GcnIyXn/9dezcuTPPAkZEVHowtxCZLpkgCIIhLxg9ejRWrlyJjh075plkBgChoaFGDVAKarUadnZ2SElJEW1YNSPrCTym7QIAxE73h7WiWBc2ERVbSXzPDcHcYhzMLSS14nzPDf6WrlmzBps3b0aPHj0MDpCIqCDMLUSmy+A5I5UqVULdunXFiIWITBhzC5HpMrgY+eabbxAcHIyMjAwx4iEiE8XcQmS6DD5N89NPP+Hq1atwcnJCrVq18kwyi4mJMVpwRGQ6mFuITJfBxQgvYSMiMTC3EJkug4uR4OBgMeIgIhPH3EJkuiS9ay8RERGRwSMjcrm80JtWaTSaVwqIiEwTcwuR6TK4GNmyZYve8+zsbJw8eRJr1qzh3SmJqNiYW4hMl8HFSO/evfNse+edd+Dp6YmNGzdi+PDhRgmMiEwLcwuR6TLanJE2bdro3c2SiMgYmFuIyj+jFCOPHj3CTz/9hGrVqhmjOyIiAMwtRKbC4NM0Dg4OepPMBEFAamoqrK2tsXbtWqMGR0Smg7mFyHQZXIwsWLBA77lcLkeVKlXQunVrODg4GCsuIjIxzC1EpsvgYmTYsGFixEFEJo65hch0GVyMAEBycjKOHz+OpKQkaLVavX1Dhw41SmBEZHqYW4hMk8HFyP/+9z8MGjQIaWlpsLW11TvHK5PJmDCIqFiYW4hMl8FX03zxxRf48MMPkZaWhuTkZDx8+FD3ePDggRgxEpEJYG4hMl0GFyO3b9/G2LFjYW1tLUY8RGSimFuITJfBxYi/vz+ioqLEiIWITBhzC5HpMnjOSI8ePTBhwgTExsbCy8sLFhYWevt79epltOCIyHQwtxCZLoOLkREjRgAApk+fnmefTCbjnTWJqFiYW4hMl8HFyIuX2xERGQNzC5HpMtqN8oiIiIiKg8UIERERSapUFCOLFy9GrVq1YGlpidatW+P48eMFtl2xYgXatWsHBwcHODg4wM/PL0/7999/HzKZTO/RtWtXsd8GEZUyzC1EZYPkxcjGjRsxfvx4BAcHIyYmBt7e3vD390dSUlK+7SMiIjBw4EDs27cPkZGRcHNzQ5cuXXD79m29dl27dkVCQoLusX79+pJ4O0RUSjC3EJUdkhcj8+bNw4gRI/DBBx/Aw8MDy5Ytg7W1NVavXp1v+//7v//Dp59+iqZNm6Jhw4ZYuXIltFotwsPD9doplUo4OzvrHrzrJ5FpYW4hKjsMLkbat2+PX3/9FY8ePXrlg2dlZSE6Ohp+fn7PApLL4efnh8jIyCL1kZGRgezsbFSqVElve0REBKpWrQp3d3eMGjUK9+/fL7CPzMxMqNVqvQcRlSzmFiLTZXAx4uPjgy+//BLOzs4YMWIEjh49WuyD37t3DxqNBk5OTnrbnZycoFKpitTHpEmT4Orqqpd0unbtil9//RXh4eGYPXs29u/fj27duhW4TkFISAjs7Ox0Dzc3t2K/JyIqHuYWItNlcDGyYMEC3LlzB6GhoUhKSsIbb7wBDw8PzJkzB4mJiWLEWKBZs2Zhw4YN2LJlCywtLXXbBwwYgF69esHLywsBAQHYvn07Tpw4gYiIiHz7CQoKQkpKiu4RHx9fQu+AiHIxtxCZrmLNGTE3N0ffvn2xbds23Lp1C++99x6mTp0KNzc3BAQE4N9//y1SP46OjjAzM8uTaBITE+Hs7Fzoa+fMmYNZs2Zh9+7daNKkSaFt69SpA0dHR1y5ciXf/UqlEra2tnoPIip5zC1EpumVJrAeP34cwcHBmDt3LqpWrYqgoCA4OjrirbfewpdffvnS1ysUCjRv3lxvgljuhDFfX98CX/fDDz9gxowZ2LlzJ1q0aPHS49y6dQv379+Hi4tL0d4YEUmKuYXIxAgGSkxMFObMmSN4enoKCoVCePvtt4V//vlH0Gq1ujYHDx4UKlSoUKT+NmzYICiVSiEsLEyIjY0VPv74Y8He3l5QqVSCIAjCkCFDhMmTJ+vaz5o1S1AoFMIff/whJCQk6B6pqamCIAhCamqq8OWXXwqRkZFCXFycsHfvXqFZs2ZC/fr1hcePHxcpppSUFAGAkJKSUtSPxWDpmdlCzUnbhZqTtgvpmdmiHYeoICXxPTcEc4txMLeQ1IrzPTf43jTVq1dH3bp18eGHH+L9999HlSpV8rRp0qQJWrZsWaT++vfvj7t372LatGlQqVRo2rQpdu7cqZt4dvPmTcjlzwZwli5diqysLLzzzjt6/QQHB+Obb76BmZkZzpw5gzVr1iA5ORmurq7o0qULZsyYAaVSaejbJaISwtxCZLpkgiAIhrzg4MGDaNeunVjxlApqtRp2dnZISUkR7RxvRtYTeEzbBQCIne4Pa4XBdSHRKymJ77khmFuMg7mFpFac77nB39LcZJGUlIRLly4BANzd3VG1alVDuyIi0mFuITJdBk9gTU1NxZAhQ1CtWjW0b98e7du3R7Vq1TB48GCkpKSIESMRmQDmFiLTZXAx8tFHH+HYsWPYvn07kpOTkZycjO3btyMqKgojR44UI0YiMgHMLUSmy+DTNNu3b8euXbvw+uuv67b5+/tjxYoVvHslERUbcwuR6TJ4ZKRy5cqws7PLs93Ozo43jCKiYmNuITJdBhcjU6ZMwfjx4/Xu76BSqTBhwgRMnTrVqMERkelgbiEyXQafplm6dCmuXLmCGjVqoEaNGgByrtdXKpW4e/culi9frmsbExNjvEiJqFxjbiEyXQYXIwEBASKEQUSmjrmFyHQZXIwEBweLEQcRmTjmFiLTVeyl+aKjo3HhwgUAgKenJ3x8fIwWFBGZLuYWItNjcDGSlJSEAQMGICIiAvb29gCA5ORkdOzYERs2bMj3fhJERC/D3EJkugy+mmbMmDFITU3F+fPn8eDBAzx48ADnzp2DWq3G2LFjxYiRiEwAcwuR6TJ4ZGTnzp3Yu3cvGjVqpNvm4eGBxYsXo0uXLkYNjohMB3MLkekyeGREq9XCwsIiz3YLCwtotVqjBEVEpoe5hch0GVyMvPnmm/jss89w584d3bbbt2/j888/R6dOnYwaHBGZDuYWItNlcDGyaNEiqNVq1KpVC3Xr1kXdunVRu3ZtqNVq/Pzzz2LESEQmgLmFyHQZPGfEzc0NMTEx2Lt3Ly5evAgAaNSoEfz8/IweHBGZDuYWItNVrHVGZDIZOnfujM6dOxs7HiIyYcwtRKapyKdp/v33X3h4eECtVufZl5KSAk9PTxw8eNCowRFR+cfcQkRFLkYWLFiAESNGwNbWNs8+Ozs7jBw5EvPmzTNqcERU/jG3EFGRi5HTp0+ja9euBe7v0qULoqOjjRIUEZkO5hYiKnIxkpiYmO8aALnMzc1x9+5dowRFRKaDuYWIilyMVKtWDefOnStw/5kzZ+Di4mKUoIjIdDC3EFGRi5Hu3btj6tSpePz4cZ59jx49QnBwMN566y2jBkdE5R9zCxEVuRiZMmUKHjx4gAYNGuCHH37Atm3bsG3bNsyePRvu7u548OABvv7662IFsXjxYtSqVQuWlpZo3bo1jh8/Xmj7TZs2oWHDhrC0tISXlxd27Niht18QBEybNg0uLi6wsrKCn58fLl++XKzYiEhczC1EBMEA169fF7p16ybI5XJBJpMJMplMkMvlQrdu3YRr164Z0pXOhg0bBIVCIaxevVo4f/68MGLECMHe3l5ITEzMt/3hw4cFMzMz4YcffhBiY2OFKVOmCBYWFsLZs2d1bWbNmiXY2dkJW7duFU6fPi306tVLqF27tvDo0aMixZSSkiIAEFJSUor1nooiPTNbqDlpu1Bz0nYhPTNbtOMQFaQkvudFxdxiPMwtJLXifM9lgiAIhhYwDx8+xJUrVyAIAurXrw8HB4diF0OtW7dGy5YtsWjRIgA5N8tyc3PDmDFjMHny5Dzt+/fvj/T0dGzfvl23rU2bNmjatCmWLVsGQRDg6uqKL774Al9++SWAnLUKnJycEBYWhgEDBrw0JrVaDTs7O6SkpOS53FAQBDzK1hT7/ebKyNKgxXd7AQCx0/1hrSjW+nNExVbY91wqppxbjCUj6wk8pu0CAERN8YO1wkyU4xBZWZhBJpPl2V6c73mx/gV0cHBAy5Yti/NSPVlZWYiOjkZQUJBum1wuh5+fHyIjI/N9TWRkJMaPH6+3zd/fH1u3bgUAxMXFQaVS6S0hbWdnh9atWyMyMjLfhJGZmYnMzEzd8/wWX8r1KFuj+4tORMZlyrlFDLk/eIjEYMwf0gbfKM+Y7t27B41GAycnJ73tTk5OUKlU+b5GpVIV2j73v4b0GRISAjs7O93Dzc2tWO+nOFrUdICVBX+5EBmTKecWKwsztKhZ/BElIinw3ACAoKAgvV9EarW6wKRhZWGG2On+Rjt2QcNcRFT2GZJbjEUmk2HTJ75GOZ1MVBhj/pCWtBhxdHSEmZkZEhMT9bYnJibC2dk539c4OzsX2j73v4mJiXprEyQmJqJp06b59qlUKqFUKosUs0wm4/wOolKuLOYWY2KeorJG0tM0CoUCzZs3R3h4uG6bVqtFeHg4fH19832Nr6+vXnsA2LNnj6597dq14ezsrNdGrVbj2LFjBfZJROULcwtRGSPGZT2G2LBhg6BUKoWwsDAhNjZW+PjjjwV7e3tBpVIJgiAIQ4YMESZPnqxrf/jwYcHc3FyYM2eOcOHCBSE4ODjfy+/s7e2Fbdu2CWfOnBF69+5d6i6/I5Jaef+eM7cQSaM433PJixFBEISff/5ZqFGjhqBQKIRWrVoJR48e1e1r3769MGzYML32v//+u9CgQQNBoVAInp6ewt9//623X6vVClOnThWcnJwEpVIpdOrUSbh06VKR42HCIFNgCt9z5haikldi64yUdykpKbC3t0d8fHypWX+ByNhyJ1MmJyfDzs5O6nBMAnMLmYLi5BbOcMpHamoqAJToJb5EUklNTWUxUkKYW8iUGJJbODKSD61Wizt37sDGxqbA1eXc3Nz468aI+Jka38s+U0EQkJqaCldXV8jlks5lNxnMLSWPn6nxiZFbODKSD7lcjurVq7+0na2tLb/cRsbP1PgK+0w5IlKymFukw8/U+IyZW/hziIiIiCTFYoSIiIgkxWKkGJRKJYKDgyVZWbG84mdqfPxMyx7+PzM+fqbGJ8ZnygmsREREJCmOjBAREZGkWIwQERGRpFiMEBERkaRYjBAREZGkWIwUYPHixahVqxYsLS3RunVrHD9+vND2mzZtQsOGDWFpaQkvLy/s2LGjhCItOwz5TMPCwiCTyfQelpaWJRht6XbgwAH07NkTrq6ukMlk2Lp160tfExERgWbNmkGpVKJevXoICwsTPU7Ki7nF+JhbjEeq3MJiJB8bN27E+PHjERwcjJiYGHh7e8Pf3x9JSUn5tj9y5AgGDhyI4cOH4+TJkwgICEBAQADOnTtXwpGXXoZ+pkDO6n4JCQm6x40bN0ow4tItPT0d3t7eWLx4cZHax8XFoUePHujYsSNOnTqFcePG4aOPPsKuXbtEjpSex9xifMwtxiVZbhHh7sFlXqtWrYTAwEDdc41GI7i6ugohISH5tn/33XeFHj166G1r3bq1MHLkSFHjLEsM/UxDQ0MFOzu7EoqubAMgbNmypdA2EydOFDw9PfW29e/fX/D39xcxMnoRc4vxMbeIpyRzC0dGXpCVlYXo6Gj4+fnptsnlcvj5+SEyMjLf10RGRuq1BwB/f/8C25ua4nymAJCWloaaNWvCzc0NvXv3xvnz50si3HKJ31HpMbcYH3OL9Iz1HWUx8oJ79+5Bo9HAyclJb7uTkxNUKlW+r1GpVAa1NzXF+Uzd3d2xevVqbNu2DWvXroVWq0Xbtm1x69atkgi53CnoO6pWq/Ho0SOJojItzC3Gx9wiPWPlFt61l0olX19f+Pr66p63bdsWjRo1wvLlyzFjxgwJIyOisoy5pXTiyMgLHB0dYWZmhsTERL3tiYmJcHZ2zvc1zs7OBrU3NcX5TF9kYWEBHx8fXLlyRYwQy72CvqO2trawsrKSKCrTwtxifMwt0jNWbmEx8gKFQoHmzZsjPDxct02r1SI8PFyvmn6er6+vXnsA2LNnT4HtTU1xPtMXaTQanD17Fi4uLmKFWa7xOyo95hbjY26RntG+o4bOrjUFGzZsEJRKpRAWFibExsYKH3/8sWBvby+oVCpBEARhyJAhwuTJk3XtDx8+LJibmwtz5swRLly4IAQHBwsWFhbC2bNnpXoLpY6hn+m3334r7Nq1S7h69aoQHR0tDBgwQLC0tBTOnz8v1VsoVVJTU4WTJ08KJ0+eFAAI8+bNE06ePCncuHFDEARBmDx5sjBkyBBd+2vXrgnW1tbChAkThAsXLgiLFy8WzMzMhJ07d0r1FkwSc4vxMbcYl1S5hcVIAX7++WehRo0agkKhEFq1aiUcPXpUt699+/bCsGHD9Nr//vvvQoMGDQSFQiF4enoKf//9dwlHXPoZ8pmOGzdO19bJyUno3r27EBMTI0HUpdO+ffsEAHkeuZ/hsGHDhPbt2+d5TdOmTQWFQiHUqVNHCA0NLfG4iblFDMwtxiNVbpEJgiC84igNERERUbFxzggRERFJisUIERERSYrFCBEREUmKxQgRERFJisUIERERSYrFCBEREUmKxQgRERFJisUIlZj3338fAQEBUodBROUMc0vZx7v2klHIZLJC9wcHB2PhwoXgGntEZAjmFtPAFVjJKFQqle7PGzduxLRp03Dp0iXdtooVK6JixYpShEZEZRhzi2ngaRoyCmdnZ93Dzs4OMplMb1vFihXzDKV26NABY8aMwbhx4+Dg4AAnJyesWLEC6enp+OCDD2BjY4N69erhn3/+0TvWuXPn0K1bN1SsWBFOTk4YMmQI7t27V8LvmIhKAnOLaWAxQpJas2YNHB0dcfz4cYwZMwajRo1Cv3790LZtW8TExKBLly4YMmQIMjIyAADJycl488034ePjg6ioKOzcuROJiYl49913JX4nRFSaMLeUMa9ydz+i/ISGhgp2dnZ5tg8bNkzo3bu37nn79u2F119/Xff8yZMnQoUKFfRuT52QkCAAECIjIwVBEIQZM2YIXbp00es3Pj5eACBcunTJuG+EiEoV5pbyixNYSVJNmjTR/dnMzAyVK1eGl5eXbpuTkxMAICkpCQBw+vRp7Nu3L99zxFevXkWDBg1EjpiIygLmlrKFxQhJysLCQu+5TCbT25Y7k16r1QIA0tLS0LNnT8yePTtPXy4uLiJGSkRlCXNL2cJihMqUZs2aYfPmzahVqxbMzfn1JSLjYG6RFiewUpkSGBiIBw8eYODAgThx4gSuXr2KXbt24YMPPoBGo5E6PCIqo5hbpMVihMoUV1dXHD58GBqNBl26dIGXlxfGjRsHe3t7yOX8OhNR8TC3SIuLnhEREZGkWO4RERGRpFiMEBERkaRYjBAREZGkWIwQERGRpFiMEBERkaRYjBAREZGkWIwQERGRpFiMEBERkaRYjBAREZGkWIwQERGRpFiMEBERkaRYjBAREZGkWIwQERGRpFiMEBERkaRYjBAREZGkWIwQERGRpFiMEBERkaRYjBAREZGkWIwQERGRpFiMEBERkaRYjBAREZGkWIwQERGRpFiMEBERkaRYjBAREZGkWIwQERGRpFiMEBERkaRYjBAREZGkWIwQERGRpFiMEBERkaRYjBAREZGkWIwQERGRpFiMEBERkaRYjBAREZGkWIwQERGRpFiMEBERkaRYjBAREZGkWIwQERGRpFiMEBERkaRYjBAREZGkWIwQERGRpFiMEBERkaRYjBAREZGkWIwQERGRpFiMEBERkaRYjBAREZGkWIwQERGRpFiMEBERkaRYjBAREZGkWIwQERGRpFiMEBERkaRYjBAREZGkWIwQERGRpFiMEBERkaRYjBAREZGkWIwQERGRpFiMEBERkaRYjBAREZGkWIwQERGRpFiMEBERkaRYjBAREZGkWIwQERGRpFiMEBERkaRYjBAREZGkWIwQERGRpFiMEBERkaRYjBAREZGkWIwQERGRpFiMEBERkaRYjBAREZGkWIwQERGRpFiMEBERkaRYjBAREZGkWIwQERGRpFiMEBERkaRYjBAREZGkWIwQERGRpFiMEBERkaRYjBAREZGkWIwQERGRpFiMEBERkaRYjBAREZGkWIwQERGRpFiMEBERkaRYjBAREZGkWIwQERGRpFiMEBERkaRYjBAREZGkWIwQERGRpFiMEBERkaRYjBAREZGkWIwQERGRpFiMEBERkaRYjBAREZGkWIwQERGRpFiMEBERkaRYjBAREZGkWIwQERGRpFiMEBERkaRYjBAREZGkWIwQERGRpFiMEBERkaRYjBAREZGkWIwQERGRpFiMEBERkaRYjBAREZGkWIwQERGRpFiMEBERkaRYjBAREZGkWIwQERGRpFiMEBERkaRYjBAREZGkWIwQERGRpMxL6kBqtRr/93//h1WrViEqKqqkDlssWq0Wd+7cgY2NDWQymdThEIlCEASkpqbC1dUVcjl/l5QE5hYyBcXKLYLI/v33X2Hw4MGCtbW14OLiInz66adiH/KVxcfHCwD44MMkHvHx8VL/lStxS5YsEby8vAQbGxvBxsZGaNOmjbBjx45CX/P7778L7u7uglKpFBo3biz8/fffBh+XuYUPU3oYkltEGRm5ffs2wsLCEBoaiuTkZDx8+BDr1q3Du+++WyZ+DdjY2AAA4uPjYWtrK3E0ROJQq9Vwc3PTfd9NSfXq1TFr1izUr18fgiBgzZo16N27N06ePAlPT8887Y8cOYKBAwciJCQEb731FtatW4eAgADExMSgcePGRT4ucwuZgmLlFoNL+0L88ccfQrdu3YQKFSoI77zzjrB161YhMzNTMDc3F86fP29wf1L9eklJSREACCkpKQa/lqis4Pdcn4ODg7By5cp897377rtCjx499La1bt1aGDlypEHH4GdOpqA433Ojniju378/fHx8kJCQgE2bNqF3795QKBTF7i/310t0dDSioqLw5ptvonfv3jh//ny+7XN/vQwfPhwnT55EQEAAAgICcO7cuWLHQETlm0ajwYYNG5Ceng5fX99820RGRsLPz09vm7+/PyIjIwvtOzMzE2q1Wu9BRHkZtRgZPnw4Fi9ejK5du2LZsmV4+PDhK/XXs2dPdO/eHfXr10eDBg3w/fffo2LFijh69Gi+7RcuXIiuXbtiwoQJaNSoEWbMmIFmzZph0aJFrxQHEZU/Z8+eRcWKFaFUKvHJJ59gy5Yt8PDwyLetSqWCk5OT3jYnJyeoVKpCjxESEgI7Ozvdw83NzWjxE5UnRi1Gli9fjoSEBHz88cdYv349XFxc0Lt3bwiCAK1W+0p9i/nrxRCCICAj64nuIQiC0fomopLj7u6OU6dO4dixYxg1ahSGDRuG2NhYox4jKCgIKSkpukd8fHyh7S+pUvHP2QRcUqUaNQ6i0s7oE1itrKwwbNgwDBs2DJcvX0ZoaCiioqLw2muvoUePHnjnnXfQt2/fIvd39uxZ+Pr64vHjx6hYsaIov14yMzORmZmpe17YUOqjbA08pu3SPW9R0wGbPvEtExNziegZhUKBevXqAQCaN2+OEydOYOHChVi+fHmets7OzkhMTNTblpiYCGdn50KPoVQqoVQqixzTumM3sCbyBsa8WQ/uzu5Ffh1RWSfq4gL169fHzJkzER8fj7Vr1yIjIwMDBw40qI+S+PXyKkOpUTce4lG2xqjxEFHJ02q1ej9Knufr64vw8HC9bXv27ClwlLa4Kihzfh+mZT4xar9EpV2JrHQkl8vRs2dPbN269aXDlC/K/fXSvHlzhISEwNvbGwsXLsy3bXF/vRgylGplYYbY6f6ImuJXYBsiKt2CgoJw4MABXL9+HWfPnkVQUBAiIiIwaNAgAMDQoUMRFBSka//ZZ59h586dmDt3Li5evIhvvvkGUVFRGD16tFHjyi1G0lmMkIkx6mmaAwcOFKld1apVi32Movx6GTdunG5bUX69GDKUKpPJYK0osYVriUgESUlJGDp0KBISEmBnZ4cmTZpg165d6Ny5MwDg5s2beitHtm3bFuvWrcOUKVPw1VdfoX79+ti6datBa4wURQWFGQAgPYujrWRajPqvaocOHQrclzunQiaT4cmTolX9QUFB6NatG2rUqIHU1FSsW7cOERER2LUrZ87G0KFDUa1aNYSEhADI+fXSvn17zJ07Fz169MCGDRsQFRWFX3755dXeGBGVK6tWrSp0f0RERJ5t/fr1Q79+/USKKAdHRshUGbUYKehS3oyMDCxcuBA//fQT6tSpU+T+SuuvFyIiMVRkMUImyqjFiJ2dnd5zrVaL1atX49tvv4VcLsfixYsxbNiwIvdXWn+9EBGJ4dkEVp6mIdMi2uSHP//8E1999RXu3r2LoKAgjBkzxqBL3IiITA1P05CpMvrVNPv370ebNm0wZMgQ9O3bF9euXcOXX37JQoSI6CUqKHMmsGZksRgh02LUkZHu3btj7969+PDDD7F169aXXlJLRETPVFBwnREyTUYdGdm5cycAYOPGjfDw8EClSpXyfRARFUdMTAzOnj2re75t2zYEBATgq6++QlZWloSRGUfuBNbH2Vo80bzaLTSIyhKjjoyEhoYaszsiIj0jR47E5MmT4eXlhWvXrmHAgAHo06cPNm3ahIyMDCxYsEDqEF9J7pwRIGetETurElmXkkhyRi1GDLlShojIUP/99x+aNm0KANi0aRPeeOMNrFu3DocPH8aAAQPKfDGiMJdDYSZHlkaL9MwnsLOykDokohJRomV3QkKC0ZdPJiLT8fwdwPfu3Yvu3bsDANzc3HDv3j0pQzMa66eTWHlFDZkSoxcj58+fx6JFi/DLL78gOTkZAHDv3j18/vnnqFOnDvbt22fsQxKRiWjRogW+++47/Pbbb9i/fz969OgBAIiLi8tzx+6yKncSK5eEJ1Ni1GLkr7/+go+PD8aOHYtPPvkELVq0wL59+9CoUSNcuHABW7Zswfnz5415SCIyIQsWLEBMTAxGjx6Nr7/+GvXq1QMA/PHHH2jbtq3E0RkHV2ElU2TUOSPfffcdAgMDMWPGDKxcuRLjx4/H2LFjsWPHDrRs2dKYhyIiE9SkSRO9q2ly/fjjjzAzM5MgIuPLXWuEl/eSKTHqyMilS5cQGBiIihUrYsyYMZDL5Zg/fz4LESIyivj4eNy6dUv3/Pjx4xg3bhx+/fVXWFiUj8meXIWVTJFRi5HU1FTY2toCAMzMzGBlZWXQjfGIiArz3nvv6eadqVQqdO7cGcePH8fXX3+N6dOnSxydcejmjLAYIRNi9HvT7Nq1S3fDPK1Wi/DwcJw7d06vTa9evYx9WCIyAefOnUOrVq0AAL///jsaN26Mw4cPY/fu3fjkk08wbdo0iSN8dbqREU5gJRNi9GLkxbVGRo4cqfdcJpNBo+FfMiIyXHZ2tu4+V3v37tX9sGnYsCESEhKkDM1oKvLSXjJBRj1No9VqX/pgIUJExeXp6Ylly5bh4MGD2LNnD7p27QoAuHPnDipXrixxdMaROzLCCaxkSrjWMBGVGbNnz8by5cvRoUMHDBw4EN7e3gBylhXIPX1T1nECK5kio5+myXXnzh0cOnQISUlJuhUTc40dO1aswxJROdahQwfcu3cParUaDg4Ouu0ff/wxrK2tJYzMeCoock/TcBSZTIcoxUhYWBhGjhwJhUKBypUrQyaT6fbJZDIWI0RUbIIgIDo6GlevXsV7770HGxsbKBSK8lOM6CawcmSETIcoxcjUqVMxbdo0BAUFQS7nmSAiMo4bN26ga9euuHnzJjIzM9G5c2fY2Nhg9uzZyMzMxLJly6QO8ZVxBVYyRaJUChkZGRgwYAALESIyqs8++wwtWrTAw4cPYWVlpdvep08fhIeHF7mfkJAQtGzZEjY2NqhatSoCAgJw6dKlQl8TFhYGmUym97C0tCz2eynIswmsPE1DpkOUamH48OHYtGmTGF0TkQk7ePAgpkyZAoVCobe9Vq1auH37dpH72b9/PwIDA3H06FHs2bMH2dnZ6NKlC9LT0wt9na2tLRISEnSPGzduFOt9FIYTWMkUiXKaJiQkBG+99RZ27twJLy+vPMs0z5s3T4zDElE5V9DyALdu3YKNjU2R+9m5c6fe87CwMFStWhXR0dF44403CnydTCaDs7Nz0QMuhgpcZ4RMkGjFyK5du+Du7g4AeSawEhEVR5cuXbBgwQL88ssvAHLySVpaGoKDg9G9e/di95uSkgIAqFSpUqHt0tLSULNmTWi1WjRr1gwzZ86Ep6dnge0zMzORmZmpe65Wq18ai245eE5gJRMiSjEyd+5crF69Gu+//74Y3RORiZo7dy78/f3h4eGBx48f47333sPly5fh6OiI9evXF6tPrVaLcePG4bXXXkPjxo0LbOfu7o7Vq1ejSZMmSElJwZw5c9C2bVucP38e1atXz/c1ISEh+Pbbbw2KJ3cC6+NsLZ5otDA349w7Kv9EKUaUSiVee+01MbomIhNWvXp1nD59Ghs3bsTp06eRlpaG4cOHY9CgQXoTWg0RGBiIc+fO4dChQ4W28/X1ha+vr+5527Zt0ahRIyxfvhwzZszI9zVBQUEYP3687rlarYabm1uhx8mdMwLk3J/GzorFCJV/onzLP/vsM/z888+v3E9pnvFORNIwNzfHoEGD8MMPP2DJkiX46KOPil2IjB49Gtu3b8e+ffsKHN0oiIWFBXx8fHDlypUC2yiVStja2uo9XkZhLofi6WgI542QqRBlZOT48eP4999/sX37dnh6euaZwPrnn38WqZ/cGe8tW7bEkydP8NVXX6FLly6IjY1FhQoVCnydra2tXtHCeSpE5UNISAicnJzw4Ycf6m1fvXo17t69i0mTJhWpH0EQMGbMGGzZsgURERGoXbu2wbFoNBqcPXv2leaqFMRaaYasDC2LETIZohQj9vb26Nu37yv3U5pnvBNRyVu+fDnWrVuXZ7unpycGDBhQ5GIkMDAQ69atw7Zt22BjYwOVSgUAsLOz042yDB06FNWqVUNISAgAYPr06WjTpg3q1auH5ORk/Pjjj7hx4wY++ugjI727ZyoozJGckY30LK41QqZBlGIkNDRUjG5L1Yx3Iip5KpUKLi4uebZXqVIFCQkJRe5n6dKlAHLudfO80NBQ3cT7mzdv6i3c+PDhQ4wYMQIqlQoODg5o3rw5jhw5Ag8PD8PfyEtwFVYyNaLdKM/YStuMdyIqeW5ubjh8+HCe0yqHDx+Gq6trkfsRBOGlbSIiIvSez58/H/Pnzy/yMV5F7lojaSxGyESUmWKktM14J6KSN2LECIwbNw7Z2dl48803AQDh4eGYOHEivvjiC4mjMx6uwkqmpkwUI7kz3g8cOCDajHelUvmqYRKRyCZMmID79+/j008/RVZWFgDA0tISkyZNQlBQkMTRGY9u4TMWI2QiSnUxUtpnvBNRyZLJZJg9ezamTp2KCxcuwMrKCvXr1y93PyZ0IyOcwEomwmjrjFSqVAn37t0DAHz44YdITU195T4DAwOxdu1arFu3TjfjXaVS4dGjR7o2Q4cO1ftFNH36dOzevRvXrl1DTEwMBg8eLNqMdyKSRsWKFeHi4gJ7e/tyV4gAQEXen4ZMjNGKkaysLN1VKGvWrMHjx49fuc+lS5ciJSUFHTp0gIuLi+6xceNGXZubN2/qzaLPnfHeqFEjdO/eHWq1WrQZ70RUsrRaLaZPnw47OzvUrFkTNWvWhL29PWbMmAGtVit1eEaTOzLCCaxkKox2msbX1xcBAQFo3rw5BEHA2LFjC1wVcfXq1UXqs7TPeCeikvX1119j1apVmDVrlu6WE4cOHcI333yDx48f4/vvv5c4QuPgBFYyNUYrRtauXYv58+fj6tWrkMlkSElJMcroCBFRrjVr1mDlypXo1auXbluTJk1QrVo1fPrpp+WmGHm2zgjnjJBpMFox4uTkhFmzZgEAateujd9++w2VK1c2VvdERHjw4AEaNmyYZ3vDhg3x4MEDCSISh7Xi6ZyRLI6MkGkQ5UZ5cXFxLESIyOi8vb2xaNGiPNsXLVoEb29vCSISB1dgJVMj2qW9+/fvx5w5c3DhwgUAgIeHByZMmIB27dqJdUgiKud++OEH9OjRA3v37tUtbhgZGYn4+Hjs2LFD4uiM59kEVp6mIdMgysjI2rVr4efnB2tra4wdO1Y3mbVTp0753uSKiKgo2rdvj//++w99+vRBcnIykpOT0bdvX1y6dKlc/dDhBFYyNaKMjHz//ff44Ycf8Pnnn+u2jR07FvPmzcOMGTPw3nvviXFYIirHsrOz0bVrVyxbtqzcTFQtCE/TkKkRZWTk2rVr6NmzZ57tvXr1QlxcnBiHJKJyzsLCAmfOnJE6jBLBCaxkakQpRtzc3BAeHp5n+969e3kDOiIqtsGDB2PVqlVShyG63JGRx9laPNGUn8XciAoiymmaL774AmPHjsWpU6fQtm1bADm3+A4LC8PChQvFOCQRmYAnT55g9erV2Lt3L5o3b44KFSro7Z83b55EkRlX7pwRIOf+NHZWovxuJCo1RClGRo0aBWdnZ8ydOxe///47AKBRo0bYuHEjevfuLcYhicgEnDt3Ds2aNQMA/Pfff3r7ZDKZFCGJQmEuh8JMjiyNFumZT2BnZSF1SESiEu3S3j59+qBPnz5idU9EJmjfvn1Sh1BiKijNkJWh5SRWMgkc+yOiMik+Ph7x8fFShyEaa8XTK2qyStdaI+mZT9Bl/n58/GuU1KFQOcJihIjKjCdPnmDq1Kmws7NDrVq1UKtWLdjZ2WHKlCnIzs6WOjyjKq2X9x65eh//JaZhd2wi/ktMlTocKidEO01DRGRsY8aMwZ9//okffvhBbwXWb775Bvfv38fSpUsljtB4KihzLu9NK2XFyNFr93V//uvUHXzp7y5hNFResBghojJj3bp12LBhA7p166bb1qRJE7i5uWHgwIHlrBgpeGTk1sMMDFt9HC1qVsLsd5rk2X/zfgac7JRQmpsZPa5jcc8VI6fv4IsuDcrV5GGShtFP02RnZ6Nu3bq6e9IQERmLUqlErVq18myvXbs2FApFkfsJCQlBy5YtYWNjg6pVqyIgIACXLl166es2bdqEhg0bwtLSEl5eXqLeD6eg0zQarYAvfj+Nq3fTsTEqHmdvpejtP3zlHtrP2Yc+i48gJcO4p65SHmXj/B01gJwrfm4+yMDpF47/IkEQjBoDlU9GL0YsLCzw+PFjY3dLRITRo0djxowZyMzM1G3LzMzE999/j9GjRxe5n/379yMwMBBHjx7Fnj17kJ2djS5duiA9Pb3A1xw5cgQDBw7E8OHDcfLkSQQEBCAgIADnzp17pfdUkIImsK44eA3H4h7oni/bf1X3Z0EQEPLPBQgCEJugxvthx4065yTq+gMIAlDbsQK6ejoDyDlVkx/142x8/GsUWs0MhyqF/yZQ4USZwBoYGIjZs2fjyZPSda6TiMq2kydPYvv27ahevTr8/Pzg5+eH6tWr43//+x9Onz6Nvn376h6F2blzJ95//314enrC29sbYWFhuHnzJqKjowt8zcKFC9G1a1dMmDABjRo1wowZM9CsWTMsWrTI2G8TAFDx6ZyR54uJ83dSMHd3zgjOR6/XBgDsOJeAuHs5RdTOcyqcu61GBYUZ7K0tcPJmMkb8GoXH2ca5Iid3vkjr2pXQy9sVALD9zB1otPqjH3H30tFn8WHsjk3E3dRMnLmVbJTjU/klypyREydOIDw8HLt374aXl1eeVRL//PNPMQ5LROWcvb093n77bb1txrjFREpKzqmGSpUqFdgmMjIS48eP19vm7++PrVu3FviazMxMvVEctVpd5Jhy54zkTmB9nK3BuA2nkK0R0MXDCV/3aIS4e+kIv5iEXw5cxXcBXpjztFAZ3q4OOjWsivdWHMWRq/cR+H8xWDakOSzMCv79qX6cjd9PxKNXU1dUtbHMt03uiEybOpXxRoMqsLOyQFJqJo7F3Ufbuo4Ack4Tffp/MUh59OwU0SMjFUNUfolSjOSXMIiIXlVoaKjR+9RqtRg3bhxee+01NG7cuMB2KpUKTk5OetucnJygUqkKfE1ISAi+/fbbYsX14gTWBXsv43JSGqrYKDHr7SaQyWT4tGNdhF9Mwubo26juYI2rd9Nhb22Bj9rVhq2lBVa93xLDVh9H+MUkjNtwCgsHNIV5AQXJT3svY+WhOPwRfQt/ftpWd5ool/pxNs7dzinaWtepBIW5HN0aO2PDiXj87/QdeLrYYe6eS1h79Aa0AtDUzR5mchmibzxERilbK4VKH1GKETESBhGRGAIDA3Hu3DkcOnTI6H0HBQXpjaao1eoij+Q8m8CqQaL6MUIP59zxfGYfL1SqkDNZt3nNSmhVqxKOX3+AH3fljIqMal8XtpY5y8e3qVMZy4Y0x8e/RuHvswlQmssxp5835HL9q180WgF/nc6Z+3FRlYqJf5zBzwN99K6Sib7+EFoBqFnZGi52VgCAXt6uT4uRBOw6n4gH6VkAgH7Nq2NGQGNM/OMMixEqEtEWPXvy5An27t2L5cuXIzU1Z2GcO3fuIC0tTaxDEhEZZPTo0di+fTv27duH6tWrF9rW2dkZiYmJetsSExPh7Oxc4GuUSiVsbW31HkVlrXg6ZyTrCZZGXEXmEy2a13SAX6Oqeu1Gdair+3NVGyWG+tbS29/RvSp+HtgMZnIZ/jx5G19vPZfnCpfjcQ+QlJoJa4UZzOUybD+TgBUHr+m1eX6+SK7WdSqjqo0SaZlP8CA9C/WrVsS6Ea3xYz9vWFqY6d5DRilbK4VKH1GKkRs3bsDLywu9e/dGYGAg7t69CwCYPXs2vvzySzEOSURUZIIgYPTo0diyZQv+/fdf1K5d+6Wv8fX1RXh4uN62PXv26BZfM7bckZG4e+lYd+wmAOCLznnX9OjgXgWNXHKKnDGd6sNKkXdtka6NnTG/f1PIZMD64zfx879X9Pb/70zOqEjPJq6Y1tMDADDrn4s4ePmurs3Rp/NFWteurNtmJpdhfOcGqFOlAqa+5YEdn7XTzR0Bnl0RlME5I/QSohQjn332GVq0aIGHDx/CyspKt71Pnz55/jIXpiysBUBEZU9gYCDWrl2LdevWwcbGBiqVCiqVCo8ePdK1GTp0KIKCgnTPP/vsM+zcuRNz587FxYsX8c033yAqKsqgS4oNkTtn5Mb9DGRptGhduxJ861bO004mk2HlsBZYMqgZBreuUWB/vbxdMbOPFwBgScQVJKpzLrfN1mjxz9kEAEBPb1cMaVMT/ZpXh1YAPv41GksjriI5I0tvvsjzBrSqgX+/6IDhr9fOM0E2d2TkEU/T0EuIUowcPHgQU6ZMybMIUa1atXD79u0i91MW1gIgorJn6dKlSElJQYcOHeDi4qJ7bNy4Udfm5s2bSEhI0D1v27Yt1q1bh19++QXe3t74448/sHXr1kInvb6K3GIk1/h8RkVyVbO3Qncvl5euhDqgpRua13TA42wtFoZfBgAcunIPDzOy4VhRgTZ1KkEmk2FGQGO0q++IR9kazN55EZ3m7odGK6C6gxWqO1gX+T3kjtJkZPE0DRVOlAmsWq0WGk3eSvjWrVuwsbEpcj87d+7Uex4WFoaqVasiOjoab7zxRr6veX4tAACYMWMG9uzZg0WLFmHZsmUGvAsiKg1++umnIrcdO3ZskdoVZVXQiIiIPNv69euHfv36FTmeV1HxuWLk9XqOaF0n76iIoWQyGSZ1bYh3l0di44l4jGhXB/97OnG1u5eL7kobSwsz/PphK2w5eRszd1zAvbScialtDIzh2bwXjoxQ4UQpRrp06YIFCxbgl19+AZDzFyAtLQ3BwcHo3r17sfsVay2AV1WeZopbWZjxPhNUqsyfP1/v+d27d5GRkQF7e3sAQHJyMqytrVG1atUiFyNlgY3ls/T8eecGRuu3Ve1KeLNhVfx7MQkzd1xA5NWciam5i5jlkslk6NusOjo1dMKc3ZewO1aFd5oXPsn3RRWezhnhaRp6GVGKkblz58Lf3x8eHh54/Pgx3nvvPVy+fBmOjo5Yv359sfoUcy2AV1mYCABafLfXoPalWYuaDtj0iS8LEio14uLidH9et24dlixZglWrVsHdPedusZcuXcKIESMwcuRIqUIUhau9FT5pXxd2VhZoXtPBqH1P8HfHvktJ2BObc3WQq50lmtXI/xh21haYEdAYMwIMPx3F0zRUVKIUI9WrV8fp06exYcMGnDlzBmlpaRg+fDgGDRqkN6HVEGKuBVCchYmsLMzQoqYDom48NHo8Uoq68RCPsjV5FjwiKg2mTp2KP/74Q1eIAIC7uzvmz5+Pd955B4MGDZIwOuOb3K2hKP02crFFb29XbH16X5m3vF3zrD1iDJzASkUl2r845ubmGDx4sFH6yl0L4MCBA6KsBVCchYlkMhk2feJbbpY5zsjSlKsRHiqfEhIS8r3nlUajyfP3ngo3vrM7/j6bgGyNkOcUjbE8GxkpH3mSxCNaMXLp0iX8/PPPuHDhAgCgUaNGGD16NBo2LHqlLwgCxowZgy1btiAiIsKgtQDGjRun2/aytQCUSiWUSmWR48olk8k4gkBUgjp16oSRI0di5cqVaNasGQAgOjoao0aNgp+fn8TRlS01Kltj5bCWeJiehcbV7EQ5hm6dERYj9BKiXNq7efNmNG7cGNHR0fD29oa3tzdiYmLg5eWFzZs3F7mfsrAWABGVnNWrV8PZ2RktWrTQ/Yho1aoVnJycsHLlSqnDK3PaN6iCAJ9qovVfgXNGqIhE+Vk/ceJEBAUFYfr06Xrbg4ODMXHixCLfRG/p0qUAgA4dOuhtDw0Nxfvvvw8gZy0AufxZTZW7FsCUKVPw1VdfoX79+qKuBUBEJadKlSrYsWMH/vvvP1y8eBEA0LBhQzRoYLyrTch4eJqGikqUYiQhIQFDhw7Ns33w4MH48ccfi9xPWVgLgIhKXq1atSAIAurWrQtzc54qLa1yT9NkPtFCoxVgJsIkWSofRDlN06FDBxw8eDDP9kOHDqFdu3ZiHJKITEBGRgaGDx8Oa2treHp64ubNnHu2jBkzBrNmzZI4OnqR9XP3ySkvk/1JHEb7SfHXX3/p/tyrVy9MmjQJ0dHRaNOmDQDg6NGj2LRpk8GX0BIR5QoKCsLp06cRERGBrl276rb7+fnhm2++weTJkyWMjl6kNJdDJgMEIefOvRWVHMWi/BntmxEQEJBn25IlS7BkyRK9bYGBgfjkk0+MdVgiMiFbt27Fxo0b0aZNG72F+Tw9PXH16lUJI6P8yGQyVFCYIy3zCeeNUKGMVoxotVpjdUVElK+7d++iatWqebanp6dz1eBSykphxmKEXkqUOSNERGJo0aIF/v77b93z3AJk5cqVha4lRNLRrcKazct7qWCincA7ceIE9u3bh6SkpDyjJvPmzRPrsERUjs2cORPdunVDbGwsnjx5goULFyI2NhZHjhzB/v37pQ6P8mFl8fTOvZkcGaGCiVKMzJw5E1OmTIG7uzucnJz0hk85lEpExfX666/j1KlTmDVrFry8vLB79240a9YMkZGR8PLykjo8ykcFJVdhpZcTpRhZuHAhVq9erVuYjIjIWOrWrYsVK1ZIHQYVEU/TUFGIMmdELpfjtddeE6NrIjJhfn5+CAsLg1qtljoUKqLc0zQcGaHCiFKMfP7551i8eLEYXRORCfP09ERQUBCcnZ3Rr18/bNu2DdnZ2VKHRYXQjYywGKFCiHKa5ssvv0SPHj1Qt25deHh4wMLCQm//n3/+KcZhiaicW7hwIebPn4+9e/di3bp1GDp0KMzMzPDOO+9g0KBBaN++vdQh0gusni4JzwmsVBhRRkbGjh2Lffv2oUGDBqhcuTLs7Oz0HkRExSWXy9GlSxeEhYUhMTERy5cvx/Hjx/Hmm29KHRrlQ3fnXs4ZoUKIMjKyZs0abN68GT169BCjeyIiqFQqbNiwAWvXrsWZM2fQqlUrqUOifPA0DRWFKCMjlSpVQt26dcXomohMmFqtRmhoKDp37gw3NzcsXboUvXr1wuXLl3H06FGD+jpw4AB69uwJV1dXyGQybN26tdD2ERERkMlkeR4qleoV3lH5l3uahhNYqTCijIx88803CA4ORmhoKKytrcU4BBGZICcnJzg4OKB///4ICQlBixYtit1Xeno6vL298eGHH6Jv375Fft2lS5dga2ure57f8vT0DEdGqChEKUZ++uknXL16FU5OTqhVq1aeCawxMTFiHJaIyjFBEPDTTz9h0KBBRvmR061bN3Tr1s3g11WtWhX29vavfHxTkVuMpGdxzggVTJRiJL87+BIRvQpBEBAYGIgOHTqgfv36ksXRtGlTZGZmonHjxvjmm28KXVMpMzMTmZmZuuemuD6KNU/TUBGIUowEBweL0S0RmTC5XI769evj/v37khQjLi4uWLZsGVq0aIHMzEysXLkSHTp0wLFjx9CsWbN8XxMSEoJvv/22hCMtXXiahoqCd+0lojJj1qxZmDBhAs6dO1fix3Z3d8fIkSPRvHlztG3bFqtXr0bbtm0xf/78Al8TFBSElJQU3SM+Pr4EIy4drHIv7eVpGiqEKCMjcrm80BviaTSskInIcEOHDkVGRga8vb2hUChgZWWlt//BgwclGk+rVq1w6NChAvcrlUoolcoSjKj0sVZwOXh6OVGKkS1btug9z87OxsmTJ7FmzRqTH7IkouJbsGCB1CHoOXXqFFxcXKQOo1TjnBEqClGKkd69e+fZ9s4778DT0xMbN27E8OHDxTgsEZVzw4YNM1pfaWlpuHLliu55XFwcTp06hUqVKqFGjRoICgrC7du38euvvwLIKYRq164NT09PPH78GCtXrsS///6L3bt3Gy2m8ohzRqgoSnTOSJs2bRAeHl6ShySicubq1auYMmUKBg4ciKSkJADAP//8g/PnzxvUT1RUFHx8fODj4wMAGD9+PHx8fDBt2jQAQEJCAm7evKlrn5WVhS+++AJeXl5o3749Tp8+jb1796JTp05GemflU24xkqXR4olGK3E0VFqJMjKSn0ePHuGnn35CtWrVSuqQRFTO7N+/H926dcNrr72GAwcO4Pvvv0fVqlVx+vRprFq1Cn/88UeR++rQoQMEQShwf1hYmN7ziRMnYuLEicUN3WTlTmAFgIxsDWzNeN0E5SVKMeLg4KA3gVUQBKSmpsLa2hpr164V45BEZAImT56M7777DuPHj4eNjY1u+5tvvolFixZJGBkVRGEmh7lchidaARmZGthaWrz8RWRyRClGXpxkJpfLUaVKFbRu3RoODg4G9XXgwAH8+OOPiI6ORkJCArZs2VLoomoRERHo2LFjnu0JCQlwdnY26NhEVLqcPXsW69aty7O9atWquHfvngQR0cvIZDJYKcyQ+vgJL++lAolSjBhzkhnvH0FEuezt7ZGQkIDatWvrbT958iRPAZdi1rpihJNYKX+izRlJTk7G8ePHkZSUBK1Wf9LS0KFDi9wP7x9BRLkGDBiASZMmYdOmTZDJZNBqtTh8+DC+/PJLg/IKlaycy3sz8SibxQjlT5Ri5H//+x8GDRqEtLQ02Nra6s0fkclkJZI0eP8IovJn5syZCAwMhJubGzQaDTw8PKDRaPDee+9hypQpUodHBbCy4MJnVDhRpjV/8cUX+PDDD5GWlobk5GQ8fPhQ9xB7hcTc+0ds3rwZmzdvhpubGzp06FDonYJDQkJgZ2ene7i5uYkaIxEVj0KhwIoVK3Dt2jVs374da9euxcWLF/Hbb7/BzMzs5R2QJCoonxYjmZwzQvkTZWTk9u3bGDt2rFFu820od3d3uLu76563bdsWV69exfz58/Hbb7/l+5qgoCCMHz9e91ytVrMgISrF3NzcdKMjZ8+excOHDw2eHE8lx4qrsNJLiDIy4u/vj6ioKDG6LpZWrVrprbT4IqVSCVtbW70HEZU+48aNw6pVqwDk3OOqffv2aNasGdzc3BARESFtcFQg69zTNJwzQgUQZWSkR48emDBhAmJjY+Hl5QULC/3rynv16iXGYQvE+0cQlQ9//PEHBg8eDCBnbtq1a9d0p2m+/vprHD58WOIIKT/PloTnaRrKnyjFyIgRIwAA06dPz7NPJpMZdNde3j+CiHLdu3dPt17Qjh078O6776JBgwb48MMPsXDhQomjo4LkrsKansmREcqfKMXIi5fyvoqoqCi9Rcxy53YMGzYMYWFhBd4/4vbt27C2tkaTJk2wd+/efBdCI6KyxcnJCbGxsXBxccHOnTuxdOlSAEBGRgYnsJZiFZQ5/9Tw0l4qSIndm6a4eP8IIsr1wQcf4N1334WLiwtkMhn8/PwAAMeOHUPDhg0ljo4K8uzSXp6mofyV+mKEiCjXN998g8aNGyM+Ph79+vWDUqkEAJiZmWHy5MkSR0cFyZ0zwqtpqCAsRoioTHnnnXfybDPmLSjI+J5NYGUxQvnjvZyJqEwJDw/HW2+9hbp166Ju3bp46623sHfvXqnDokJYP11nJL2UFCNrjlxH4P/F4DHnsJQaLEaIqMxYsmQJunbtChsbG3z22Wf47LPPYGtri+7du2Px4sVSh0cFKE2X9p69lYJv/ncef59NwL8Xk6QOh54SpRhp3749fv31Vzx69EiM7onIRM2cORPz58/H+vXrMXbsWIwdOxbr1q3D/PnzMXPmTKnDowJYlZI5I1qtgKnbziH3moizt1MkjYeeEaUY8fHxwZdffglnZ2eMGDECR48eFeMwRGRikpOT0bVr1zzbu3TpgpQU/sNSWuWeppF6zsjGqHicik/WPT97i9+Zgty4n46sJ8ZbpuNlRClGFixYgDt37iA0NBRJSUl444034OHhgTlz5iAxMVGMQxKRCejVqxe2bNmSZ/u2bdvw1ltvSRARFUVpuJrmQXoWZu+8CADo41MNQM7ISGFLR5yKT0b/5ZH4OfwyUh9nl0icpcGB/+6i/Y8R6PHTQVxJSi2RY4p2NY25uTn69u2Lvn37IikpCb/88gumTp2Kr776Ct27d8fYsWPx5ptvinV4IionfvrpJ92fPTw88P333yMiIgK+vr4AgKNHj+Lw4cP44osvpAqRXiK3GEkXcc7I42wNvvs7Fo1d7TCgVY08+3/YeRHJGdlo6GyDmX288PeZBKQ8ykb8g0eoUTnvTV2TUh/j41+jkJSaiWNxD7DqcBxGtKuDYW1roaKyfF+I+tfpOwCAy0lp6PnzYczs2xh9fKqLekzRP9Hjx48jNDQUGzZsQNWqVfH+++/j9u3beOutt/Dpp59izpw5YodARGXY/Pnz9Z47ODggNjYWsbGxum329vZYvXo1pkyZUtLhURG8ymkarVbABZUajZxtIZfLCmy36lAc1h69CSsLM7zdvDoszJ4N/F9SpWLDiXgAwIyAxrBSmKGRiw1O30rBmdvJeYqRbI0Wo9edRFJqJuo4VgBkwLW76fhx1yX8GnkdSwY1Q/OalQx+L6WNIAiQyfQ/U61WQMSlnIm99apWxJWkNHy+8TSibzzEjN6N87Q3FlFO0yQlJWHu3Llo3Lgx2rVrh7t372L9+vW4fv06vv32W6xcuRK7d+/GsmXLxDg8EZUjcXFxRXpcu3bNoH4PHDiAnj17wtXVFTKZDFu3bn3payIiItCsWTMolUrUq1cvzwrQlL/cCaxPtILB8xBCj1xHj58OYU3k9QLbJKU+xpJ9Ofcwe5StwYUEtd7+fU//ce3oXgUta+UUEV7V7QDkP2/kh50XcTzuASoqzbFyWAvs+bw9FvRvipqVrZGozkT/5Uex5sj1Qk/xvKqLKjW+3nIW0TceFtpOEAT8HhWPLSdvGRTPtbtpaDp9DyZvPqO3/cztFNxLy0JFpTm2j3kd4/zqQyYD1h69iaPXHhTrvRSFKMVI9erVsXLlSgwbNgy3bt3CH3/8ga5du+pVVE2aNEHLli3FODwRlXP37t3DvXv3XqmP9PR0eHt7F/mS4Li4OPTo0QMdO3bEqVOnMG7cOHz00UfYtWvXK8VhCnJP0wDPRkd2nE3A3N2XkK0pvDgJv5Azz/DAf3cLbDN31396a5icuK7/D/jxuJx/RF+r56jb1qSaPQDgzAvFyI6zCVhxMA4AMKdfE9SpUhFmchkCfKphx9h26OHlgidaAcF/ncf430/nO9qz72ISgv48g4QUw68ozdZosXDvZfT8+RD+79hNDFl1DDE3Cy5I5u35DxP/OIPPN57G6PUnkZ5ZtFNhSyKuIuVRNjZGxePWwwzd9n+fft5vNHCEpYUZxvk1wNvNck7R/HMuweD3U1SiFCPh4eG4cOECJkyYgCpVquTbxtbWFvv27RPj8ERUDiUnJyMwMBCOjo5wcnKCk5MTHB0dMXr0aCQnJxvcX7du3fDdd9+hT58+RWq/bNky1K5dG3PnzkWjRo0wevRovPPOO3lOI1FeFmZyKJ6eNknPeoInGi0m/nEGP/97BUv2XS3wdRqtgNNPr345f0edb5tzt1Pwe3TOKZjOHk4AgKjrz37Ba7QCTjwtRtrUqazbnjsycu52CrTanBGFx9kafL3lLABg5Bt10LWxi96xKijNseg9H3zdvRHM5DJsOXkbfZcewY376bo2oYfj8OGaE1h/PB7Dw6LyFCtJqY9x8PJdaLR5RzHO3U5B70WHMX/vf8jWCHCsqERGlgbvrz6eZ7QHAJbtv4qf/80ZETKTy/D3mQT0XXIEcffS87R9XqL6Mbadug0AEARg49NTWAAQ/nTtlTcbOum29fDK+Rz+OafKN25jEKUYadeuHYCc0zUHDx7EwYMHkZTExWWIqHgePHiA1q1bY82aNXj77bcxd+5czJ07F3379kVYWBh8fX3x8GHhw9mvKjIyUndjvlz+/v6IjIws8DWZmZlQq9V6D1P1/FojsQlqpD39Bf/zv5dx/k7+l9heUqXqRjySUjNxNzVTb78gCPju71gIAtDT2xUj2tUBAETdeKg7ZXEhQY3UzCeoqDRHIxdb3WvrV60IpbkcqZlPcP1pMbHjbAIeZmSjmr0VJvi75xuTTCbDiDfqYO3w1nCsqMCFBDV6/nwIe2MT8d32WHz7v5x4FGZyxCaoMeGP07pYjsc9QNcFBzFk1XH0XnwIJ54WTQ/Ts/D1lrPotegQYhPUsLe2wMIBTXFgYgc0r+kA9eMnGLLqOK7dTdPF8Vvkdcz6J+fqoMndGmLjx21QxUaJS4mp6LXoEDZHF3zaJvTwdWRrBNhY5szl2XAiHtkaLVQpj3H+jhoyGdDB/dlAwmv1HGFjaY67qZl6hZ4xiTKBNTU1FZ9++ik2bNgAjSbni2RmZob+/ftj8eLFsLOzE+OwRFROTZ8+HQqFAlevXoWTk1OefV26dMH06dNFHaVQqVR5ju3k5AS1Wo1Hjx7Bysoqz2tCQkLw7bffihZTWWKtMEPKo2w8ytLoTpsAOfNIvvj9NP4a/ToU5vq/j188PXH+Tgo6uFfVPd8dm4ij1x5AaS7HpK7ucKyohMJMjrupmbj5IAM1K1fQHatFLQeYPTcB1txMDk9XW8TcTMbZ2ymoU6Ui1h27CQAY2MoN5maF/1b3rVsZ/xvzOj79vxicvJmMj36N0u2b2NUdzWs4YNDKY9h+JgGNXGxR1UaJr7acRbYmp0A4d1uNfssi0dG9CmJuJiPlUc6lw281cUFwT09Uscm5CeTq91ti4C9HEZugxptz98NMLoOZXKabezO6Yz180r4uAODvp/FE3XiILzadxp8nb+H7AC/Ucqygiy0t8wn+79gNAMDst5tg2rbzuJuaib2xiUh+GoN3dXs4VlTqXqMwl6OzhxP+jLmNf86p0Pq5ESZjEWVk5KOPPsKxY8ewfft2JCcnIzk5Gdu3b0dUVBRGjhwpxiGJqBzbunUr5syZk6cYAABnZ2f88MMP+a4/IrWgoCCkpKToHvHx8S9/UTn1bGTkiW5E4KPXa6NSBQUuqlKx6OkE1OflLUb0R5Zyi4cPX6+N6g7WsLQwQ+NqOaMfUU/njRyLuw8AaFU779UvTarbA8iZN/JfYiqibjyEuVyGd1u4Fek9udhZYcPHbTCkTU0AgIWZDAv6N8WnHeqhdZ3K+La3JwDgx12XMOGPM8jWCOju5YyDEztiYKsakMmAfZfuIuVRziXHGz5ug0XvNdMVIgBgZ2WBX4e3gqdrzvvSPDcJeES72viiSwNd26q2llj/cRtM6toQSnM5Dl+5D/8FB7Ak4opubs7GE/FIffwEdapUQFdPZ/RvmTMfZN3xmwi/kHMGo1PDZwVfrmenahJ0p7WMSZSRke3bt2PXrl14/fXXddv8/f2xYsWKfFdPJCIqTEJCAjw9PQvc37hxY6hUKlFjcHZ2zrNoY2JiImxtbfMdFQEApVIJpVKZ7z5T8/zCZ7kTTLt5OcOnhgMC18Vg8b4r6NzISTeXAwBO3UwGALSuXQnH4h4g9rliJOuJVlfU9PJ21W1vWasSYm4mI+rGA/RtVk03MtK6dt5f817Vnl1RkzsXwq+RE6raWhb5fSnNzTAjoDF6NHFBpQoKNHCy0e0b1LomLiSosfZoTtE0tlN9jOtUH3K5DCF9vTC4TQ2sOhQHnxoOGNiy4NEYx4pKbB/zOu6nZ0GjFfBEK0BhJtcrWnJZmMkxqkNddPdyxtdbzuHQlXv4Yecl/HXqDr4LaIzVh3Im545oVwdyuQwDWtbAkoirOHj5nm5k6s1GeYuR1+s7wkZpjkR1JqJvPtRdlWQsooyMVK5cOd9TMXZ2dnBwcBDjkERUjjk6OuL69esF7o+Li0OlSuKu++Dr64vw8HC9bXv27NEtvkaFy11r5OztFDxIz4LSXA6vavbo0cQFPZq4QKMVMGf3JV37h+lZuPZ0IuagpyMPz88tOX0rGRlZGlSqoID7cwVAi6f/SEZdf4grSWl4mJENSwu5rvB4XpPcSax3UrA55hYA4L3WeRdMK4o2dSrrFSK5gnt64uvujRD6QUuM79xAb60UT1c7zHu3KYa0qfnS00IymQyOFZVwsrVENXurfAuR59WsXAG/DW+Fuf284WBtgYuqVLyzLBK3kx/BsaJCtwqtWyVrtG+QMz8k64kWzraW8Hhubk0upbkZ/J5OEN5x1vhX1YhSjEyZMgXjx4/X+6WiUqkwYcIETJ06VYxDElE55u/vj6+//hpZWVl59mVmZmLq1KkGj7qmpaXh1KlTOHXqFICcgubUqVO4eTPnV2xQUBCGDh2qa//JJ5/g2rVrmDhxIi5evIglS5bg999/x+eff178N2ZCckdGchfU8qlhr/slPqFLzmTRA5fv4nZyzuWwJ+NzRk/qOFbA608vyb1+P0O3LPuRKzmnX3zrVNb7B755zZwfvJeT0rDrfM6/Qc1qOOSZjwIAdapUhLXCDBlZGqQ+foIalax1xzIWCzM5RrxRBx3d8442iE0mk+Ht5tWxd3x79H1afADAUN9asLR4drn1oNY1dX/u2LBqgQubdc89VXNWZfRTNaKcplm6dCmuXLmCGjVqoEaNnCrz5s2bUCqVuHv3LpYvX65rGxMTI0YIRFSOTJ8+HS1atED9+vURGBiIhg0bQhAEXLhwAUuWLEFmZiZ+++03g/qMiopCx44ddc/Hjx8PABg2bBjCwsKQkJCgK0wAoHbt2vj777/x+eefY+HChbr1lPz9/Y3zJsu53GIk90Z1rZ4b5q/lWAG+dSoj8tp9bIqKxzi/Boi5kdPOp4YDKlVQwMXOEgkpj3EhIRWtalfCkas568z41tU//VKpggJ1q1TA1bvpCD18PedY+cwXAXIuh/V0tdWdNhrQyq3QVV7LqsoVlZjXvyneaV4dZ26n4IPXaunt7+heBa52lriT8hhdPPLOy8rVrr4jKirNoVI/xsn4h0ZdhVaUYiQgIECMbonIRFWvXh2RkZH49NNPERQUpLtkUSaToXPnzli0aBHc3Io26TBXhw4dCl2xMr/VVTt06ICTJ08adBzKYWWR889N7g/qVi/M4RjQyg2R1+7j9xPxGPNmfd3k1WY17QHknNJISHmM83dS4FXNDiefzidpWzfvXJAWNSvh6t103E/PGUnLb75ILq9q9jhxPWfiar/mhn2Hypq29RzRNp+RH3MzOVZ/0BLnbqv1Lul9kaWFGfwaVcXWU3ew46yq9BcjwcHBYnRLRCasdu3a+Oeff/Dw4UNcvnwZAFCvXj3R54qQcTy/CquZXAafGvZ6+/09nWFnZYE7KY+x/78k3WJnzWrknHbxdLXF3guJOH9HjegbD5Gl0cLFzhK1n7tsNVeLWg7YGJVz5ZKFWd5jPa9jwypYfTgOfZtVe+k8jPKsobMtGjrnnSvyom5eLgi/kKR3mbQxiHqjvOjoaFy4cAEA4OnpCR8fHzEPR0QmwMHBAa1atZI6DDKQtfJZMdLY1RYVXrjzraWFGfr4VEPYkesI2XER6VkaVFCY6SaF5l7aev6OGlWfFg2+dSvnO7/h+Ss9vKvb682PeFG7+lWw78sOqGaf/xVRpO/NhlURNdUPSvOCP9PiEKUYSUpKwoABAxAREQF7e3sAOUs5d+zYERs2bChwiXgiIiqfrC2e/XNT0ByOAa3cEHbkOi4n5aw06u1mr/sF7vn0apjLianILT/a1s1/smnNytZwrKjAvbSsAo/1vPxGVyh/Fi+56qe4ROl1zJgxSE1Nxfnz5/HgwQM8ePAA586dg1qtxtixY8U4JBERlWLPn6YpaI2Khs62aOpmr3uee4oGAFztLGFvbYEnWgGxT+/T8uLk1VwymQy9m1aDwkyOHk1c8m1DpYsoxcjOnTuxZMkSNGrUSLfNw8MDixcvxj///GNQX7zNNxFR2WdVhGIEAAa0fDaJNHfyKpBTYOSeqgGAWpWtCz218nX3RjjzTRd4uvL2I2WBKMWIVquFhYVFnu0WFhbQagu/XfSLeJtvIqKyr+LTOSINnCrCoYKiwHY9vV1RqYICFRRmeiMjAPQKC98CTtHkkstlhc4VodJFlDkjb775Jj777DOsX78erq45y/Tevn0bn3/+OTp16mRQX926dUO3bt2K3P7523wDQKNGjXDo0CHMnz+f6wEUUcYLt7ymss3KwqzARYyISkq7+o7o1LAq3m5evdB2FZTm2Bb4GrI0Wthb6xctz4+M5HdJL5VdohQjixYtQq9evVCrVi3dtf/x8fFo3Lgx1q5dK8YhdQq6zfe4ceMKfE1mZiYyM5/dmtqUb/MNAC2+2yt1CGREsdP9dUtxE0mlckUlVr3fskht3SpZ57v9+ZGRNiLcOZakI0qGcnNzQ0xMDPbu3YuLFy8CyBmheLFIEANv8108VhZmaFHTAVE3Hr68MRGRBOpWqYCxnerD1tLcpNcEKY9E+7mUuzJi586dxTqE0QQFBemWggZyRkYMXc2xrJPJZNj0iS8eZfMUTXljxfPmVE7IZDKM79xA6jBIBEYtRv7991+MHj0aR48eha2t/kpuKSkpaNu2LZYtW4Z27doZ87B6eJvv4pPJZBzOJyKiEmfUq2kWLFiAESNG5ClEAMDOzg4jR47EvHnzjHnIPHibbyIiorLFqMXI6dOnC72Nd5cuXRAdHW1Qn7zNNxERUflm1GIkMTEx3/VFcpmbm+Pu3bsG9RkVFQUfHx/dfW3Gjx8PHx8fTJs2DQAKvM33nj174O3tjblz5/I230RERKWYUScIVKtWDefOnUO9evXy3X/mzBm4uBi2NK8Ut/nOPZ6pX+JL5Vvu97uwv19kXMwtZAqKk1uMWox0794dU6dORdeuXWFpaam379GjRwgODsZbb71lzEOKIjU1FQBM7ooaMk2pqamws+OS2SWBuYVMiSG5RSYY8WdRYmIimjVrBjMzM4wePRru7u4AgIsXL2Lx4sXQaDSIiYnJsw5IaaPVanHnzh3Y2Njku3Jl7qW/8fHx+U7WJcPxMzW+l32mgiAgNTUVrq6ukMvFuRMn6WNuKXn8TI1PjNxi1JERJycnHDlyBKNGjUJQUJBuiEYmk8Hf3x+LFy8u9YUIAMjlclSvXviSxQBga2vLL7eR8TM1vsI+U46IlCzmFunwMzU+Y+YWoy8qUbNmTezYsQMPHz7ElStXIAgC6tevDwcHh5e/mIiIiEyOaCtcOTg4oGXLot2HgIiIiEwXTxQXg1KpRHBwMFdtNSJ+psbHz7Ts4f8z4+NnanxifKZGncBKREREZCiOjBAREZGkWIwQERGRpFiMEBERkaRYjBAREZGkWIwUYPHixahVqxYsLS3RunVrHD9+vND2mzZtQsOGDWFpaQkvLy/s2LGjhCItOwz5TMPCwiCTyfQeL95iwJQdOHAAPXv2hKurK2QyGbZu3frS10RERKBZs2ZQKpWoV69evvd1IvExtxgfc4vxSJVbWIzkY+PGjRg/fjyCg4MRExMDb29v+Pv7IykpKd/2R44cwcCBAzF8+HCcPHkSAQEBCAgIwLlz50o48tLL0M8UyFndLyEhQfe4ceNGCUZcuqWnp8Pb2xuLFy8uUvu4uDj06NEDHTt2xKlTpzBu3Dh89NFH2LVrl8iR0vOYW4yPucW4JMstAuXRqlUrITAwUPdco9EIrq6uQkhISL7t3333XaFHjx5621q3bi2MHDlS1DjLEkM/09DQUMHOzq6EoivbAAhbtmwptM3EiRMFT09PvW39+/cX/P39RYyMXsTcYnzMLeIpydzCkZEXZGVlITo6Gn5+frptcrkcfn5+iIyMzPc1kZGReu0BwN/fv8D2pqY4nykApKWloWbNmnBzc0Pv3r1x/vz5kgi3XOJ3VHrMLcbH3CI9Y31HWYy84N69e9BoNHlu6Ofk5ASVSpXva1QqlUHtTU1xPlN3d3esXr0a27Ztw9q1a6HVatG2bVvcunWrJEIudwr6jqrVajx69EiiqEwLc4vxMbdIz1i5RbR70xC9Cl9fX/j6+uqet23bFo0aNcLy5csxY8YMCSMjorKMuaV04sjICxwdHWFmZobExES97YmJiXB2ds73Nc7Ozga1NzXF+UxfZGFhAR8fH1y5ckWMEMu9gr6jtra2sLKykigq08LcYnzMLdIzVm5hMfIChUKB5s2bIzw8XLdNq9UiPDxcr5p+nq+vr157ANizZ0+B7U1NcT7TF2k0Gpw9exYuLi5ihVmu8TsqPeYW42NukZ7RvqOGzq41BRs2bBCUSqUQFhYmxMbGCh9//LFgb28vqFQqQRAEYciQIcLkyZN17Q8fPiyYm5sLc+bMES5cuCAEBwcLFhYWwtmzZ6V6C6WOoZ/pt99+K+zatUu4evWqEB0dLQwYMECwtLQUzp8/L9VbKFVSU1OFkydPCidPnhQACPPmzRNOnjwp3LhxQxAEQZg8ebIwZMgQXftr164J1tbWwoQJE4QLFy4IixcvFszMzISdO3dK9RZMEnOL8TG3GJdUuYXFSAF+/vlnoUaNGoJCoRBatWolHD16VLevffv2wrBhw/Ta//7770KDBg0EhUIheHp6Cn///XcJR1z6GfKZjhs3TtfWyclJ6N69uxATEyNB1KXTvn37BAB5Hrmf4bBhw4T27dvneU3Tpk0FhUIh1KlTRwgNDS3xuIm5RQzMLcYjVW6RCYIgvOIoDREREVGxcc4IERERSYrFCBEREUmKxQgRERFJisUIERERSYrFCBEREUmKxQgRERFJisUIERERSYrFCJWY999/HwEBAVKHQUREpQzv2ktGIZPJCt0fHByMhQsXgmvsERHRi1iMkFEkJCTo/rxx40ZMmzYNly5d0m2rWLEiKlasKEVoRERUyvE0DRmFs7Oz7mFnZweZTKa3rWLFinlO03To0AFjxozBuHHj4ODgACcnJ6xYsQLp6en44IMPYGNjg3r16uGff/7RO9a5c+fQrVs3VKxYEU5OThgyZAju3btXwu+YiIiMhcUISWrNmjVwdHTE8ePHMWbMGIwaNQr9+vVD27ZtERMTgy5dumDIkCHIyMgAACQnJ+PNN9+Ej48PoqKisHPnTiQmJuLdd9+V+J0QEVFxsRghSXl7e2PKlCmoX78+goKCYGlpCUdHR4wYMQL169fHtGnTcP/+fZw5cwYAsGjRIvj4+GDmzJlo2LAhfHx8sHr1auzbtw///fefxO+GiIiKg3NGSFJNmjTR/dnMzAyVK1eGl5eXbpuTkxMAICkpCQBw+vRp7Nu3L9/5J1evXkWDBg1EjpiIiIyNxQhJysLCQu+5TCbT25Z7lY5WqwUApKWloWfPnpg9e3aevlxcXESMlIiIxMJihMqUZs2aYfPmzahVqxbMzfn1JSIqDzhnhMqUwMBAPHjwAAMHDsSJEydw9epV7Nq1Cx988AE0Go3U4RERUTGwGKEyxdXVFYcPH4ZGo0GXLl3g5eWFcePGwd7eHnI5v85ERGWRTOCSmERERCQh/pQkIiIiSbEYISIiIkmxGCEiIiJJsRghIiIiSbEYISIiIkmxGCEiIiJJsRghIiIiSbEYISIiIkmxGCEiIiJJsRghIiIiSbEYISIiIkmxGCEiIiJJ/T/1P4tD/9Y4GwAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 600x600 with 4 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Get a trajectory of the system\n",
    "parameter_values_sets = []\n",
    "# parameter_values_sets.append({'b1': 5, 'b2': 5, 'd1': 0.5, 'd2': 1}) # base parameters\n",
    "parameter_values_sets.append({'k1': 1, 'k2': 1, 'kp1': 1})\n",
    "\n",
    "# parameter_values_sets.append({'k': 1, 'g': 1})\n",
    "\n",
    "\n",
    "parameter_set_index = 0\n",
    "parameter_values = parameter_values_sets[parameter_set_index]\n",
    "#initial_state = {'M': 0, 'P': 0}\n",
    "initial_states = []\n",
    "initial_states.append({'G0': 1, 'G1': 0, 'mRNA': 0})\n",
    "initial_states.append({'G0': 0, 'G1': 1, 'mRNA': 0})\n",
    "initial_states.append({'G0': 1, 'G1': 0, 'mRNA': 1})\n",
    "initial_states.append({'G0': 0, 'G1': 1, 'mRNA': 1})\n",
    "initial_states.append({'G0': 1, 'G1': 0, 'mRNA': 2})\n",
    "initial_states.append({'G0': 0, 'G1': 1, 'mRNA': 2})\n",
    "initial_states.append({'G0': 1, 'G1': 0, 'mRNA': 3})\n",
    "initial_states.append({'G0': 0, 'G1': 1, 'mRNA': 3})\n",
    "initial_states.append({'G0': 1, 'G1': 0, 'mRNA': 4})\n",
    "initial_states.append({'G0': 0, 'G1': 1, 'mRNA': 4})\n",
    "initial_states.append({'G0': 1, 'G1': 0, 'mRNA': 1})\n",
    "initial_states.append({'G0': 0, 'G1': 1, 'mRNA': 1})\n",
    "initial_states.append({'G0': 1, 'G1': 0, 'mRNA': 2})\n",
    "initial_states.append({'G0': 0, 'G1': 1, 'mRNA': 2})\n",
    "initial_states.append({'G0': 1, 'G1': 0, 'mRNA': 3})\n",
    "initial_states.append({'G0': 0, 'G1': 1, 'mRNA': 3})\n",
    "\n",
    "# for i in range(10):\n",
    "#     initial_states.append({'mRNA': i})\n",
    "\n",
    "tf = 1.0\n",
    "time_list, state_list, cPP = MI.SSA(initial_states[-1], parameter_values, 0, tf, compute_centered_poisson_process=True)\n",
    "MI.plot_trajectories(time_list, state_list)\n",
    "\n",
    "# Generate the observations\n",
    "\n",
    "Observation_times_list = np.linspace(0, tf, 50)\n",
    "Y_list = MI.generate_observations(state_list, time_list, parameter_values, Observation_times_list)\n",
    "\n",
    "# plot the observations\n",
    "plt.plot(Observation_times_list, Y_list)\n",
    "plt.ylabel(\"Observed process\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "visualize the centered Poisson Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "cPP.plot() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Debugging information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.0,\n",
       " 0.007386791293333567,\n",
       " 0.10412552096291895,\n",
       " 0.31598546295336993,\n",
       " 0.9999999999999999]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cPP.time_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([0., 1., 1., 3.]),\n",
       " array([0., 1., 1., 3.]),\n",
       " array([0., 1., 1., 2.]),\n",
       " array([1., 0., 0., 2.]),\n",
       " array([1., 0., 0., 1.])]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cPP.current_a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([0., 0., 0., 0.]),\n",
       " array([0.        , 0.00738679, 0.00738679, 0.02216037]),\n",
       " array([0.        , 0.10412552, 0.10412552, 0.21563783]),\n",
       " array([0.21185994, 0.10412552, 0.10412552, 0.63935772]),\n",
       " array([0.89587448, 0.10412552, 0.10412552, 1.32337225])]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cPP.centered_PP_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([0., 0., 0., 0.]),\n",
       " array([ 0.        , -0.00738679, -0.00738679,  0.97783963]),\n",
       " array([ 0.        ,  0.89587448, -0.10412552,  0.78436217]),\n",
       " array([-0.21185994,  0.89587448, -0.10412552,  1.36064228]),\n",
       " array([-0.89587448,  0.89587448, -0.10412552,  0.67662775])]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cPP.centered_PP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-9.88587448,  0.89587448, -0.10412552, -8.31337225],\n",
       "       [-9.89587448,  0.89587448, -0.10412552, -8.32337225],\n",
       "       [-9.90587448,  0.89587448, -0.10412552, -8.33337225]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cPP.sample_at_times([9.99, 10., 10.01])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-9.88587448  0.89587448 -0.10412552 -8.31337225]\n",
      "[-9.90587448  0.89587448 -0.10412552 -8.33337225]\n"
     ]
    }
   ],
   "source": [
    "print(cPP.at(9.99))\n",
    "print(cPP.at(10.01))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([0., 0., 0., 0.]),\n",
       " array([0., 0., 0., 1.]),\n",
       " array([0., 1., 0., 1.]),\n",
       " array([0., 1., 0., 2.]),\n",
       " array([0., 1., 0., 2.])]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cPP.centered_PP_actual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(cPP.time_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.searchsorted(np.array(cPP.time_list), 0.01, side='right') - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.0,\n",
       " 0.007386791293333567,\n",
       " 0.10412552096291895,\n",
       " 0.31598546295336993,\n",
       " 0.9999999999999999]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cPP.time_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([0., 0., 0., 0.]),\n",
       " array([ 0.        , -0.00738679, -0.00738679,  0.97783963]),\n",
       " array([ 0.        ,  0.89587448, -0.10412552,  0.78436217]),\n",
       " array([-0.21185994,  0.89587448, -0.10412552,  1.36064228]),\n",
       " array([-0.89587448,  0.89587448, -0.10412552,  0.67662775])]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cPP.centered_PP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-9.88587448,  0.89587448, -0.10412552, -8.31337225],\n",
       "       [-9.90587448,  0.89587448, -0.10412552, -8.33337225]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cPP.sample_at_times([9.99, 10.01])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-9.88587448,  0.89587448, -0.10412552, -8.31337225])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cPP.at(9.99)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.searchsorted(np.array(cPP.time_list), 9.99, side='right') - 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataloader structure\n",
    "\n",
    "Let $\\mathcal{T}$ be the set of measurement times, $\\Tau$ the time discretization set, and $\\mathcal{R}$ be the set of reactions. The dataloader should contain:\n",
    "\n",
    "* $X(t) \\forall t\\in\\Tau$ the full process \n",
    "* $Y(t) \\forall t\\in\\mathcal{T}$ the observed process\n",
    "* $\\tilde{R}_k(t) \\forall t\\in\\Tau, k\\in \\mathcal{R}$ the centered poisson process associated to the reaction $k$ \n",
    "\n",
    "this implies the following tensor dimensions (we condiser the batch size $B$) of the input tensors, respectively:\n",
    "\n",
    "* $[B, |\\Tau|, |X|]$\n",
    "* $[B, |\\mathcal{T}|, |y|]$\n",
    "* $[B, |\\Tau|, |r|]$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # this can be done quickly by running \n",
    "# import math\n",
    "# n_samples = 1000  # p in the paper\n",
    "# batch_size = 100 #int(math.sqrt(n_samples))\n",
    "\n",
    "# n_Y_measurements = 2 # n in the paper\n",
    "# n_X_measurements_between_Y_measurements = 100 # m_bar-2 in the paper\n",
    "\n",
    "# dataset = run_SSA_for_filtering(MI, initial_states, parameter_values, tf, n_Y_measurements, n_X_measurements_between_Y_measurements, n_samples=n_samples)\n",
    "# train_dataset, val_dataset, Xtimes, Ytimes = CRN_simulations_to_dataloaders(dataset, batch_size, test_split=0.2)\n",
    "\n",
    "# print(\"--- check batch sizes ---\")\n",
    "# print(\"training : \", [x.shape for x in next(iter(train_dataset))])\n",
    "# print(\"validation : \", [x.shape for x in next(iter(val_dataset))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:01<00:00, 658.59it/s]\n"
     ]
    }
   ],
   "source": [
    "# use SSA to compute the conditional expectations:\n",
    "\n",
    "n_samples = 1000  # p in the paper\n",
    "batch_size = 250 #int(math.sqrt(n_samples))\n",
    "\n",
    "n_Y_measurements = 3 # n in the paper\n",
    "n_X_measurements_between_Y_measurements = 100 # m_bar-2 in the paper\n",
    "\n",
    "dataset = run_SSA_for_filtering(MI, initial_states, parameter_values, tf, n_Y_measurements, n_X_measurements_between_Y_measurements, n_samples=n_samples)\n",
    "#train_dataset, val_dataset, Xtimes, Ytimes = CRN_simulations_to_dataloaders(dataset, batch_size, test_split=0.2)\n",
    "train_dataset, val_dataset, Xtimes, Ytimes = CRN_simulations_to_dataloaders(dataset, batch_size, test_split=0.2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_538689/1828700344.py:1: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:274.)\n",
      "  finalX = torch.tensor(dataset['X'])[:, -1].flatten()\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.211000</td>\n",
       "      <td>0.157667</td>\n",
       "      <td>0.000667</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.179333</td>\n",
       "      <td>0.212667</td>\n",
       "      <td>0.007000</td>\n",
       "      <td>0.002333</td>\n",
       "      <td>0.000333</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.025000</td>\n",
       "      <td>0.042333</td>\n",
       "      <td>0.015667</td>\n",
       "      <td>0.003000</td>\n",
       "      <td>0.000333</td>\n",
       "      <td>0.000667</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.020333</td>\n",
       "      <td>0.038000</td>\n",
       "      <td>0.028000</td>\n",
       "      <td>0.009000</td>\n",
       "      <td>0.001333</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.007333</td>\n",
       "      <td>0.015333</td>\n",
       "      <td>0.012333</td>\n",
       "      <td>0.007667</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.000333</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000333</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          0         1         2         3         4         5    6         7\n",
       "0  0.211000  0.157667  0.000667  0.001000  0.000000  0.000000  0.0  0.000000\n",
       "1  0.179333  0.212667  0.007000  0.002333  0.000333  0.000000  0.0  0.000000\n",
       "2  0.025000  0.042333  0.015667  0.003000  0.000333  0.000667  0.0  0.000000\n",
       "3  0.020333  0.038000  0.028000  0.009000  0.001333  0.000000  0.0  0.000000\n",
       "4  0.007333  0.015333  0.012333  0.007667  0.001000  0.000333  0.0  0.000333"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "finalX = torch.tensor(dataset['X'])[:, -1].flatten()\n",
    "initialX = torch.tensor(dataset['X'])[:, 0].flatten()\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "conditional_probability = torch.zeros(int(max(initialX))+1, int(max(finalX))+1)\n",
    "for i in range(int(max(initialX))+1):\n",
    "    for j in range(int(max(finalX))+1):\n",
    "        # select the rows where the initial state is i and the final state is j\n",
    "        try:\n",
    "            conditional_probability[i, j] = torch.sum((initialX == i) & (finalX == j)).item() / finalX.shape[0]\n",
    "        except:\n",
    "            conditional_probability[i, j] = 0\n",
    "\n",
    "\n",
    "# convert to pandas\n",
    "df = pd.DataFrame(conditional_probability.numpy())\n",
    "        \n",
    "df\n",
    "\n",
    "# divide each column by the sum of the column\n",
    "# df = df.div(df.sum(axis=1), axis=1)\n",
    "# df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reimplementing DeepCME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from OtherNetworks import MLP\n",
    "# from DeepCME import DeepCME, TemporalFeatureExtractor\n",
    "\n",
    "\n",
    "# r = 1                          # number of temporal features\n",
    "# n = MI.get_number_of_species() # number of species\n",
    "\n",
    "# g_functions = [\n",
    "#     lambda x: x[:, 0],\n",
    "#     lambda x: x[:, 1],\n",
    "#     #lambda x: x[:, 0]**2,\n",
    "#     #lambda x: x[:, 1]**2\n",
    "# ]\n",
    "\n",
    "# R = len(g_functions)\n",
    "# K = MI.get_number_of_reactions()\n",
    "\n",
    "# device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# temporal_feature_extractor = TemporalFeatureExtractor(r, tf, device=device)\n",
    "# L = 2                          # number of layers\n",
    "# backbone = MLP(2*r+n, L, R*K, L, activation=torch.nn.ReLU, postprocessing_layer=None).to(device)\n",
    "\n",
    "# tau_list = torch.linspace(0, tf, n_X_measurements_between_Y_measurements+2)\n",
    "\n",
    "# delta_threshold = torch.tensor([])\n",
    "\n",
    "# deepCME = DeepCME(backbone, tau_list, g_functions, temporal_feature_extractor, R, K, device=device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def deepCME_training_loop(model, train_loader, optimizer, n_epochs, validation_loader=None, scheduler=None):\n",
    "#     training_loss = []\n",
    "#     validation_loss = []\n",
    "#     for epoch in range(n_epochs):\n",
    "#         for i, (X, Y, R) in enumerate(train_loader):\n",
    "#             X = X.to(model.device)\n",
    "#             Y = Y.to(model.device)\n",
    "#             R = R.to(model.device)\n",
    "#             optimizer.zero_grad()\n",
    "#             loss = model.loss(X, R)\n",
    "#             loss.backward()\n",
    "#             optimizer.step()\n",
    "#             if scheduler is not None:\n",
    "#                 scheduler.step()\n",
    "#             if epoch % 10 == 0:\n",
    "#                 print(f'Training: Epoch {epoch}, batch {i}, Loss {loss.item()}')\n",
    "#             training_loss.append((epoch*len(train_loader) + i, loss.item()))\n",
    "#         if epoch % 10 == 0 and validation_loader is not None:\n",
    "#             for i, (X, Y, R) in enumerate(validation_loader):\n",
    "#                 with torch.no_grad():\n",
    "#                     X = X.to(model.device)\n",
    "#                     Y = Y.to(model.device)\n",
    "#                     R = R.to(model.device)\n",
    "#                     loss = model.loss(X, R)\n",
    "#                     validation_loss.append((epoch*len(validation_loader) + i, loss.item()))\n",
    "#                     if i % 10 == 0:\n",
    "#                         print(f'Validation: Epoch {epoch}, batch {i}, Loss {loss.item()}')\n",
    "#     return training_loss, validation_loss\n",
    "\n",
    "# optimizer = torch.optim.Adam(deepCME.parameters(), lr=0.01)\n",
    "# n_epochs = 1000\n",
    "# training_loss, validation_loss = deepCME_training_loop(deepCME, train_dataset, optimizer, n_epochs, validation_loader=val_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# deepCME.Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# raise Exception(\"Stop here\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### New implementation based on deepCME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from OtherNetworks import MLP, RNNEncoder\n",
    "from DeepCME import FilteringDeepCME, TemporalFeatureExtractor\n",
    "from copy import deepcopy\n",
    "\n",
    "def deepcopy_flatten(x):\n",
    "    other = deepcopy(x)\n",
    "    if type(x) == RNNEncoder:\n",
    "        other.RNN.flatten_parameters()\n",
    "    return other\n",
    "\n",
    "\n",
    "r = 1                          # number of temporal features\n",
    "n = MI.get_number_of_species() # number of species\n",
    "O = 1 # number of observed species\n",
    "\n",
    "# BD \n",
    "# g_functions = [\n",
    "#     lambda x: x[:, 0],\n",
    "#     # lambda x: x[:, 1],\n",
    "#     # lambda x: x[:, 2],\n",
    "#     lambda x: x[:, 0]*0. + 1., # constant function\n",
    "#     #lambda x: x[:, 0]**2,\n",
    "#     #lambda x: x[:, 1]**2\n",
    "# ]\n",
    "\n",
    "g_functions = [\n",
    "    lambda x: x[:, 0],\n",
    "    lambda x: x[:, 1],\n",
    "    lambda x: x[:, 2],\n",
    "    lambda x: x[:, 0]*0. + 1., # constant function\n",
    "    #lambda x: x[:, 0]**2,\n",
    "    #lambda x: x[:, 1]**2\n",
    "]\n",
    "\n",
    "R = len(g_functions)\n",
    "K = MI.get_number_of_reactions()\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "temporal_feature_extractor = TemporalFeatureExtractor(r, tf, device=device)\n",
    "\n",
    "x_hidden_size = 32\n",
    "x_encoder = MLP(n, x_hidden_size, x_hidden_size, 2, activation=torch.nn.ReLU, postprocessing_layer=None).to(device)\n",
    "y_hidden_size = 32\n",
    "#postproc = MLP(y_hidden_size, y_hidden_size, y_hidden_size, 2, activation=torch.nn.ReLU, postprocessing_layer=None).to(device)\n",
    "y_encoder = RNNEncoder(O, y_hidden_size, activation=None).to(device)\n",
    "\n",
    "L = 2                          # number of layers\n",
    "hidden_size = 32\n",
    "backbone = MLP(2*r+x_hidden_size+y_hidden_size, hidden_size, R*K, L, activation=torch.nn.ReLU, postprocessing_layer=None).to(device)\n",
    "#baseline_net = MLP(2*r+x_hidden_size+y_hidden_size, hidden_size, R, L, activation=torch.nn.ReLU, postprocessing_layer=torch.nn.Softplus()).to(device)\n",
    "baseline_net = MLP(x_hidden_size+y_hidden_size, hidden_size, R, L, activation=torch.nn.ReLU, postprocessing_layer=torch.nn.Softplus()).to(device) # without time\n",
    "\n",
    "\n",
    "tau_list = torch.linspace(0, tf, n_X_measurements_between_Y_measurements+2)\n",
    "\n",
    "# n_Y_measurements = 2 # n in the paper\n",
    "# n_X_measurements_between_Y_measurements = 100 # m_bar-2 in the paper\n",
    "\n",
    "measurement_times = torch.linspace(0, tf, n_Y_measurements)\n",
    "tau_times = torch.linspace(0, tf/(n_Y_measurements-1), n_X_measurements_between_Y_measurements+2)\n",
    "\n",
    "# likelihoods and h functions\n",
    "from ElenaLosses import likelihood_GaussianNoise_vmap_compatible\n",
    "\n",
    "def h_fun(x):\n",
    "    out = x[:, 2:3]\n",
    "    #out = x\n",
    "    return out\n",
    "\n",
    "sigma_prior = sigma\n",
    "likelihood = likelihood_GaussianNoise_vmap_compatible\n",
    "likelihood_parameters = {'noise_covariance': (torch.eye(1)*sigma_prior).to(device)}\n",
    "likelihood_parameters[\"noise_covariance_determinant\"] = torch.det(likelihood_parameters[\"noise_covariance\"])\n",
    "likelihood_parameters[\"noise_covariance_inverse\"] = torch.inverse(likelihood_parameters[\"noise_covariance\"])\n",
    "\n",
    "\n",
    "chain = []\n",
    "for i in range(n_Y_measurements-2,-1,-1):\n",
    "    print(i)\n",
    "    if i == n_Y_measurements-2:\n",
    "        chain.append(FilteringDeepCME(deepcopy_flatten(backbone), deepcopy_flatten(x_encoder), deepcopy_flatten(y_encoder), deepcopy_flatten(baseline_net), tau_times, measurement_times, g_functions, deepcopy_flatten(temporal_feature_extractor), R, K, O, position_in_the_chain=i, n_NN_in_chain=n_Y_measurements-1, device=device, h_transform=h_fun, likelihood=likelihood, likelihood_parameters=likelihood_parameters, next_in_chain=None))\n",
    "    else:\n",
    "        chain.append(FilteringDeepCME(deepcopy_flatten(backbone), deepcopy_flatten(x_encoder), deepcopy_flatten(y_encoder), deepcopy_flatten(baseline_net), tau_times, measurement_times, g_functions, deepcopy_flatten(temporal_feature_extractor), R, K, O, position_in_the_chain=i, n_NN_in_chain=n_Y_measurements-1, device=device, h_transform=h_fun, likelihood=likelihood, likelihood_parameters=likelihood_parameters, next_in_chain=chain[-1]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+++++ Training model 1 +++++\n",
      "Training: Epoch 0, batch 0, Loss 0.6695846319198608\n",
      "Training: Epoch 0, batch 1, Loss 0.41962742805480957\n",
      "Training: Epoch 0, batch 2, Loss 0.3291645646095276\n",
      "Training: Epoch 0, batch 3, Loss 0.2355484813451767\n",
      "Validation: Epoch 0, batch 0, Loss 0.17457455396652222\n",
      "Training: Epoch 3, batch 0, Loss 0.1709817796945572\n",
      "Training: Epoch 3, batch 1, Loss 0.1652393341064453\n",
      "Training: Epoch 3, batch 2, Loss 0.1567939817905426\n",
      "Training: Epoch 3, batch 3, Loss 0.15280471742153168\n",
      "Training: Epoch 6, batch 0, Loss 0.15204021334648132\n",
      "Training: Epoch 6, batch 1, Loss 0.15754331648349762\n",
      "Training: Epoch 6, batch 2, Loss 0.15705731511116028\n",
      "Training: Epoch 6, batch 3, Loss 0.16193100810050964\n",
      "Training: Epoch 9, batch 0, Loss 0.1420033574104309\n",
      "Training: Epoch 9, batch 1, Loss 0.1399279087781906\n",
      "Training: Epoch 9, batch 2, Loss 0.14821156859397888\n",
      "Training: Epoch 9, batch 3, Loss 0.15043728053569794\n",
      "Validation: Epoch 10, batch 0, Loss 0.12638738751411438\n",
      "Training: Epoch 12, batch 0, Loss 0.1197681725025177\n",
      "Training: Epoch 12, batch 1, Loss 0.12524640560150146\n",
      "Training: Epoch 12, batch 2, Loss 0.11918476223945618\n",
      "Training: Epoch 12, batch 3, Loss 0.12841710448265076\n",
      "Training: Epoch 15, batch 0, Loss 0.11310791224241257\n",
      "Training: Epoch 15, batch 1, Loss 0.10271760821342468\n",
      "Training: Epoch 15, batch 2, Loss 0.10225095599889755\n",
      "Training: Epoch 15, batch 3, Loss 0.09588940441608429\n",
      "Training: Epoch 18, batch 0, Loss 0.08763524889945984\n",
      "Training: Epoch 18, batch 1, Loss 0.08639390021562576\n",
      "Training: Epoch 18, batch 2, Loss 0.09166707843542099\n",
      "Training: Epoch 18, batch 3, Loss 0.08957637846469879\n",
      "Validation: Epoch 20, batch 0, Loss 0.07386999577283859\n",
      "Training: Epoch 21, batch 0, Loss 0.08073264360427856\n",
      "Training: Epoch 21, batch 1, Loss 0.07308618724346161\n",
      "Training: Epoch 21, batch 2, Loss 0.07761537283658981\n",
      "Training: Epoch 21, batch 3, Loss 0.07784157246351242\n",
      "Training: Epoch 24, batch 0, Loss 0.07550343871116638\n",
      "Training: Epoch 24, batch 1, Loss 0.07058819383382797\n",
      "Training: Epoch 24, batch 2, Loss 0.0649385079741478\n",
      "Training: Epoch 24, batch 3, Loss 0.06431742012500763\n",
      "Training: Epoch 27, batch 0, Loss 0.06283512711524963\n",
      "Training: Epoch 27, batch 1, Loss 0.062459610402584076\n",
      "Training: Epoch 27, batch 2, Loss 0.054799892008304596\n",
      "Training: Epoch 27, batch 3, Loss 0.05592205002903938\n",
      "Training: Epoch 30, batch 0, Loss 0.05654298886656761\n",
      "Training: Epoch 30, batch 1, Loss 0.05196216329932213\n",
      "Training: Epoch 30, batch 2, Loss 0.05218729004263878\n",
      "Training: Epoch 30, batch 3, Loss 0.048998165875673294\n",
      "Validation: Epoch 30, batch 0, Loss 0.04437685385346413\n",
      "Training: Epoch 33, batch 0, Loss 0.04766623303294182\n",
      "Training: Epoch 33, batch 1, Loss 0.03648940473794937\n",
      "Training: Epoch 33, batch 2, Loss 0.04235114902257919\n",
      "Training: Epoch 33, batch 3, Loss 0.04041270911693573\n",
      "Training: Epoch 36, batch 0, Loss 0.02508039027452469\n",
      "Training: Epoch 36, batch 1, Loss 0.025219440460205078\n",
      "Training: Epoch 36, batch 2, Loss 0.02418597601354122\n",
      "Training: Epoch 36, batch 3, Loss 0.025910217314958572\n",
      "Training: Epoch 39, batch 0, Loss 0.016135193407535553\n",
      "Training: Epoch 39, batch 1, Loss 0.018244797363877296\n",
      "Training: Epoch 39, batch 2, Loss 0.01951470784842968\n",
      "Training: Epoch 39, batch 3, Loss 0.027247628197073936\n",
      "Validation: Epoch 40, batch 0, Loss 0.019312581047415733\n",
      "Training: Epoch 42, batch 0, Loss 0.01939072273671627\n",
      "Training: Epoch 42, batch 1, Loss 0.019064582884311676\n",
      "Training: Epoch 42, batch 2, Loss 0.02033412456512451\n",
      "Training: Epoch 42, batch 3, Loss 0.014651323668658733\n",
      "Training: Epoch 45, batch 0, Loss 0.014875644817948341\n",
      "Training: Epoch 45, batch 1, Loss 0.015281451866030693\n",
      "Training: Epoch 45, batch 2, Loss 0.01592240482568741\n",
      "Training: Epoch 45, batch 3, Loss 0.018420662730932236\n",
      "Training: Epoch 48, batch 0, Loss 0.01628980040550232\n",
      "Training: Epoch 48, batch 1, Loss 0.017804745584726334\n",
      "Training: Epoch 48, batch 2, Loss 0.013227451592683792\n",
      "Training: Epoch 48, batch 3, Loss 0.020019393414258957\n",
      "Validation: Epoch 50, batch 0, Loss 0.014245623722672462\n",
      "Training: Epoch 51, batch 0, Loss 0.017459318041801453\n",
      "Training: Epoch 51, batch 1, Loss 0.017533667385578156\n",
      "Training: Epoch 51, batch 2, Loss 0.021730441600084305\n",
      "Training: Epoch 51, batch 3, Loss 0.023373480886220932\n",
      "Training: Epoch 54, batch 0, Loss 0.012622185051441193\n",
      "Training: Epoch 54, batch 1, Loss 0.01529332809150219\n",
      "Training: Epoch 54, batch 2, Loss 0.017379745841026306\n",
      "Training: Epoch 54, batch 3, Loss 0.014927892945706844\n",
      "Training: Epoch 57, batch 0, Loss 0.01441307831555605\n",
      "Training: Epoch 57, batch 1, Loss 0.014205794781446457\n",
      "Training: Epoch 57, batch 2, Loss 0.01169591024518013\n",
      "Training: Epoch 57, batch 3, Loss 0.01154068112373352\n",
      "Training: Epoch 60, batch 0, Loss 0.014481564983725548\n",
      "Training: Epoch 60, batch 1, Loss 0.01046077348291874\n",
      "Training: Epoch 60, batch 2, Loss 0.01189795695245266\n",
      "Training: Epoch 60, batch 3, Loss 0.010295623913407326\n",
      "Validation: Epoch 60, batch 0, Loss 0.010067022405564785\n",
      "Training: Epoch 63, batch 0, Loss 0.011579915881156921\n",
      "Training: Epoch 63, batch 1, Loss 0.009411158971488476\n",
      "Training: Epoch 63, batch 2, Loss 0.012996568344533443\n",
      "Training: Epoch 63, batch 3, Loss 0.01401473768055439\n",
      "Training: Epoch 66, batch 0, Loss 0.012480968609452248\n",
      "Training: Epoch 66, batch 1, Loss 0.009487305767834187\n",
      "Training: Epoch 66, batch 2, Loss 0.010226648300886154\n",
      "Training: Epoch 66, batch 3, Loss 0.012066653929650784\n",
      "Training: Epoch 69, batch 0, Loss 0.010174511931836605\n",
      "Training: Epoch 69, batch 1, Loss 0.009599619545042515\n",
      "Training: Epoch 69, batch 2, Loss 0.010102864354848862\n",
      "Training: Epoch 69, batch 3, Loss 0.006438847165554762\n",
      "Validation: Epoch 70, batch 0, Loss 0.008338223211467266\n",
      "Training: Epoch 72, batch 0, Loss 0.009479214437305927\n",
      "Training: Epoch 72, batch 1, Loss 0.009166036732494831\n",
      "Training: Epoch 72, batch 2, Loss 0.009661433286964893\n",
      "Training: Epoch 72, batch 3, Loss 0.008933580480515957\n",
      "Training: Epoch 75, batch 0, Loss 0.009257279336452484\n",
      "Training: Epoch 75, batch 1, Loss 0.007712226826697588\n",
      "Training: Epoch 75, batch 2, Loss 0.008903236128389835\n",
      "Training: Epoch 75, batch 3, Loss 0.009296286851167679\n",
      "Training: Epoch 78, batch 0, Loss 0.010076497681438923\n",
      "Training: Epoch 78, batch 1, Loss 0.008198130875825882\n",
      "Training: Epoch 78, batch 2, Loss 0.00847772229462862\n",
      "Training: Epoch 78, batch 3, Loss 0.006549740210175514\n",
      "Validation: Epoch 80, batch 0, Loss 0.0076438351534307\n",
      "Training: Epoch 81, batch 0, Loss 0.008206668309867382\n",
      "Training: Epoch 81, batch 1, Loss 0.010621679946780205\n",
      "Training: Epoch 81, batch 2, Loss 0.00901539996266365\n",
      "Training: Epoch 81, batch 3, Loss 0.007344224490225315\n",
      "Training: Epoch 84, batch 0, Loss 0.011193696409463882\n",
      "Training: Epoch 84, batch 1, Loss 0.010353361256420612\n",
      "Training: Epoch 84, batch 2, Loss 0.009438893757760525\n",
      "Training: Epoch 84, batch 3, Loss 0.007293093483895063\n",
      "Training: Epoch 87, batch 0, Loss 0.012836053967475891\n",
      "Training: Epoch 87, batch 1, Loss 0.01183410082012415\n",
      "Training: Epoch 87, batch 2, Loss 0.01618293672800064\n",
      "Training: Epoch 87, batch 3, Loss 0.010334014892578125\n",
      "Training: Epoch 90, batch 0, Loss 0.008783533237874508\n",
      "Training: Epoch 90, batch 1, Loss 0.008518507704138756\n",
      "Training: Epoch 90, batch 2, Loss 0.009535197168588638\n",
      "Training: Epoch 90, batch 3, Loss 0.01004804391413927\n",
      "Validation: Epoch 90, batch 0, Loss 0.009372653439640999\n",
      "Training: Epoch 93, batch 0, Loss 0.00921343732625246\n",
      "Training: Epoch 93, batch 1, Loss 0.007842868566513062\n",
      "Training: Epoch 93, batch 2, Loss 0.009480019100010395\n",
      "Training: Epoch 93, batch 3, Loss 0.006929541472345591\n",
      "Training: Epoch 96, batch 0, Loss 0.007369790691882372\n",
      "Training: Epoch 96, batch 1, Loss 0.006982050370424986\n",
      "Training: Epoch 96, batch 2, Loss 0.0077619049698114395\n",
      "Training: Epoch 96, batch 3, Loss 0.008073684759438038\n",
      "Training: Epoch 99, batch 0, Loss 0.0071218241937458515\n",
      "Training: Epoch 99, batch 1, Loss 0.006951092276722193\n",
      "Training: Epoch 99, batch 2, Loss 0.007673443295061588\n",
      "Training: Epoch 99, batch 3, Loss 0.008473296649754047\n",
      "Validation: Epoch 100, batch 0, Loss 0.00625417148694396\n",
      "Training: Epoch 102, batch 0, Loss 0.006017882842570543\n",
      "Training: Epoch 102, batch 1, Loss 0.00899495929479599\n",
      "Training: Epoch 102, batch 2, Loss 0.006630840711295605\n",
      "Training: Epoch 102, batch 3, Loss 0.005641930270940065\n",
      "Training: Epoch 105, batch 0, Loss 0.006987326312810183\n",
      "Training: Epoch 105, batch 1, Loss 0.00659553799778223\n",
      "Training: Epoch 105, batch 2, Loss 0.007147904485464096\n",
      "Training: Epoch 105, batch 3, Loss 0.00583697110414505\n",
      "Training: Epoch 108, batch 0, Loss 0.006653546821326017\n",
      "Training: Epoch 108, batch 1, Loss 0.006817392539232969\n",
      "Training: Epoch 108, batch 2, Loss 0.006578686181455851\n",
      "Training: Epoch 108, batch 3, Loss 0.005014942027628422\n",
      "Validation: Epoch 110, batch 0, Loss 0.006596371065825224\n",
      "Training: Epoch 111, batch 0, Loss 0.008196568116545677\n",
      "Training: Epoch 111, batch 1, Loss 0.006163871847093105\n",
      "Training: Epoch 111, batch 2, Loss 0.006570706609636545\n",
      "Training: Epoch 111, batch 3, Loss 0.006828201934695244\n",
      "Training: Epoch 114, batch 0, Loss 0.0061925798654556274\n",
      "Training: Epoch 114, batch 1, Loss 0.006701278500258923\n",
      "Training: Epoch 114, batch 2, Loss 0.005880393553525209\n",
      "Training: Epoch 114, batch 3, Loss 0.006871757563203573\n",
      "Training: Epoch 117, batch 0, Loss 0.006111324764788151\n",
      "Training: Epoch 117, batch 1, Loss 0.00661334116011858\n",
      "Training: Epoch 117, batch 2, Loss 0.005390942562371492\n",
      "Training: Epoch 117, batch 3, Loss 0.005341143812984228\n",
      "Training: Epoch 120, batch 0, Loss 0.005395411979407072\n",
      "Training: Epoch 120, batch 1, Loss 0.006340792868286371\n",
      "Training: Epoch 120, batch 2, Loss 0.005803135689347982\n",
      "Training: Epoch 120, batch 3, Loss 0.006402154453098774\n",
      "Validation: Epoch 120, batch 0, Loss 0.004924435634166002\n",
      "Training: Epoch 123, batch 0, Loss 0.006140502169728279\n",
      "Training: Epoch 123, batch 1, Loss 0.006393980234861374\n",
      "Training: Epoch 123, batch 2, Loss 0.007039198186248541\n",
      "Training: Epoch 123, batch 3, Loss 0.0071854619309306145\n",
      "Training: Epoch 126, batch 0, Loss 0.00676971348002553\n",
      "Training: Epoch 126, batch 1, Loss 0.006681925617158413\n",
      "Training: Epoch 126, batch 2, Loss 0.006586970295757055\n",
      "Training: Epoch 126, batch 3, Loss 0.006276784464716911\n",
      "Training: Epoch 129, batch 0, Loss 0.009177472442388535\n",
      "Training: Epoch 129, batch 1, Loss 0.006720817182213068\n",
      "Training: Epoch 129, batch 2, Loss 0.007872923277318478\n",
      "Training: Epoch 129, batch 3, Loss 0.007066512946039438\n",
      "Validation: Epoch 130, batch 0, Loss 0.005099589470773935\n",
      "Training: Epoch 132, batch 0, Loss 0.006596540100872517\n",
      "Training: Epoch 132, batch 1, Loss 0.007602517493069172\n",
      "Training: Epoch 132, batch 2, Loss 0.007589935790747404\n",
      "Training: Epoch 132, batch 3, Loss 0.00626299949362874\n",
      "Training: Epoch 135, batch 0, Loss 0.005601815413683653\n",
      "Training: Epoch 135, batch 1, Loss 0.00592715572565794\n",
      "Training: Epoch 135, batch 2, Loss 0.006338129751384258\n",
      "Training: Epoch 135, batch 3, Loss 0.005757352337241173\n",
      "Training: Epoch 138, batch 0, Loss 0.00604335218667984\n",
      "Training: Epoch 138, batch 1, Loss 0.006203692872077227\n",
      "Training: Epoch 138, batch 2, Loss 0.005249693989753723\n",
      "Training: Epoch 138, batch 3, Loss 0.005748467985540628\n",
      "Validation: Epoch 140, batch 0, Loss 0.004650888033211231\n",
      "Training: Epoch 141, batch 0, Loss 0.0054505812004208565\n",
      "Training: Epoch 141, batch 1, Loss 0.005189943127334118\n",
      "Training: Epoch 141, batch 2, Loss 0.005453262943774462\n",
      "Training: Epoch 141, batch 3, Loss 0.0031895211432129145\n",
      "Training: Epoch 144, batch 0, Loss 0.00465470552444458\n",
      "Training: Epoch 144, batch 1, Loss 0.004975267685949802\n",
      "Training: Epoch 144, batch 2, Loss 0.004688445944339037\n",
      "Training: Epoch 144, batch 3, Loss 0.00645188894122839\n",
      "Training: Epoch 147, batch 0, Loss 0.005097769666463137\n",
      "Training: Epoch 147, batch 1, Loss 0.005062032025307417\n",
      "Training: Epoch 147, batch 2, Loss 0.004295620135962963\n",
      "Training: Epoch 147, batch 3, Loss 0.005832042079418898\n",
      "Training: Epoch 150, batch 0, Loss 0.0052911690436303616\n",
      "Training: Epoch 150, batch 1, Loss 0.004749133717268705\n",
      "Training: Epoch 150, batch 2, Loss 0.003947652410715818\n",
      "Training: Epoch 150, batch 3, Loss 0.004406830295920372\n",
      "Validation: Epoch 150, batch 0, Loss 0.0040365601889789104\n",
      "Training: Epoch 153, batch 0, Loss 0.005007848609238863\n",
      "Training: Epoch 153, batch 1, Loss 0.004299131687730551\n",
      "Training: Epoch 153, batch 2, Loss 0.004455813206732273\n",
      "Training: Epoch 153, batch 3, Loss 0.005976932588964701\n",
      "Training: Epoch 156, batch 0, Loss 0.004456765484064817\n",
      "Training: Epoch 156, batch 1, Loss 0.004932677838951349\n",
      "Training: Epoch 156, batch 2, Loss 0.0045538730919361115\n",
      "Training: Epoch 156, batch 3, Loss 0.007423654664307833\n",
      "Training: Epoch 159, batch 0, Loss 0.004358320496976376\n",
      "Training: Epoch 159, batch 1, Loss 0.004417415242642164\n",
      "Training: Epoch 159, batch 2, Loss 0.004562460817396641\n",
      "Training: Epoch 159, batch 3, Loss 0.0027302156668156385\n",
      "Validation: Epoch 160, batch 0, Loss 0.003979015164077282\n",
      "Training: Epoch 162, batch 0, Loss 0.004279837012290955\n",
      "Training: Epoch 162, batch 1, Loss 0.0048996140249073505\n",
      "Training: Epoch 162, batch 2, Loss 0.003952093422412872\n",
      "Training: Epoch 162, batch 3, Loss 0.0046174004673957825\n",
      "Training: Epoch 165, batch 0, Loss 0.005595594644546509\n",
      "Training: Epoch 165, batch 1, Loss 0.003814320545643568\n",
      "Training: Epoch 165, batch 2, Loss 0.003985249903053045\n",
      "Training: Epoch 165, batch 3, Loss 0.004849694669246674\n",
      "Training: Epoch 168, batch 0, Loss 0.0041467794217169285\n",
      "Training: Epoch 168, batch 1, Loss 0.004158957861363888\n",
      "Training: Epoch 168, batch 2, Loss 0.004572170786559582\n",
      "Training: Epoch 168, batch 3, Loss 0.002943999832496047\n",
      "Validation: Epoch 170, batch 0, Loss 0.003959019668400288\n",
      "Training: Epoch 171, batch 0, Loss 0.004335788078606129\n",
      "Training: Epoch 171, batch 1, Loss 0.00431065121665597\n",
      "Training: Epoch 171, batch 2, Loss 0.004559052176773548\n",
      "Training: Epoch 171, batch 3, Loss 0.004047154448926449\n",
      "Training: Epoch 174, batch 0, Loss 0.004551538731902838\n",
      "Training: Epoch 174, batch 1, Loss 0.004638036247342825\n",
      "Training: Epoch 174, batch 2, Loss 0.003737127408385277\n",
      "Training: Epoch 174, batch 3, Loss 0.0029639643616974354\n",
      "Training: Epoch 177, batch 0, Loss 0.005850032437592745\n",
      "Training: Epoch 177, batch 1, Loss 0.0038410464767366648\n",
      "Training: Epoch 177, batch 2, Loss 0.004319989122450352\n",
      "Training: Epoch 177, batch 3, Loss 0.007406607735902071\n",
      "Training: Epoch 180, batch 0, Loss 0.007802399341017008\n",
      "Training: Epoch 180, batch 1, Loss 0.008288437500596046\n",
      "Training: Epoch 180, batch 2, Loss 0.00806292425841093\n",
      "Training: Epoch 180, batch 3, Loss 0.00794564001262188\n",
      "Validation: Epoch 180, batch 0, Loss 0.005782073829323053\n",
      "Training: Epoch 183, batch 0, Loss 0.01190288458019495\n",
      "Training: Epoch 183, batch 1, Loss 0.0070157949812710285\n",
      "Training: Epoch 183, batch 2, Loss 0.007047404069453478\n",
      "Training: Epoch 183, batch 3, Loss 0.01323890034109354\n",
      "Training: Epoch 186, batch 0, Loss 0.00951758585870266\n",
      "Training: Epoch 186, batch 1, Loss 0.006741211749613285\n",
      "Training: Epoch 186, batch 2, Loss 0.007362611126154661\n",
      "Training: Epoch 186, batch 3, Loss 0.007687148172408342\n",
      "Training: Epoch 189, batch 0, Loss 0.005759137682616711\n",
      "Training: Epoch 189, batch 1, Loss 0.004671513568609953\n",
      "Training: Epoch 189, batch 2, Loss 0.005295911338180304\n",
      "Training: Epoch 189, batch 3, Loss 0.004933612886816263\n",
      "Validation: Epoch 190, batch 0, Loss 0.00365048972889781\n",
      "Training: Epoch 192, batch 0, Loss 0.004425168968737125\n",
      "Training: Epoch 192, batch 1, Loss 0.004071524832397699\n",
      "Training: Epoch 192, batch 2, Loss 0.004647815600037575\n",
      "Training: Epoch 192, batch 3, Loss 0.003593445522710681\n",
      "Training: Epoch 195, batch 0, Loss 0.003570892382413149\n",
      "Training: Epoch 195, batch 1, Loss 0.0042005861178040504\n",
      "Training: Epoch 195, batch 2, Loss 0.0036978384014219046\n",
      "Training: Epoch 195, batch 3, Loss 0.006126685533672571\n",
      "Training: Epoch 198, batch 0, Loss 0.004280672408640385\n",
      "Training: Epoch 198, batch 1, Loss 0.003941794857382774\n",
      "Training: Epoch 198, batch 2, Loss 0.003351315390318632\n",
      "Training: Epoch 198, batch 3, Loss 0.0031241653487086296\n",
      "Validation: Epoch 200, batch 0, Loss 0.0031191122252494097\n",
      "Training: Epoch 201, batch 0, Loss 0.004729701206088066\n",
      "Training: Epoch 201, batch 1, Loss 0.0031335102394223213\n",
      "Training: Epoch 201, batch 2, Loss 0.0034091840498149395\n",
      "Training: Epoch 201, batch 3, Loss 0.0032453318126499653\n",
      "Training: Epoch 204, batch 0, Loss 0.0036779320798814297\n",
      "Training: Epoch 204, batch 1, Loss 0.003375678788870573\n",
      "Training: Epoch 204, batch 2, Loss 0.0038252128288149834\n",
      "Training: Epoch 204, batch 3, Loss 0.004245262127369642\n",
      "Training: Epoch 207, batch 0, Loss 0.0050547169521451\n",
      "Training: Epoch 207, batch 1, Loss 0.0034263813868165016\n",
      "Training: Epoch 207, batch 2, Loss 0.004092681687325239\n",
      "Training: Epoch 207, batch 3, Loss 0.003726306138560176\n",
      "Training: Epoch 210, batch 0, Loss 0.00397845171391964\n",
      "Training: Epoch 210, batch 1, Loss 0.004043090157210827\n",
      "Training: Epoch 210, batch 2, Loss 0.0038363479543477297\n",
      "Training: Epoch 210, batch 3, Loss 0.005978326313197613\n",
      "Validation: Epoch 210, batch 0, Loss 0.003414337057620287\n",
      "Training: Epoch 213, batch 0, Loss 0.00409625843167305\n",
      "Training: Epoch 213, batch 1, Loss 0.0033964253962039948\n",
      "Training: Epoch 213, batch 2, Loss 0.0038326620124280453\n",
      "Training: Epoch 213, batch 3, Loss 0.002738740760833025\n",
      "Training: Epoch 216, batch 0, Loss 0.0042994883842766285\n",
      "Training: Epoch 216, batch 1, Loss 0.007174554746598005\n",
      "Training: Epoch 216, batch 2, Loss 0.005151429679244757\n",
      "Training: Epoch 216, batch 3, Loss 0.004479801282286644\n",
      "Training: Epoch 219, batch 0, Loss 0.006441561970859766\n",
      "Training: Epoch 219, batch 1, Loss 0.005539885722100735\n",
      "Training: Epoch 219, batch 2, Loss 0.005175604484975338\n",
      "Training: Epoch 219, batch 3, Loss 0.007163905072957277\n",
      "Validation: Epoch 220, batch 0, Loss 0.004277701955288649\n",
      "Training: Epoch 222, batch 0, Loss 0.004868537653237581\n",
      "Training: Epoch 222, batch 1, Loss 0.004601920489221811\n",
      "Training: Epoch 222, batch 2, Loss 0.005453378893435001\n",
      "Training: Epoch 222, batch 3, Loss 0.005774015560746193\n",
      "Training: Epoch 225, batch 0, Loss 0.004206371959298849\n",
      "Training: Epoch 225, batch 1, Loss 0.004843382630497217\n",
      "Training: Epoch 225, batch 2, Loss 0.0036571871023625135\n",
      "Training: Epoch 225, batch 3, Loss 0.0030938039999455214\n",
      "Training: Epoch 228, batch 0, Loss 0.003992215730249882\n",
      "Training: Epoch 228, batch 1, Loss 0.005242947023361921\n",
      "Training: Epoch 228, batch 2, Loss 0.004045742563903332\n",
      "Training: Epoch 228, batch 3, Loss 0.003739467589184642\n",
      "Validation: Epoch 230, batch 0, Loss 0.0027555276174098253\n",
      "Training: Epoch 231, batch 0, Loss 0.003771907649934292\n",
      "Training: Epoch 231, batch 1, Loss 0.0035996127407997847\n",
      "Training: Epoch 231, batch 2, Loss 0.003768692957237363\n",
      "Training: Epoch 231, batch 3, Loss 0.0029043478425592184\n",
      "Training: Epoch 234, batch 0, Loss 0.0034778593108057976\n",
      "Training: Epoch 234, batch 1, Loss 0.0038325265049934387\n",
      "Training: Epoch 234, batch 2, Loss 0.0031959505286067724\n",
      "Training: Epoch 234, batch 3, Loss 0.003215154865756631\n",
      "Training: Epoch 237, batch 0, Loss 0.003832892281934619\n",
      "Training: Epoch 237, batch 1, Loss 0.004365125205367804\n",
      "Training: Epoch 237, batch 2, Loss 0.0030240004416555166\n",
      "Training: Epoch 237, batch 3, Loss 0.0034466926008462906\n",
      "Training: Epoch 240, batch 0, Loss 0.0036730198189616203\n",
      "Training: Epoch 240, batch 1, Loss 0.0027028073091059923\n",
      "Training: Epoch 240, batch 2, Loss 0.0034163296222686768\n",
      "Training: Epoch 240, batch 3, Loss 0.003671316895633936\n",
      "Validation: Epoch 240, batch 0, Loss 0.002532714745029807\n",
      "Training: Epoch 243, batch 0, Loss 0.0032900318037718534\n",
      "Training: Epoch 243, batch 1, Loss 0.0027957672718912363\n",
      "Training: Epoch 243, batch 2, Loss 0.0033628465607762337\n",
      "Training: Epoch 243, batch 3, Loss 0.003694087965413928\n",
      "Training: Epoch 246, batch 0, Loss 0.0032222126610577106\n",
      "Training: Epoch 246, batch 1, Loss 0.0028362537268549204\n",
      "Training: Epoch 246, batch 2, Loss 0.002888438757508993\n",
      "Training: Epoch 246, batch 3, Loss 0.005613836459815502\n",
      "Training: Epoch 249, batch 0, Loss 0.003655725624412298\n",
      "Training: Epoch 249, batch 1, Loss 0.0029605659656226635\n",
      "Training: Epoch 249, batch 2, Loss 0.0029097427614033222\n",
      "Training: Epoch 249, batch 3, Loss 0.002559892600402236\n",
      "Validation: Epoch 250, batch 0, Loss 0.002311390358954668\n",
      "Training: Epoch 252, batch 0, Loss 0.003605529433116317\n",
      "Training: Epoch 252, batch 1, Loss 0.0031155592296272516\n",
      "Training: Epoch 252, batch 2, Loss 0.0028640571981668472\n",
      "Training: Epoch 252, batch 3, Loss 0.0021481779403984547\n",
      "Training: Epoch 255, batch 0, Loss 0.003323100972920656\n",
      "Training: Epoch 255, batch 1, Loss 0.0026141400448977947\n",
      "Training: Epoch 255, batch 2, Loss 0.00314984074793756\n",
      "Training: Epoch 255, batch 3, Loss 0.002842135261744261\n",
      "Training: Epoch 258, batch 0, Loss 0.003007805673405528\n",
      "Training: Epoch 258, batch 1, Loss 0.0022570970468223095\n",
      "Training: Epoch 258, batch 2, Loss 0.0032522883266210556\n",
      "Training: Epoch 258, batch 3, Loss 0.0027790640015155077\n",
      "Validation: Epoch 260, batch 0, Loss 0.002106302184984088\n",
      "Training: Epoch 261, batch 0, Loss 0.002728154184296727\n",
      "Training: Epoch 261, batch 1, Loss 0.002767200581729412\n",
      "Training: Epoch 261, batch 2, Loss 0.0030081942677497864\n",
      "Training: Epoch 261, batch 3, Loss 0.002135626506060362\n",
      "Training: Epoch 264, batch 0, Loss 0.003728203708305955\n",
      "Training: Epoch 264, batch 1, Loss 0.00303770718164742\n",
      "Training: Epoch 264, batch 2, Loss 0.0032668597996234894\n",
      "Training: Epoch 264, batch 3, Loss 0.0021098412107676268\n",
      "Training: Epoch 267, batch 0, Loss 0.003115630941465497\n",
      "Training: Epoch 267, batch 1, Loss 0.0020719575695693493\n",
      "Training: Epoch 267, batch 2, Loss 0.0033597471192479134\n",
      "Training: Epoch 267, batch 3, Loss 0.00232061930000782\n",
      "Training: Epoch 270, batch 0, Loss 0.003030919237062335\n",
      "Training: Epoch 270, batch 1, Loss 0.0027022322174161673\n",
      "Training: Epoch 270, batch 2, Loss 0.002327649388462305\n",
      "Training: Epoch 270, batch 3, Loss 0.003554833820089698\n",
      "Validation: Epoch 270, batch 0, Loss 0.002171747852116823\n",
      "Training: Epoch 273, batch 0, Loss 0.0036074919626116753\n",
      "Training: Epoch 273, batch 1, Loss 0.002959362929686904\n",
      "Training: Epoch 273, batch 2, Loss 0.0029522585682570934\n",
      "Training: Epoch 273, batch 3, Loss 0.0029146450106054544\n",
      "Training: Epoch 276, batch 0, Loss 0.0035033232998102903\n",
      "Training: Epoch 276, batch 1, Loss 0.003395303152501583\n",
      "Training: Epoch 276, batch 2, Loss 0.003965166863054037\n",
      "Training: Epoch 276, batch 3, Loss 0.002602756256237626\n",
      "Training: Epoch 279, batch 0, Loss 0.003469172166660428\n",
      "Training: Epoch 279, batch 1, Loss 0.0029974733479321003\n",
      "Training: Epoch 279, batch 2, Loss 0.002536570420488715\n",
      "Training: Epoch 279, batch 3, Loss 0.0029189956840127707\n",
      "Validation: Epoch 280, batch 0, Loss 0.002134425565600395\n",
      "Training: Epoch 282, batch 0, Loss 0.0025189623702317476\n",
      "Training: Epoch 282, batch 1, Loss 0.003095325082540512\n",
      "Training: Epoch 282, batch 2, Loss 0.00257291947491467\n",
      "Training: Epoch 282, batch 3, Loss 0.0031797909177839756\n",
      "Training: Epoch 285, batch 0, Loss 0.00309505476616323\n",
      "Training: Epoch 285, batch 1, Loss 0.002567003946751356\n",
      "Training: Epoch 285, batch 2, Loss 0.002654376905411482\n",
      "Training: Epoch 285, batch 3, Loss 0.0019602791871875525\n",
      "Training: Epoch 288, batch 0, Loss 0.0020418681669980288\n",
      "Training: Epoch 288, batch 1, Loss 0.0025030667893588543\n",
      "Training: Epoch 288, batch 2, Loss 0.0031000105664134026\n",
      "Training: Epoch 288, batch 3, Loss 0.0023368974216282368\n",
      "Validation: Epoch 290, batch 0, Loss 0.0021065149921923876\n",
      "Training: Epoch 291, batch 0, Loss 0.0030545161571353674\n",
      "Training: Epoch 291, batch 1, Loss 0.0023330049589276314\n",
      "Training: Epoch 291, batch 2, Loss 0.002382575301453471\n",
      "Training: Epoch 291, batch 3, Loss 0.0026813007425516844\n",
      "Training: Epoch 294, batch 0, Loss 0.00272887060418725\n",
      "Training: Epoch 294, batch 1, Loss 0.002792760729789734\n",
      "Training: Epoch 294, batch 2, Loss 0.0025633573532104492\n",
      "Training: Epoch 294, batch 3, Loss 0.0027919013518840075\n",
      "Training: Epoch 297, batch 0, Loss 0.0023837219923734665\n",
      "Training: Epoch 297, batch 1, Loss 0.0029359471518546343\n",
      "Training: Epoch 297, batch 2, Loss 0.002187484409660101\n",
      "Training: Epoch 297, batch 3, Loss 0.002551132580265403\n",
      "Training: Epoch 300, batch 0, Loss 0.0021101965103298426\n",
      "Training: Epoch 300, batch 1, Loss 0.0029554797802120447\n",
      "Training: Epoch 300, batch 2, Loss 0.002799546578899026\n",
      "Training: Epoch 300, batch 3, Loss 0.0022776315454393625\n",
      "Validation: Epoch 300, batch 0, Loss 0.001878903480246663\n",
      "Training: Epoch 303, batch 0, Loss 0.0025995741598308086\n",
      "Training: Epoch 303, batch 1, Loss 0.002908913651481271\n",
      "Training: Epoch 303, batch 2, Loss 0.0020629141945391893\n",
      "Training: Epoch 303, batch 3, Loss 0.0034823843743652105\n",
      "Training: Epoch 306, batch 0, Loss 0.0025912642013281584\n",
      "Training: Epoch 306, batch 1, Loss 0.003052206477150321\n",
      "Training: Epoch 306, batch 2, Loss 0.0025679829996079206\n",
      "Training: Epoch 306, batch 3, Loss 0.0019387536449357867\n",
      "Training: Epoch 309, batch 0, Loss 0.0023959740065038204\n",
      "Training: Epoch 309, batch 1, Loss 0.002667359309270978\n",
      "Training: Epoch 309, batch 2, Loss 0.00251502706669271\n",
      "Training: Epoch 309, batch 3, Loss 0.0016223012935370207\n",
      "Validation: Epoch 310, batch 0, Loss 0.0018599864561110735\n",
      "Training: Epoch 312, batch 0, Loss 0.0029842634685337543\n",
      "Training: Epoch 312, batch 1, Loss 0.0022538681514561176\n",
      "Training: Epoch 312, batch 2, Loss 0.0023656892590224743\n",
      "Training: Epoch 312, batch 3, Loss 0.0024226985406130552\n",
      "Training: Epoch 315, batch 0, Loss 0.0030467684846371412\n",
      "Training: Epoch 315, batch 1, Loss 0.0021681522484868765\n",
      "Training: Epoch 315, batch 2, Loss 0.0024721610825508833\n",
      "Training: Epoch 315, batch 3, Loss 0.0019376346608623862\n",
      "Training: Epoch 318, batch 0, Loss 0.003330119652673602\n",
      "Training: Epoch 318, batch 1, Loss 0.0026280335150659084\n",
      "Training: Epoch 318, batch 2, Loss 0.00264533213339746\n",
      "Training: Epoch 318, batch 3, Loss 0.0023838523775339127\n",
      "Validation: Epoch 320, batch 0, Loss 0.002486209385097027\n",
      "Training: Epoch 321, batch 0, Loss 0.0027404336724430323\n",
      "Training: Epoch 321, batch 1, Loss 0.003326574107632041\n",
      "Training: Epoch 321, batch 2, Loss 0.0025348467752337456\n",
      "Training: Epoch 321, batch 3, Loss 0.0024123452603816986\n",
      "Training: Epoch 324, batch 0, Loss 0.0022779726423323154\n",
      "Training: Epoch 324, batch 1, Loss 0.002458730712532997\n",
      "Training: Epoch 324, batch 2, Loss 0.0027864750009030104\n",
      "Training: Epoch 324, batch 3, Loss 0.0022181253880262375\n",
      "Training: Epoch 327, batch 0, Loss 0.0023356424644589424\n",
      "Training: Epoch 327, batch 1, Loss 0.002411323133856058\n",
      "Training: Epoch 327, batch 2, Loss 0.0026283306069672108\n",
      "Training: Epoch 327, batch 3, Loss 0.0021294557955116034\n",
      "Training: Epoch 330, batch 0, Loss 0.0021592325065284967\n",
      "Training: Epoch 330, batch 1, Loss 0.0025413690600544214\n",
      "Training: Epoch 330, batch 2, Loss 0.0021667885594069958\n",
      "Training: Epoch 330, batch 3, Loss 0.00427571777254343\n",
      "Validation: Epoch 330, batch 0, Loss 0.0018539797747507691\n",
      "Training: Epoch 333, batch 0, Loss 0.0020568096078932285\n",
      "Training: Epoch 333, batch 1, Loss 0.002304716734215617\n",
      "Training: Epoch 333, batch 2, Loss 0.0029672570526599884\n",
      "Training: Epoch 333, batch 3, Loss 0.001647606841288507\n",
      "Training: Epoch 336, batch 0, Loss 0.0023839545901864767\n",
      "Training: Epoch 336, batch 1, Loss 0.002508813049644232\n",
      "Training: Epoch 336, batch 2, Loss 0.0023072136100381613\n",
      "Training: Epoch 336, batch 3, Loss 0.00293128308840096\n",
      "Training: Epoch 339, batch 0, Loss 0.002487339312210679\n",
      "Training: Epoch 339, batch 1, Loss 0.0019486843375489116\n",
      "Training: Epoch 339, batch 2, Loss 0.002254595747217536\n",
      "Training: Epoch 339, batch 3, Loss 0.002185009652748704\n",
      "Validation: Epoch 340, batch 0, Loss 0.0019069928675889969\n",
      "Training: Epoch 342, batch 0, Loss 0.002377614611759782\n",
      "Training: Epoch 342, batch 1, Loss 0.002517283195629716\n",
      "Training: Epoch 342, batch 2, Loss 0.002298701787367463\n",
      "Training: Epoch 342, batch 3, Loss 0.002171357860788703\n",
      "Training: Epoch 345, batch 0, Loss 0.002705773338675499\n",
      "Training: Epoch 345, batch 1, Loss 0.002100622048601508\n",
      "Training: Epoch 345, batch 2, Loss 0.002186716767027974\n",
      "Training: Epoch 345, batch 3, Loss 0.001846316852606833\n",
      "Training: Epoch 348, batch 0, Loss 0.0021938320714980364\n",
      "Training: Epoch 348, batch 1, Loss 0.0023533361963927746\n",
      "Training: Epoch 348, batch 2, Loss 0.0021394931245595217\n",
      "Training: Epoch 348, batch 3, Loss 0.0025413965340703726\n",
      "Validation: Epoch 350, batch 0, Loss 0.0017567294416949153\n",
      "Training: Epoch 351, batch 0, Loss 0.002207015873864293\n",
      "Training: Epoch 351, batch 1, Loss 0.002282657427713275\n",
      "Training: Epoch 351, batch 2, Loss 0.002241044072434306\n",
      "Training: Epoch 351, batch 3, Loss 0.001590109197422862\n",
      "Training: Epoch 354, batch 0, Loss 0.0018489122157916427\n",
      "Training: Epoch 354, batch 1, Loss 0.0027173853013664484\n",
      "Training: Epoch 354, batch 2, Loss 0.002181179355829954\n",
      "Training: Epoch 354, batch 3, Loss 0.002095558913424611\n",
      "Training: Epoch 357, batch 0, Loss 0.002181604038923979\n",
      "Training: Epoch 357, batch 1, Loss 0.001958523178473115\n",
      "Training: Epoch 357, batch 2, Loss 0.002297920873388648\n",
      "Training: Epoch 357, batch 3, Loss 0.0019103677477687597\n",
      "Training: Epoch 360, batch 0, Loss 0.0019840458407998085\n",
      "Training: Epoch 360, batch 1, Loss 0.00174569187220186\n",
      "Training: Epoch 360, batch 2, Loss 0.0024514340329915285\n",
      "Training: Epoch 360, batch 3, Loss 0.0021927200723439455\n",
      "Validation: Epoch 360, batch 0, Loss 0.0016645686700940132\n",
      "Training: Epoch 363, batch 0, Loss 0.002396564232185483\n",
      "Training: Epoch 363, batch 1, Loss 0.002548835938796401\n",
      "Training: Epoch 363, batch 2, Loss 0.002063750522211194\n",
      "Training: Epoch 363, batch 3, Loss 0.002020362764596939\n",
      "Training: Epoch 366, batch 0, Loss 0.0020979430992156267\n",
      "Training: Epoch 366, batch 1, Loss 0.0024259306956082582\n",
      "Training: Epoch 366, batch 2, Loss 0.00213370262645185\n",
      "Training: Epoch 366, batch 3, Loss 0.0035625563468784094\n",
      "Training: Epoch 369, batch 0, Loss 0.0020976150408387184\n",
      "Training: Epoch 369, batch 1, Loss 0.0020459084771573544\n",
      "Training: Epoch 369, batch 2, Loss 0.0021509460639208555\n",
      "Training: Epoch 369, batch 3, Loss 0.0014491681940853596\n",
      "Validation: Epoch 370, batch 0, Loss 0.0015217099571600556\n",
      "Training: Epoch 372, batch 0, Loss 0.001929422840476036\n",
      "Training: Epoch 372, batch 1, Loss 0.002364944899454713\n",
      "Training: Epoch 372, batch 2, Loss 0.0019290894269943237\n",
      "Training: Epoch 372, batch 3, Loss 0.0020161226857453585\n",
      "Training: Epoch 375, batch 0, Loss 0.002324273344129324\n",
      "Training: Epoch 375, batch 1, Loss 0.0018589426763355732\n",
      "Training: Epoch 375, batch 2, Loss 0.002131336834281683\n",
      "Training: Epoch 375, batch 3, Loss 0.0016430482501164079\n",
      "Training: Epoch 378, batch 0, Loss 0.002093523507937789\n",
      "Training: Epoch 378, batch 1, Loss 0.001947853248566389\n",
      "Training: Epoch 378, batch 2, Loss 0.0017061684047803283\n",
      "Training: Epoch 378, batch 3, Loss 0.0037738613318651915\n",
      "Validation: Epoch 380, batch 0, Loss 0.001577432150952518\n",
      "Training: Epoch 381, batch 0, Loss 0.0021278681233525276\n",
      "Training: Epoch 381, batch 1, Loss 0.002448192797601223\n",
      "Training: Epoch 381, batch 2, Loss 0.0016209121095016599\n",
      "Training: Epoch 381, batch 3, Loss 0.001996800769120455\n",
      "Training: Epoch 384, batch 0, Loss 0.0020340848714113235\n",
      "Training: Epoch 384, batch 1, Loss 0.0019454953726381063\n",
      "Training: Epoch 384, batch 2, Loss 0.0020936173386871815\n",
      "Training: Epoch 384, batch 3, Loss 0.0015236783074215055\n",
      "Training: Epoch 387, batch 0, Loss 0.002209658036008477\n",
      "Training: Epoch 387, batch 1, Loss 0.0017873469041660428\n",
      "Training: Epoch 387, batch 2, Loss 0.0020499618258327246\n",
      "Training: Epoch 387, batch 3, Loss 0.0020153617952018976\n",
      "Training: Epoch 390, batch 0, Loss 0.0023332717828452587\n",
      "Training: Epoch 390, batch 1, Loss 0.0019909203983843327\n",
      "Training: Epoch 390, batch 2, Loss 0.001974615501239896\n",
      "Training: Epoch 390, batch 3, Loss 0.0018657813780009747\n",
      "Validation: Epoch 390, batch 0, Loss 0.0017214148538187146\n",
      "Training: Epoch 393, batch 0, Loss 0.002528176177293062\n",
      "Training: Epoch 393, batch 1, Loss 0.001985898707062006\n",
      "Training: Epoch 393, batch 2, Loss 0.0017531502526253462\n",
      "Training: Epoch 393, batch 3, Loss 0.0013250275515019894\n",
      "Training: Epoch 396, batch 0, Loss 0.001979536609724164\n",
      "Training: Epoch 396, batch 1, Loss 0.002202484291046858\n",
      "Training: Epoch 396, batch 2, Loss 0.0029728603549301624\n",
      "Training: Epoch 396, batch 3, Loss 0.0030810819007456303\n",
      "Training: Epoch 399, batch 0, Loss 0.002580106258392334\n",
      "Training: Epoch 399, batch 1, Loss 0.0030729903373867273\n",
      "Training: Epoch 399, batch 2, Loss 0.0031910173129290342\n",
      "Training: Epoch 399, batch 3, Loss 0.0020695843268185854\n",
      "Validation: Epoch 400, batch 0, Loss 0.0038207306060940027\n",
      "Training: Epoch 402, batch 0, Loss 0.00443985452875495\n",
      "Training: Epoch 402, batch 1, Loss 0.00357142835855484\n",
      "Training: Epoch 402, batch 2, Loss 0.0029373650904744864\n",
      "Training: Epoch 402, batch 3, Loss 0.001827326719649136\n",
      "Training: Epoch 405, batch 0, Loss 0.003281418699771166\n",
      "Training: Epoch 405, batch 1, Loss 0.0040155076421797276\n",
      "Training: Epoch 405, batch 2, Loss 0.0028500158805400133\n",
      "Training: Epoch 405, batch 3, Loss 0.00354749895632267\n",
      "Training: Epoch 408, batch 0, Loss 0.003583761164918542\n",
      "Training: Epoch 408, batch 1, Loss 0.0027292396407574415\n",
      "Training: Epoch 408, batch 2, Loss 0.0026056794449687004\n",
      "Training: Epoch 408, batch 3, Loss 0.003218893427401781\n",
      "Validation: Epoch 410, batch 0, Loss 0.0021288925781846046\n",
      "Training: Epoch 411, batch 0, Loss 0.0025415646377950907\n",
      "Training: Epoch 411, batch 1, Loss 0.003187209600582719\n",
      "Training: Epoch 411, batch 2, Loss 0.0027016079984605312\n",
      "Training: Epoch 411, batch 3, Loss 0.0019704746082425117\n",
      "Training: Epoch 414, batch 0, Loss 0.0024243853986263275\n",
      "Training: Epoch 414, batch 1, Loss 0.0021638304460793734\n",
      "Training: Epoch 414, batch 2, Loss 0.0023431184235960245\n",
      "Training: Epoch 414, batch 3, Loss 0.002284517977386713\n",
      "Training: Epoch 417, batch 0, Loss 0.0018777326913550496\n",
      "Training: Epoch 417, batch 1, Loss 0.0024767324794083834\n",
      "Training: Epoch 417, batch 2, Loss 0.002065387088805437\n",
      "Training: Epoch 417, batch 3, Loss 0.0016206813743337989\n",
      "Training: Epoch 420, batch 0, Loss 0.002149379113689065\n",
      "Training: Epoch 420, batch 1, Loss 0.0020958806853741407\n",
      "Training: Epoch 420, batch 2, Loss 0.0017988784238696098\n",
      "Training: Epoch 420, batch 3, Loss 0.001739868544973433\n",
      "Validation: Epoch 420, batch 0, Loss 0.0014772143913432956\n",
      "Training: Epoch 423, batch 0, Loss 0.002128873486071825\n",
      "Training: Epoch 423, batch 1, Loss 0.0020487962756305933\n",
      "Training: Epoch 423, batch 2, Loss 0.0018308759899809957\n",
      "Training: Epoch 423, batch 3, Loss 0.001260529737919569\n",
      "Training: Epoch 426, batch 0, Loss 0.001873495988547802\n",
      "Training: Epoch 426, batch 1, Loss 0.002060579601675272\n",
      "Training: Epoch 426, batch 2, Loss 0.0016691336641088128\n",
      "Training: Epoch 426, batch 3, Loss 0.002886794973164797\n",
      "Training: Epoch 429, batch 0, Loss 0.0016932922881096601\n",
      "Training: Epoch 429, batch 1, Loss 0.0018233581213280559\n",
      "Training: Epoch 429, batch 2, Loss 0.0023778050672262907\n",
      "Training: Epoch 429, batch 3, Loss 0.00157854740973562\n",
      "Validation: Epoch 430, batch 0, Loss 0.0015694978646934032\n",
      "Training: Epoch 432, batch 0, Loss 0.002213480183854699\n",
      "Training: Epoch 432, batch 1, Loss 0.0021146656945347786\n",
      "Training: Epoch 432, batch 2, Loss 0.0020524500869214535\n",
      "Training: Epoch 432, batch 3, Loss 0.0017736970912665129\n",
      "Training: Epoch 435, batch 0, Loss 0.0023769461549818516\n",
      "Training: Epoch 435, batch 1, Loss 0.0014962905552238226\n",
      "Training: Epoch 435, batch 2, Loss 0.002464078599587083\n",
      "Training: Epoch 435, batch 3, Loss 0.0018627153476700187\n",
      "Training: Epoch 438, batch 0, Loss 0.0020898261573165655\n",
      "Training: Epoch 438, batch 1, Loss 0.0017967174062505364\n",
      "Training: Epoch 438, batch 2, Loss 0.002296976512297988\n",
      "Training: Epoch 438, batch 3, Loss 0.00241820327937603\n",
      "Validation: Epoch 440, batch 0, Loss 0.002782308030873537\n",
      "Training: Epoch 441, batch 0, Loss 0.0030086254701018333\n",
      "Training: Epoch 441, batch 1, Loss 0.0028125219978392124\n",
      "Training: Epoch 441, batch 2, Loss 0.0030072680674493313\n",
      "Training: Epoch 441, batch 3, Loss 0.003005458740517497\n",
      "Training: Epoch 444, batch 0, Loss 0.021018290892243385\n",
      "Training: Epoch 444, batch 1, Loss 0.013732864521443844\n",
      "Training: Epoch 444, batch 2, Loss 0.02511330507695675\n",
      "Training: Epoch 444, batch 3, Loss 0.016996754333376884\n",
      "Training: Epoch 447, batch 0, Loss 0.011357498355209827\n",
      "Training: Epoch 447, batch 1, Loss 0.015363886952400208\n",
      "Training: Epoch 447, batch 2, Loss 0.01150115579366684\n",
      "Training: Epoch 447, batch 3, Loss 0.01419946551322937\n",
      "Training: Epoch 450, batch 0, Loss 0.008998003788292408\n",
      "Training: Epoch 450, batch 1, Loss 0.006217251997441053\n",
      "Training: Epoch 450, batch 2, Loss 0.006510299164801836\n",
      "Training: Epoch 450, batch 3, Loss 0.00621436582878232\n",
      "Validation: Epoch 450, batch 0, Loss 0.005102050490677357\n",
      "Training: Epoch 453, batch 0, Loss 0.0039260718040168285\n",
      "Training: Epoch 453, batch 1, Loss 0.004016948398202658\n",
      "Training: Epoch 453, batch 2, Loss 0.005213117226958275\n",
      "Training: Epoch 453, batch 3, Loss 0.004934253636747599\n",
      "Training: Epoch 456, batch 0, Loss 0.0029885286930948496\n",
      "Training: Epoch 456, batch 1, Loss 0.0031580429058521986\n",
      "Training: Epoch 456, batch 2, Loss 0.0031184556428343058\n",
      "Training: Epoch 456, batch 3, Loss 0.0038847457617521286\n",
      "Training: Epoch 459, batch 0, Loss 0.0029155556112527847\n",
      "Training: Epoch 459, batch 1, Loss 0.00295831891708076\n",
      "Training: Epoch 459, batch 2, Loss 0.0026394620072096586\n",
      "Training: Epoch 459, batch 3, Loss 0.002244903240352869\n",
      "Validation: Epoch 460, batch 0, Loss 0.002010833704844117\n",
      "Training: Epoch 462, batch 0, Loss 0.002609448740258813\n",
      "Training: Epoch 462, batch 1, Loss 0.002107825595885515\n",
      "Training: Epoch 462, batch 2, Loss 0.0023643651511520147\n",
      "Training: Epoch 462, batch 3, Loss 0.0026974857319146395\n",
      "Training: Epoch 465, batch 0, Loss 0.0022910817060619593\n",
      "Training: Epoch 465, batch 1, Loss 0.0021553218830376863\n",
      "Training: Epoch 465, batch 2, Loss 0.002266043797135353\n",
      "Training: Epoch 465, batch 3, Loss 0.0019016631413251162\n",
      "Training: Epoch 468, batch 0, Loss 0.0028743965085595846\n",
      "Training: Epoch 468, batch 1, Loss 0.0022152613382786512\n",
      "Training: Epoch 468, batch 2, Loss 0.002136212307959795\n",
      "Training: Epoch 468, batch 3, Loss 0.0024386427830904722\n",
      "Validation: Epoch 470, batch 0, Loss 0.0017576842801645398\n",
      "Training: Epoch 471, batch 0, Loss 0.0027194516733288765\n",
      "Training: Epoch 471, batch 1, Loss 0.0018993545090779662\n",
      "Training: Epoch 471, batch 2, Loss 0.002168405568227172\n",
      "Training: Epoch 471, batch 3, Loss 0.0018565021455287933\n",
      "Training: Epoch 474, batch 0, Loss 0.0019535974133759737\n",
      "Training: Epoch 474, batch 1, Loss 0.002229733392596245\n",
      "Training: Epoch 474, batch 2, Loss 0.002291374607011676\n",
      "Training: Epoch 474, batch 3, Loss 0.0019670831970870495\n",
      "Training: Epoch 477, batch 0, Loss 0.002097399439662695\n",
      "Training: Epoch 477, batch 1, Loss 0.0022509335540235043\n",
      "Training: Epoch 477, batch 2, Loss 0.0016753325471654534\n",
      "Training: Epoch 477, batch 3, Loss 0.001910613151267171\n",
      "Training: Epoch 480, batch 0, Loss 0.0019033194985240698\n",
      "Training: Epoch 480, batch 1, Loss 0.00219720252789557\n",
      "Training: Epoch 480, batch 2, Loss 0.0015911295777186751\n",
      "Training: Epoch 480, batch 3, Loss 0.001996667357161641\n",
      "Validation: Epoch 480, batch 0, Loss 0.0014272569678723812\n",
      "Training: Epoch 483, batch 0, Loss 0.0018986592767760158\n",
      "Training: Epoch 483, batch 1, Loss 0.002328714821487665\n",
      "Training: Epoch 483, batch 2, Loss 0.0014480387326329947\n",
      "Training: Epoch 483, batch 3, Loss 0.0013364477781578898\n",
      "Training: Epoch 486, batch 0, Loss 0.0020208877976983786\n",
      "Training: Epoch 486, batch 1, Loss 0.0018679533386602998\n",
      "Training: Epoch 486, batch 2, Loss 0.0015921973390504718\n",
      "Training: Epoch 486, batch 3, Loss 0.0017924804706126451\n",
      "Training: Epoch 489, batch 0, Loss 0.001882158569060266\n",
      "Training: Epoch 489, batch 1, Loss 0.0015580253675580025\n",
      "Training: Epoch 489, batch 2, Loss 0.001689731958322227\n",
      "Training: Epoch 489, batch 3, Loss 0.0025837738066911697\n",
      "Validation: Epoch 490, batch 0, Loss 0.0013237897073850036\n",
      "Training: Epoch 492, batch 0, Loss 0.0018018340924754739\n",
      "Training: Epoch 492, batch 1, Loss 0.002145835431292653\n",
      "Training: Epoch 492, batch 2, Loss 0.0016181282699108124\n",
      "Training: Epoch 492, batch 3, Loss 0.0012186883250251412\n",
      "Training: Epoch 495, batch 0, Loss 0.002062120707705617\n",
      "Training: Epoch 495, batch 1, Loss 0.0017135093221440911\n",
      "Training: Epoch 495, batch 2, Loss 0.0016946751857176423\n",
      "Training: Epoch 495, batch 3, Loss 0.0019460383336991072\n",
      "Training: Epoch 498, batch 0, Loss 0.001943786977790296\n",
      "Training: Epoch 498, batch 1, Loss 0.0018182258354499936\n",
      "Training: Epoch 498, batch 2, Loss 0.001337698195129633\n",
      "Training: Epoch 498, batch 3, Loss 0.0019663344137370586\n",
      "Validation: Epoch 500, batch 0, Loss 0.0013435789151117206\n",
      "Training: Epoch 501, batch 0, Loss 0.0015128852101042867\n",
      "Training: Epoch 501, batch 1, Loss 0.0017956621013581753\n",
      "Training: Epoch 501, batch 2, Loss 0.0020407845731824636\n",
      "Training: Epoch 501, batch 3, Loss 0.0013673879439011216\n",
      "Training: Epoch 504, batch 0, Loss 0.001852795365266502\n",
      "Training: Epoch 504, batch 1, Loss 0.0018103449838235974\n",
      "Training: Epoch 504, batch 2, Loss 0.0019103066297248006\n",
      "Training: Epoch 504, batch 3, Loss 0.0014473233604803681\n",
      "Training: Epoch 507, batch 0, Loss 0.0019767547491937876\n",
      "Training: Epoch 507, batch 1, Loss 0.0017349842237308621\n",
      "Training: Epoch 507, batch 2, Loss 0.0017324419459328055\n",
      "Training: Epoch 507, batch 3, Loss 0.0014547696337103844\n",
      "Training: Epoch 510, batch 0, Loss 0.0018240634817630053\n",
      "Training: Epoch 510, batch 1, Loss 0.002187121193856001\n",
      "Training: Epoch 510, batch 2, Loss 0.001687847077846527\n",
      "Training: Epoch 510, batch 3, Loss 0.0009140904876403511\n",
      "Validation: Epoch 510, batch 0, Loss 0.0015499066794291139\n",
      "Training: Epoch 513, batch 0, Loss 0.0018792269984260201\n",
      "Training: Epoch 513, batch 1, Loss 0.0014310150872915983\n",
      "Training: Epoch 513, batch 2, Loss 0.0020613803062587976\n",
      "Training: Epoch 513, batch 3, Loss 0.0011083526769652963\n",
      "Training: Epoch 516, batch 0, Loss 0.0019460407784208655\n",
      "Training: Epoch 516, batch 1, Loss 0.00158743339125067\n",
      "Training: Epoch 516, batch 2, Loss 0.0016270114574581385\n",
      "Training: Epoch 516, batch 3, Loss 0.001393643906340003\n",
      "Training: Epoch 519, batch 0, Loss 0.002003800356760621\n",
      "Training: Epoch 519, batch 1, Loss 0.0017893373733386397\n",
      "Training: Epoch 519, batch 2, Loss 0.0012914889957755804\n",
      "Training: Epoch 519, batch 3, Loss 0.0011795718455687165\n",
      "Validation: Epoch 520, batch 0, Loss 0.0013325795298442245\n",
      "Training: Epoch 522, batch 0, Loss 0.0018793684430420399\n",
      "Training: Epoch 522, batch 1, Loss 0.001997576327994466\n",
      "Training: Epoch 522, batch 2, Loss 0.0016779607394710183\n",
      "Training: Epoch 522, batch 3, Loss 0.001312542357482016\n",
      "Training: Epoch 525, batch 0, Loss 0.0018547355430200696\n",
      "Training: Epoch 525, batch 1, Loss 0.0020908399019390345\n",
      "Training: Epoch 525, batch 2, Loss 0.0018784719286486506\n",
      "Training: Epoch 525, batch 3, Loss 0.0013385495403781533\n",
      "Training: Epoch 528, batch 0, Loss 0.001903457217849791\n",
      "Training: Epoch 528, batch 1, Loss 0.0015339128440245986\n",
      "Training: Epoch 528, batch 2, Loss 0.0015670259017497301\n",
      "Training: Epoch 528, batch 3, Loss 0.0013150267768651247\n",
      "Validation: Epoch 530, batch 0, Loss 0.001240061828866601\n",
      "Training: Epoch 531, batch 0, Loss 0.001603842480108142\n",
      "Training: Epoch 531, batch 1, Loss 0.0014104533474892378\n",
      "Training: Epoch 531, batch 2, Loss 0.0018144362838938832\n",
      "Training: Epoch 531, batch 3, Loss 0.0017053481424227357\n",
      "Training: Epoch 534, batch 0, Loss 0.0015004531014710665\n",
      "Training: Epoch 534, batch 1, Loss 0.0015760145615786314\n",
      "Training: Epoch 534, batch 2, Loss 0.0017460412345826626\n",
      "Training: Epoch 534, batch 3, Loss 0.0017597401747480035\n",
      "Training: Epoch 537, batch 0, Loss 0.0018717902712523937\n",
      "Training: Epoch 537, batch 1, Loss 0.0020816961769014597\n",
      "Training: Epoch 537, batch 2, Loss 0.0014751623384654522\n",
      "Training: Epoch 537, batch 3, Loss 0.000888693961314857\n",
      "Training: Epoch 540, batch 0, Loss 0.0020435964688658714\n",
      "Training: Epoch 540, batch 1, Loss 0.0015990183455869555\n",
      "Training: Epoch 540, batch 2, Loss 0.002445048885419965\n",
      "Training: Epoch 540, batch 3, Loss 0.0021744021214544773\n",
      "Validation: Epoch 540, batch 0, Loss 0.0015454934909939766\n",
      "Training: Epoch 543, batch 0, Loss 0.0019339348655194044\n",
      "Training: Epoch 543, batch 1, Loss 0.002205803757533431\n",
      "Training: Epoch 543, batch 2, Loss 0.002136241877451539\n",
      "Training: Epoch 543, batch 3, Loss 0.001433700555935502\n",
      "Training: Epoch 546, batch 0, Loss 0.0022020454052835703\n",
      "Training: Epoch 546, batch 1, Loss 0.0019315722165629268\n",
      "Training: Epoch 546, batch 2, Loss 0.001916564186103642\n",
      "Training: Epoch 546, batch 3, Loss 0.002952802926301956\n",
      "Training: Epoch 549, batch 0, Loss 0.0016442324267700315\n",
      "Training: Epoch 549, batch 1, Loss 0.0016921194037422538\n",
      "Training: Epoch 549, batch 2, Loss 0.001876550610177219\n",
      "Training: Epoch 549, batch 3, Loss 0.0012461491860449314\n",
      "Validation: Epoch 550, batch 0, Loss 0.0013262234861031175\n",
      "Training: Epoch 552, batch 0, Loss 0.0018110842211171985\n",
      "Training: Epoch 552, batch 1, Loss 0.001805954147130251\n",
      "Training: Epoch 552, batch 2, Loss 0.0015252619050443172\n",
      "Training: Epoch 552, batch 3, Loss 0.0011082857381552458\n",
      "Training: Epoch 555, batch 0, Loss 0.001410478143952787\n",
      "Training: Epoch 555, batch 1, Loss 0.0018370295874774456\n",
      "Training: Epoch 555, batch 2, Loss 0.0014610138023272157\n",
      "Training: Epoch 555, batch 3, Loss 0.0015623471699655056\n",
      "Training: Epoch 558, batch 0, Loss 0.0014489870518445969\n",
      "Training: Epoch 558, batch 1, Loss 0.0018784162821248174\n",
      "Training: Epoch 558, batch 2, Loss 0.0015611662529408932\n",
      "Training: Epoch 558, batch 3, Loss 0.0018419574480503798\n",
      "Validation: Epoch 560, batch 0, Loss 0.0014908096054568887\n",
      "Training: Epoch 561, batch 0, Loss 0.0018123122863471508\n",
      "Training: Epoch 561, batch 1, Loss 0.0017982234712690115\n",
      "Training: Epoch 561, batch 2, Loss 0.0016611821483820677\n",
      "Training: Epoch 561, batch 3, Loss 0.0011146602919325233\n",
      "Training: Epoch 564, batch 0, Loss 0.0014788568951189518\n",
      "Training: Epoch 564, batch 1, Loss 0.0013998766662552953\n",
      "Training: Epoch 564, batch 2, Loss 0.0017388995038345456\n",
      "Training: Epoch 564, batch 3, Loss 0.0011985799064859748\n",
      "Training: Epoch 567, batch 0, Loss 0.001268723513931036\n",
      "Training: Epoch 567, batch 1, Loss 0.0013631923357024789\n",
      "Training: Epoch 567, batch 2, Loss 0.0014922888949513435\n",
      "Training: Epoch 567, batch 3, Loss 0.0034162718802690506\n",
      "Training: Epoch 570, batch 0, Loss 0.0019059863407164812\n",
      "Training: Epoch 570, batch 1, Loss 0.001747302943840623\n",
      "Training: Epoch 570, batch 2, Loss 0.00128635810688138\n",
      "Training: Epoch 570, batch 3, Loss 0.0008723108912818134\n",
      "Validation: Epoch 570, batch 0, Loss 0.0012806288432329893\n",
      "Training: Epoch 573, batch 0, Loss 0.001778962672688067\n",
      "Training: Epoch 573, batch 1, Loss 0.0015159511240199208\n",
      "Training: Epoch 573, batch 2, Loss 0.0013867287198081613\n",
      "Training: Epoch 573, batch 3, Loss 0.002718510804697871\n",
      "Training: Epoch 576, batch 0, Loss 0.0026040859520435333\n",
      "Training: Epoch 576, batch 1, Loss 0.0024939589202404022\n",
      "Training: Epoch 576, batch 2, Loss 0.0020809515845030546\n",
      "Training: Epoch 576, batch 3, Loss 0.0014600532595068216\n",
      "Training: Epoch 579, batch 0, Loss 0.0019163445103913546\n",
      "Training: Epoch 579, batch 1, Loss 0.002058150712400675\n",
      "Training: Epoch 579, batch 2, Loss 0.0018337852088734508\n",
      "Training: Epoch 579, batch 3, Loss 0.0014396838378161192\n",
      "Validation: Epoch 580, batch 0, Loss 0.0014836591435596347\n",
      "Training: Epoch 582, batch 0, Loss 0.0014332422288134694\n",
      "Training: Epoch 582, batch 1, Loss 0.0021881454158574343\n",
      "Training: Epoch 582, batch 2, Loss 0.001797387725673616\n",
      "Training: Epoch 582, batch 3, Loss 0.0016036746092140675\n",
      "Training: Epoch 585, batch 0, Loss 0.002307712333276868\n",
      "Training: Epoch 585, batch 1, Loss 0.0015198483597487211\n",
      "Training: Epoch 585, batch 2, Loss 0.0014968320028856397\n",
      "Training: Epoch 585, batch 3, Loss 0.0017330279806628823\n",
      "Training: Epoch 588, batch 0, Loss 0.0014698462327942252\n",
      "Training: Epoch 588, batch 1, Loss 0.0014828249113634229\n",
      "Training: Epoch 588, batch 2, Loss 0.00222564279101789\n",
      "Training: Epoch 588, batch 3, Loss 0.001889554550871253\n",
      "Validation: Epoch 590, batch 0, Loss 0.0015753412153571844\n",
      "Training: Epoch 591, batch 0, Loss 0.0017653178656473756\n",
      "Training: Epoch 591, batch 1, Loss 0.001753502176143229\n",
      "Training: Epoch 591, batch 2, Loss 0.002166731283068657\n",
      "Training: Epoch 591, batch 3, Loss 0.001511179143562913\n",
      "Training: Epoch 594, batch 0, Loss 0.0022326915059238672\n",
      "Training: Epoch 594, batch 1, Loss 0.0021068418864160776\n",
      "Training: Epoch 594, batch 2, Loss 0.0017692437395453453\n",
      "Training: Epoch 594, batch 3, Loss 0.0012004859745502472\n",
      "Training: Epoch 597, batch 0, Loss 0.0018670494901016355\n",
      "Training: Epoch 597, batch 1, Loss 0.0019050408154726028\n",
      "Training: Epoch 597, batch 2, Loss 0.002276098821312189\n",
      "Training: Epoch 597, batch 3, Loss 0.0017892338801175356\n",
      "Training: Epoch 600, batch 0, Loss 0.001703851856291294\n",
      "Training: Epoch 600, batch 1, Loss 0.0019164985278621316\n",
      "Training: Epoch 600, batch 2, Loss 0.0015064362669363618\n",
      "Training: Epoch 600, batch 3, Loss 0.0013297712430357933\n",
      "Validation: Epoch 600, batch 0, Loss 0.0012309623416513205\n",
      "Training: Epoch 603, batch 0, Loss 0.0013891611015424132\n",
      "Training: Epoch 603, batch 1, Loss 0.0019625870045274496\n",
      "Training: Epoch 603, batch 2, Loss 0.0014271736145019531\n",
      "Training: Epoch 603, batch 3, Loss 0.0013289867201820016\n",
      "Training: Epoch 606, batch 0, Loss 0.0013712119543924928\n",
      "Training: Epoch 606, batch 1, Loss 0.00133424811065197\n",
      "Training: Epoch 606, batch 2, Loss 0.0018600494368001819\n",
      "Training: Epoch 606, batch 3, Loss 0.0011151653015986085\n",
      "Training: Epoch 609, batch 0, Loss 0.0016641734400764108\n",
      "Training: Epoch 609, batch 1, Loss 0.0014176882104948163\n",
      "Training: Epoch 609, batch 2, Loss 0.0013532914454117417\n",
      "Training: Epoch 609, batch 3, Loss 0.0011396226473152637\n",
      "Validation: Epoch 610, batch 0, Loss 0.0011781692737713456\n",
      "Training: Epoch 612, batch 0, Loss 0.0010917708277702332\n",
      "Training: Epoch 612, batch 1, Loss 0.001216498902067542\n",
      "Training: Epoch 612, batch 2, Loss 0.0019243512069806457\n",
      "Training: Epoch 612, batch 3, Loss 0.0013735716929659247\n",
      "Training: Epoch 615, batch 0, Loss 0.002023656154051423\n",
      "Training: Epoch 615, batch 1, Loss 0.001297496841289103\n",
      "Training: Epoch 615, batch 2, Loss 0.0013378359144553542\n",
      "Training: Epoch 615, batch 3, Loss 0.0008640993037261069\n",
      "Training: Epoch 618, batch 0, Loss 0.0015727353747934103\n",
      "Training: Epoch 618, batch 1, Loss 0.0013544665416702628\n",
      "Training: Epoch 618, batch 2, Loss 0.0013959817588329315\n",
      "Training: Epoch 618, batch 3, Loss 0.0012332173064351082\n",
      "Validation: Epoch 620, batch 0, Loss 0.0011114999651908875\n",
      "Training: Epoch 621, batch 0, Loss 0.001060643931850791\n",
      "Training: Epoch 621, batch 1, Loss 0.0013805684866383672\n",
      "Training: Epoch 621, batch 2, Loss 0.001685901777818799\n",
      "Training: Epoch 621, batch 3, Loss 0.0014657359570264816\n",
      "Training: Epoch 624, batch 0, Loss 0.0013665470760315657\n",
      "Training: Epoch 624, batch 1, Loss 0.0015995041467249393\n",
      "Training: Epoch 624, batch 2, Loss 0.0012994814896956086\n",
      "Training: Epoch 624, batch 3, Loss 0.0013914512237533927\n",
      "Training: Epoch 627, batch 0, Loss 0.0017506636213511229\n",
      "Training: Epoch 627, batch 1, Loss 0.0024770821910351515\n",
      "Training: Epoch 627, batch 2, Loss 0.002945602172985673\n",
      "Training: Epoch 627, batch 3, Loss 0.0014749037800356746\n",
      "Training: Epoch 630, batch 0, Loss 0.0024806619621813297\n",
      "Training: Epoch 630, batch 1, Loss 0.001967282500118017\n",
      "Training: Epoch 630, batch 2, Loss 0.0026083008851855993\n",
      "Training: Epoch 630, batch 3, Loss 0.0033996424172073603\n",
      "Validation: Epoch 630, batch 0, Loss 0.0018124685157090425\n",
      "Training: Epoch 633, batch 0, Loss 0.008475841954350471\n",
      "Training: Epoch 633, batch 1, Loss 0.004264134913682938\n",
      "Training: Epoch 633, batch 2, Loss 0.00584652042016387\n",
      "Training: Epoch 633, batch 3, Loss 0.006748883053660393\n",
      "Training: Epoch 636, batch 0, Loss 0.006166180595755577\n",
      "Training: Epoch 636, batch 1, Loss 0.004818197339773178\n",
      "Training: Epoch 636, batch 2, Loss 0.004816328175365925\n",
      "Training: Epoch 636, batch 3, Loss 0.0042220186442136765\n",
      "Training: Epoch 639, batch 0, Loss 0.0025942842476069927\n",
      "Training: Epoch 639, batch 1, Loss 0.003035878296941519\n",
      "Training: Epoch 639, batch 2, Loss 0.003500057850033045\n",
      "Training: Epoch 639, batch 3, Loss 0.0037375311367213726\n",
      "Validation: Epoch 640, batch 0, Loss 0.0017437362112104893\n",
      "Training: Epoch 642, batch 0, Loss 0.0022114154417067766\n",
      "Training: Epoch 642, batch 1, Loss 0.0021850042976439\n",
      "Training: Epoch 642, batch 2, Loss 0.0025920430198311806\n",
      "Training: Epoch 642, batch 3, Loss 0.0019463510252535343\n",
      "Training: Epoch 645, batch 0, Loss 0.0019507252145558596\n",
      "Training: Epoch 645, batch 1, Loss 0.001991818891838193\n",
      "Training: Epoch 645, batch 2, Loss 0.0016024848446249962\n",
      "Training: Epoch 645, batch 3, Loss 0.00160989910364151\n",
      "Training: Epoch 648, batch 0, Loss 0.00178471056278795\n",
      "Training: Epoch 648, batch 1, Loss 0.0013905587838962674\n",
      "Training: Epoch 648, batch 2, Loss 0.0018946429481729865\n",
      "Training: Epoch 648, batch 3, Loss 0.0018717099446803331\n",
      "Validation: Epoch 650, batch 0, Loss 0.0013236147351562977\n",
      "Training: Epoch 651, batch 0, Loss 0.0015976449940353632\n",
      "Training: Epoch 651, batch 1, Loss 0.0019445095676928759\n",
      "Training: Epoch 651, batch 2, Loss 0.0012679803185164928\n",
      "Training: Epoch 651, batch 3, Loss 0.0019076758762821555\n",
      "Training: Epoch 654, batch 0, Loss 0.0020842424128204584\n",
      "Training: Epoch 654, batch 1, Loss 0.0014006516430526972\n",
      "Training: Epoch 654, batch 2, Loss 0.0013295667013153434\n",
      "Training: Epoch 654, batch 3, Loss 0.0016887784004211426\n",
      "Training: Epoch 657, batch 0, Loss 0.001614399254322052\n",
      "Training: Epoch 657, batch 1, Loss 0.0017608115449547768\n",
      "Training: Epoch 657, batch 2, Loss 0.0014786326792091131\n",
      "Training: Epoch 657, batch 3, Loss 0.0012206248939037323\n",
      "Training: Epoch 660, batch 0, Loss 0.0013851550174877048\n",
      "Training: Epoch 660, batch 1, Loss 0.001589943771250546\n",
      "Training: Epoch 660, batch 2, Loss 0.001305349520407617\n",
      "Training: Epoch 660, batch 3, Loss 0.0023001255467534065\n",
      "Validation: Epoch 660, batch 0, Loss 0.00120945752132684\n",
      "Training: Epoch 663, batch 0, Loss 0.001286342740058899\n",
      "Training: Epoch 663, batch 1, Loss 0.0012593293795362115\n",
      "Training: Epoch 663, batch 2, Loss 0.0016460282495245337\n",
      "Training: Epoch 663, batch 3, Loss 0.001097718719393015\n",
      "Training: Epoch 666, batch 0, Loss 0.0012917503481730819\n",
      "Training: Epoch 666, batch 1, Loss 0.0012351273326203227\n",
      "Training: Epoch 666, batch 2, Loss 0.0015934030525386333\n",
      "Training: Epoch 666, batch 3, Loss 0.0011472445912659168\n",
      "Training: Epoch 669, batch 0, Loss 0.0015053747920319438\n",
      "Training: Epoch 669, batch 1, Loss 0.0013584275729954243\n",
      "Training: Epoch 669, batch 2, Loss 0.0014854123583063483\n",
      "Training: Epoch 669, batch 3, Loss 0.0009395881206728518\n",
      "Validation: Epoch 670, batch 0, Loss 0.0012722802348434925\n",
      "Training: Epoch 672, batch 0, Loss 0.0012959410669282079\n",
      "Training: Epoch 672, batch 1, Loss 0.001574316411279142\n",
      "Training: Epoch 672, batch 2, Loss 0.0014942893758416176\n",
      "Training: Epoch 672, batch 3, Loss 0.0011051167966797948\n",
      "Training: Epoch 675, batch 0, Loss 0.0018599918112158775\n",
      "Training: Epoch 675, batch 1, Loss 0.0014028784353286028\n",
      "Training: Epoch 675, batch 2, Loss 0.001326456549577415\n",
      "Training: Epoch 675, batch 3, Loss 0.0010599230881780386\n",
      "Training: Epoch 678, batch 0, Loss 0.0013135145418345928\n",
      "Training: Epoch 678, batch 1, Loss 0.0011503088753670454\n",
      "Training: Epoch 678, batch 2, Loss 0.0018004983430728316\n",
      "Training: Epoch 678, batch 3, Loss 0.0012465516338124871\n",
      "Validation: Epoch 680, batch 0, Loss 0.001041744719259441\n",
      "Training: Epoch 681, batch 0, Loss 0.0012159592006355524\n",
      "Training: Epoch 681, batch 1, Loss 0.0015848176553845406\n",
      "Training: Epoch 681, batch 2, Loss 0.0013764287577942014\n",
      "Training: Epoch 681, batch 3, Loss 0.0009006288019008934\n",
      "Training: Epoch 684, batch 0, Loss 0.001034051994793117\n",
      "Training: Epoch 684, batch 1, Loss 0.0019063071813434362\n",
      "Training: Epoch 684, batch 2, Loss 0.0015191779239103198\n",
      "Training: Epoch 684, batch 3, Loss 0.0009400726994499564\n",
      "Training: Epoch 687, batch 0, Loss 0.001031906227581203\n",
      "Training: Epoch 687, batch 1, Loss 0.0015188307734206319\n",
      "Training: Epoch 687, batch 2, Loss 0.0015337958466261625\n",
      "Training: Epoch 687, batch 3, Loss 0.0017315634759142995\n",
      "Training: Epoch 690, batch 0, Loss 0.0012188921682536602\n",
      "Training: Epoch 690, batch 1, Loss 0.0016009148675948381\n",
      "Training: Epoch 690, batch 2, Loss 0.0012928606010973454\n",
      "Training: Epoch 690, batch 3, Loss 0.0016791507368907332\n",
      "Validation: Epoch 690, batch 0, Loss 0.0011237961007282138\n",
      "Training: Epoch 693, batch 0, Loss 0.0011178530985489488\n",
      "Training: Epoch 693, batch 1, Loss 0.0013530474388971925\n",
      "Training: Epoch 693, batch 2, Loss 0.0017970192711800337\n",
      "Training: Epoch 693, batch 3, Loss 0.0011290875263512135\n",
      "Training: Epoch 696, batch 0, Loss 0.0012278734939172864\n",
      "Training: Epoch 696, batch 1, Loss 0.0013256954262033105\n",
      "Training: Epoch 696, batch 2, Loss 0.001476400182582438\n",
      "Training: Epoch 696, batch 3, Loss 0.0010833226842805743\n",
      "Training: Epoch 699, batch 0, Loss 0.0015736904460936785\n",
      "Training: Epoch 699, batch 1, Loss 0.0016753643285483122\n",
      "Training: Epoch 699, batch 2, Loss 0.0014113555662333965\n",
      "Training: Epoch 699, batch 3, Loss 0.0017701418837532401\n",
      "Validation: Epoch 700, batch 0, Loss 0.0015576428268104792\n",
      "Training: Epoch 702, batch 0, Loss 0.0013278124388307333\n",
      "Training: Epoch 702, batch 1, Loss 0.0018503202591091394\n",
      "Training: Epoch 702, batch 2, Loss 0.0015680927317589521\n",
      "Training: Epoch 702, batch 3, Loss 0.0011640861630439758\n",
      "Training: Epoch 705, batch 0, Loss 0.0014263300690799952\n",
      "Training: Epoch 705, batch 1, Loss 0.0011376538313925266\n",
      "Training: Epoch 705, batch 2, Loss 0.0019092137226834893\n",
      "Training: Epoch 705, batch 3, Loss 0.0021984975319355726\n",
      "Training: Epoch 708, batch 0, Loss 0.002303075511008501\n",
      "Training: Epoch 708, batch 1, Loss 0.0013580200029537082\n",
      "Training: Epoch 708, batch 2, Loss 0.0015044799074530602\n",
      "Training: Epoch 708, batch 3, Loss 0.001128481701016426\n",
      "Validation: Epoch 710, batch 0, Loss 0.0012564548524096608\n",
      "Training: Epoch 711, batch 0, Loss 0.0013412697007879615\n",
      "Training: Epoch 711, batch 1, Loss 0.0018412390490993857\n",
      "Training: Epoch 711, batch 2, Loss 0.0013836082071065903\n",
      "Training: Epoch 711, batch 3, Loss 0.0014979675179347396\n",
      "Training: Epoch 714, batch 0, Loss 0.0018526929197832942\n",
      "Training: Epoch 714, batch 1, Loss 0.0014234762638807297\n",
      "Training: Epoch 714, batch 2, Loss 0.001445843605324626\n",
      "Training: Epoch 714, batch 3, Loss 0.001249045366421342\n",
      "Training: Epoch 717, batch 0, Loss 0.0012934972764924169\n",
      "Training: Epoch 717, batch 1, Loss 0.0018217979231849313\n",
      "Training: Epoch 717, batch 2, Loss 0.0015334287891164422\n",
      "Training: Epoch 717, batch 3, Loss 0.0014366961549967527\n",
      "Training: Epoch 720, batch 0, Loss 0.0012449701316654682\n",
      "Training: Epoch 720, batch 1, Loss 0.0014898863155394793\n",
      "Training: Epoch 720, batch 2, Loss 0.0017221441958099604\n",
      "Training: Epoch 720, batch 3, Loss 0.001492729177698493\n",
      "Validation: Epoch 720, batch 0, Loss 0.001088944380171597\n",
      "Training: Epoch 723, batch 0, Loss 0.0012277236673980951\n",
      "Training: Epoch 723, batch 1, Loss 0.0013137812493368983\n",
      "Training: Epoch 723, batch 2, Loss 0.0013611589092761278\n",
      "Training: Epoch 723, batch 3, Loss 0.001305255340412259\n",
      "Training: Epoch 726, batch 0, Loss 0.001447573071345687\n",
      "Training: Epoch 726, batch 1, Loss 0.0011245579225942492\n",
      "Training: Epoch 726, batch 2, Loss 0.0013337539276108146\n",
      "Training: Epoch 726, batch 3, Loss 0.0008931379998102784\n",
      "Training: Epoch 729, batch 0, Loss 0.001391987083479762\n",
      "Training: Epoch 729, batch 1, Loss 0.0009771953336894512\n",
      "Training: Epoch 729, batch 2, Loss 0.0014559236587956548\n",
      "Training: Epoch 729, batch 3, Loss 0.0016171610914170742\n",
      "Validation: Epoch 730, batch 0, Loss 0.0010269832564517856\n",
      "Training: Epoch 732, batch 0, Loss 0.0010452067945152521\n",
      "Training: Epoch 732, batch 1, Loss 0.0015341086545959115\n",
      "Training: Epoch 732, batch 2, Loss 0.001631567720323801\n",
      "Training: Epoch 732, batch 3, Loss 0.001049296697601676\n",
      "Training: Epoch 735, batch 0, Loss 0.0014999215491116047\n",
      "Training: Epoch 735, batch 1, Loss 0.0015218777116388083\n",
      "Training: Epoch 735, batch 2, Loss 0.0018674738239496946\n",
      "Training: Epoch 735, batch 3, Loss 0.0016412249533459544\n",
      "Training: Epoch 738, batch 0, Loss 0.0016646659933030605\n",
      "Training: Epoch 738, batch 1, Loss 0.0013400764437392354\n",
      "Training: Epoch 738, batch 2, Loss 0.0018830718472599983\n",
      "Training: Epoch 738, batch 3, Loss 0.0010798927396535873\n",
      "Validation: Epoch 740, batch 0, Loss 0.0014385528629645705\n",
      "Training: Epoch 741, batch 0, Loss 0.0014891814207658172\n",
      "Training: Epoch 741, batch 1, Loss 0.0018448736518621445\n",
      "Training: Epoch 741, batch 2, Loss 0.0015982055338099599\n",
      "Training: Epoch 741, batch 3, Loss 0.0013425919460132718\n",
      "Training: Epoch 744, batch 0, Loss 0.0014850749867036939\n",
      "Training: Epoch 744, batch 1, Loss 0.0016539758071303368\n",
      "Training: Epoch 744, batch 2, Loss 0.0011659784941002727\n",
      "Training: Epoch 744, batch 3, Loss 0.0011443631956353784\n",
      "Training: Epoch 747, batch 0, Loss 0.0013939435593783855\n",
      "Training: Epoch 747, batch 1, Loss 0.0011636655544862151\n",
      "Training: Epoch 747, batch 2, Loss 0.0015207461547106504\n",
      "Training: Epoch 747, batch 3, Loss 0.0013645817525684834\n",
      "Training: Epoch 750, batch 0, Loss 0.0009019317803904414\n",
      "Training: Epoch 750, batch 1, Loss 0.0015668693231418729\n",
      "Training: Epoch 750, batch 2, Loss 0.0011684740893542767\n",
      "Training: Epoch 750, batch 3, Loss 0.001864288467913866\n",
      "Validation: Epoch 750, batch 0, Loss 0.0010321178706362844\n",
      "Training: Epoch 753, batch 0, Loss 0.0015256872866302729\n",
      "Training: Epoch 753, batch 1, Loss 0.0010244606528431177\n",
      "Training: Epoch 753, batch 2, Loss 0.0014210984809324145\n",
      "Training: Epoch 753, batch 3, Loss 0.0012586198281496763\n",
      "Training: Epoch 756, batch 0, Loss 0.001995928818359971\n",
      "Training: Epoch 756, batch 1, Loss 0.0016704766312614083\n",
      "Training: Epoch 756, batch 2, Loss 0.001212870585732162\n",
      "Training: Epoch 756, batch 3, Loss 0.0011908445740118623\n",
      "Training: Epoch 759, batch 0, Loss 0.0017620789585635066\n",
      "Training: Epoch 759, batch 1, Loss 0.0015954001573845744\n",
      "Training: Epoch 759, batch 2, Loss 0.0015366313746199012\n",
      "Training: Epoch 759, batch 3, Loss 0.0010305627947673202\n",
      "Validation: Epoch 760, batch 0, Loss 0.0012929695658385754\n",
      "Training: Epoch 762, batch 0, Loss 0.0013928529806435108\n",
      "Training: Epoch 762, batch 1, Loss 0.0013126481790095568\n",
      "Training: Epoch 762, batch 2, Loss 0.0021454847883433104\n",
      "Training: Epoch 762, batch 3, Loss 0.001274668495170772\n",
      "Training: Epoch 765, batch 0, Loss 0.0013433998683467507\n",
      "Training: Epoch 765, batch 1, Loss 0.0020663656760007143\n",
      "Training: Epoch 765, batch 2, Loss 0.0013158255023881793\n",
      "Training: Epoch 765, batch 3, Loss 0.0011194233084097505\n",
      "Training: Epoch 768, batch 0, Loss 0.0012614044826477766\n",
      "Training: Epoch 768, batch 1, Loss 0.0014450849266722798\n",
      "Training: Epoch 768, batch 2, Loss 0.00174011941999197\n",
      "Training: Epoch 768, batch 3, Loss 0.0007589386659674346\n",
      "Validation: Epoch 770, batch 0, Loss 0.001125528127886355\n",
      "Training: Epoch 771, batch 0, Loss 0.0013368467334657907\n",
      "Training: Epoch 771, batch 1, Loss 0.001824373146519065\n",
      "Training: Epoch 771, batch 2, Loss 0.0016166630666702986\n",
      "Training: Epoch 771, batch 3, Loss 0.0017602411098778248\n",
      "Training: Epoch 774, batch 0, Loss 0.0014505499275401235\n",
      "Training: Epoch 774, batch 1, Loss 0.0021289982832968235\n",
      "Training: Epoch 774, batch 2, Loss 0.0015101643512025476\n",
      "Training: Epoch 774, batch 3, Loss 0.0019950137939304113\n",
      "Training: Epoch 777, batch 0, Loss 0.0013697008835151792\n",
      "Training: Epoch 777, batch 1, Loss 0.0021388614550232887\n",
      "Training: Epoch 777, batch 2, Loss 0.0032832738943398\n",
      "Training: Epoch 777, batch 3, Loss 0.0017596372636035085\n",
      "Training: Epoch 780, batch 0, Loss 0.0020652925595641136\n",
      "Training: Epoch 780, batch 1, Loss 0.004465670790523291\n",
      "Training: Epoch 780, batch 2, Loss 0.0024456805549561977\n",
      "Training: Epoch 780, batch 3, Loss 0.0018137809820473194\n",
      "Validation: Epoch 780, batch 0, Loss 0.0026992287021130323\n",
      "Training: Epoch 783, batch 0, Loss 0.0021204212680459023\n",
      "Training: Epoch 783, batch 1, Loss 0.002497832989320159\n",
      "Training: Epoch 783, batch 2, Loss 0.0017124071018770337\n",
      "Training: Epoch 783, batch 3, Loss 0.0018453377997502685\n",
      "Training: Epoch 786, batch 0, Loss 0.0020931530743837357\n",
      "Training: Epoch 786, batch 1, Loss 0.001182490959763527\n",
      "Training: Epoch 786, batch 2, Loss 0.0020252130925655365\n",
      "Training: Epoch 786, batch 3, Loss 0.0015777652151882648\n",
      "Training: Epoch 789, batch 0, Loss 0.0015219911001622677\n",
      "Training: Epoch 789, batch 1, Loss 0.0013717878609895706\n",
      "Training: Epoch 789, batch 2, Loss 0.0014705348294228315\n",
      "Training: Epoch 789, batch 3, Loss 0.003268384374678135\n",
      "Validation: Epoch 790, batch 0, Loss 0.001211270340718329\n",
      "Training: Epoch 792, batch 0, Loss 0.0016137714264914393\n",
      "Training: Epoch 792, batch 1, Loss 0.0013487397227436304\n",
      "Training: Epoch 792, batch 2, Loss 0.0013444582000374794\n",
      "Training: Epoch 792, batch 3, Loss 0.0028393224347382784\n",
      "Training: Epoch 795, batch 0, Loss 0.001340484363026917\n",
      "Training: Epoch 795, batch 1, Loss 0.0017231894889846444\n",
      "Training: Epoch 795, batch 2, Loss 0.0015913562383502722\n",
      "Training: Epoch 795, batch 3, Loss 0.0010982663370668888\n",
      "Training: Epoch 798, batch 0, Loss 0.0017498442903161049\n",
      "Training: Epoch 798, batch 1, Loss 0.0012632047291845083\n",
      "Training: Epoch 798, batch 2, Loss 0.0014080889523029327\n",
      "Training: Epoch 798, batch 3, Loss 0.0010889237746596336\n",
      "Validation: Epoch 800, batch 0, Loss 0.001022221869789064\n",
      "Training: Epoch 801, batch 0, Loss 0.0013331284280866385\n",
      "Training: Epoch 801, batch 1, Loss 0.0010525700636208057\n",
      "Training: Epoch 801, batch 2, Loss 0.0015888054622337222\n",
      "Training: Epoch 801, batch 3, Loss 0.0010961799416691065\n",
      "Training: Epoch 804, batch 0, Loss 0.0012920093722641468\n",
      "Training: Epoch 804, batch 1, Loss 0.0014637613203376532\n",
      "Training: Epoch 804, batch 2, Loss 0.0011467714793980122\n",
      "Training: Epoch 804, batch 3, Loss 0.0010055629536509514\n",
      "Training: Epoch 807, batch 0, Loss 0.0015180216869339347\n",
      "Training: Epoch 807, batch 1, Loss 0.0012652077712118626\n",
      "Training: Epoch 807, batch 2, Loss 0.0011756212916225195\n",
      "Training: Epoch 807, batch 3, Loss 0.0010623445268720388\n",
      "Training: Epoch 810, batch 0, Loss 0.0012432305375114083\n",
      "Training: Epoch 810, batch 1, Loss 0.0017103354912251234\n",
      "Training: Epoch 810, batch 2, Loss 0.0010606225114315748\n",
      "Training: Epoch 810, batch 3, Loss 0.001656585605815053\n",
      "Validation: Epoch 810, batch 0, Loss 0.0010351546807214618\n",
      "Training: Epoch 813, batch 0, Loss 0.0010518085910007358\n",
      "Training: Epoch 813, batch 1, Loss 0.0016654118662700057\n",
      "Training: Epoch 813, batch 2, Loss 0.0012903657043352723\n",
      "Training: Epoch 813, batch 3, Loss 0.0012326078722253442\n",
      "Training: Epoch 816, batch 0, Loss 0.001835540751926601\n",
      "Training: Epoch 816, batch 1, Loss 0.0015615089796483517\n",
      "Training: Epoch 816, batch 2, Loss 0.0011768098920583725\n",
      "Training: Epoch 816, batch 3, Loss 0.0012000456918030977\n",
      "Training: Epoch 819, batch 0, Loss 0.0013248256873339415\n",
      "Training: Epoch 819, batch 1, Loss 0.0017151667270809412\n",
      "Training: Epoch 819, batch 2, Loss 0.0011409997241571546\n",
      "Training: Epoch 819, batch 3, Loss 0.0011944188736379147\n",
      "Validation: Epoch 820, batch 0, Loss 0.0012475873809307814\n",
      "Training: Epoch 822, batch 0, Loss 0.001203841296955943\n",
      "Training: Epoch 822, batch 1, Loss 0.0017655810806900263\n",
      "Training: Epoch 822, batch 2, Loss 0.0011184524046257138\n",
      "Training: Epoch 822, batch 3, Loss 0.0029253829270601273\n",
      "Training: Epoch 825, batch 0, Loss 0.0017198848072439432\n",
      "Training: Epoch 825, batch 1, Loss 0.0014976701932027936\n",
      "Training: Epoch 825, batch 2, Loss 0.0012593161081895232\n",
      "Training: Epoch 825, batch 3, Loss 0.0013192495098337531\n",
      "Training: Epoch 828, batch 0, Loss 0.0013480244670063257\n",
      "Training: Epoch 828, batch 1, Loss 0.0015330222668126225\n",
      "Training: Epoch 828, batch 2, Loss 0.0011546897003427148\n",
      "Training: Epoch 828, batch 3, Loss 0.0014099011896178126\n",
      "Validation: Epoch 830, batch 0, Loss 0.0009751184261403978\n",
      "Training: Epoch 831, batch 0, Loss 0.0014007727149873972\n",
      "Training: Epoch 831, batch 1, Loss 0.0014002348762005568\n",
      "Training: Epoch 831, batch 2, Loss 0.0010018189204856753\n",
      "Training: Epoch 831, batch 3, Loss 0.0010166355641558766\n",
      "Training: Epoch 834, batch 0, Loss 0.0013677631504833698\n",
      "Training: Epoch 834, batch 1, Loss 0.0014145246241241693\n",
      "Training: Epoch 834, batch 2, Loss 0.0011288676178082824\n",
      "Training: Epoch 834, batch 3, Loss 0.0009532466647215188\n",
      "Training: Epoch 837, batch 0, Loss 0.001268585678189993\n",
      "Training: Epoch 837, batch 1, Loss 0.0013907175743952394\n",
      "Training: Epoch 837, batch 2, Loss 0.0016403623158112168\n",
      "Training: Epoch 837, batch 3, Loss 0.0009130108519457281\n",
      "Training: Epoch 840, batch 0, Loss 0.0019085239619016647\n",
      "Training: Epoch 840, batch 1, Loss 0.0017214077524840832\n",
      "Training: Epoch 840, batch 2, Loss 0.002750241896137595\n",
      "Training: Epoch 840, batch 3, Loss 0.0022656028158962727\n",
      "Validation: Epoch 840, batch 0, Loss 0.0017872878815978765\n",
      "Training: Epoch 843, batch 0, Loss 0.0033539480064064264\n",
      "Training: Epoch 843, batch 1, Loss 0.0017279766034334898\n",
      "Training: Epoch 843, batch 2, Loss 0.002154022455215454\n",
      "Training: Epoch 843, batch 3, Loss 0.0022776403930038214\n",
      "Training: Epoch 846, batch 0, Loss 0.0023864225950092077\n",
      "Training: Epoch 846, batch 1, Loss 0.0018346232827752829\n",
      "Training: Epoch 846, batch 2, Loss 0.0018026625039055943\n",
      "Training: Epoch 846, batch 3, Loss 0.0016859557945281267\n",
      "Training: Epoch 849, batch 0, Loss 0.0015766110736876726\n",
      "Training: Epoch 849, batch 1, Loss 0.0019510224228724837\n",
      "Training: Epoch 849, batch 2, Loss 0.001416931045241654\n",
      "Training: Epoch 849, batch 3, Loss 0.0009926360798999667\n",
      "Validation: Epoch 850, batch 0, Loss 0.001292167347855866\n",
      "Training: Epoch 852, batch 0, Loss 0.00144222064409405\n",
      "Training: Epoch 852, batch 1, Loss 0.0015795868821442127\n",
      "Training: Epoch 852, batch 2, Loss 0.0013411566615104675\n",
      "Training: Epoch 852, batch 3, Loss 0.0010233825305476785\n",
      "Training: Epoch 855, batch 0, Loss 0.0016353573882952332\n",
      "Training: Epoch 855, batch 1, Loss 0.0011253615375608206\n",
      "Training: Epoch 855, batch 2, Loss 0.0014925592113286257\n",
      "Training: Epoch 855, batch 3, Loss 0.00123332510702312\n",
      "Training: Epoch 858, batch 0, Loss 0.0015983685152605176\n",
      "Training: Epoch 858, batch 1, Loss 0.0009837034158408642\n",
      "Training: Epoch 858, batch 2, Loss 0.0012122590560466051\n",
      "Training: Epoch 858, batch 3, Loss 0.0014525811420753598\n",
      "Validation: Epoch 860, batch 0, Loss 0.0009172988939099014\n",
      "Training: Epoch 861, batch 0, Loss 0.001027046819217503\n",
      "Training: Epoch 861, batch 1, Loss 0.0012163447681814432\n",
      "Training: Epoch 861, batch 2, Loss 0.0009940762538462877\n",
      "Training: Epoch 861, batch 3, Loss 0.0031132951844483614\n",
      "Training: Epoch 864, batch 0, Loss 0.0015375607181340456\n",
      "Training: Epoch 864, batch 1, Loss 0.0013553122989833355\n",
      "Training: Epoch 864, batch 2, Loss 0.00163889245595783\n",
      "Training: Epoch 864, batch 3, Loss 0.0006824368028901517\n",
      "Training: Epoch 867, batch 0, Loss 0.0011540658306330442\n",
      "Training: Epoch 867, batch 1, Loss 0.0012572866398841143\n",
      "Training: Epoch 867, batch 2, Loss 0.0017258464358747005\n",
      "Training: Epoch 867, batch 3, Loss 0.0012396478559821844\n",
      "Training: Epoch 870, batch 0, Loss 0.0013732945080846548\n",
      "Training: Epoch 870, batch 1, Loss 0.001243788399733603\n",
      "Training: Epoch 870, batch 2, Loss 0.0016039267648011446\n",
      "Training: Epoch 870, batch 3, Loss 0.0010338155552744865\n",
      "Validation: Epoch 870, batch 0, Loss 0.0010385699570178986\n",
      "Training: Epoch 873, batch 0, Loss 0.0017506747972220182\n",
      "Training: Epoch 873, batch 1, Loss 0.001101515837945044\n",
      "Training: Epoch 873, batch 2, Loss 0.0012521452736109495\n",
      "Training: Epoch 873, batch 3, Loss 0.0012671146541833878\n",
      "Training: Epoch 876, batch 0, Loss 0.0016676364466547966\n",
      "Training: Epoch 876, batch 1, Loss 0.0014998730039224029\n",
      "Training: Epoch 876, batch 2, Loss 0.0018388023599982262\n",
      "Training: Epoch 876, batch 3, Loss 0.0010625950526446104\n",
      "Training: Epoch 879, batch 0, Loss 0.0016746012261137366\n",
      "Training: Epoch 879, batch 1, Loss 0.0015206912066787481\n",
      "Training: Epoch 879, batch 2, Loss 0.0012598576722666621\n",
      "Training: Epoch 879, batch 3, Loss 0.001327836187556386\n",
      "Validation: Epoch 880, batch 0, Loss 0.0012902595335617661\n",
      "Training: Epoch 882, batch 0, Loss 0.0013715539826080203\n",
      "Training: Epoch 882, batch 1, Loss 0.0014779591001570225\n",
      "Training: Epoch 882, batch 2, Loss 0.0014303107745945454\n",
      "Training: Epoch 882, batch 3, Loss 0.0009728095028549433\n",
      "Training: Epoch 885, batch 0, Loss 0.0016972218872979283\n",
      "Training: Epoch 885, batch 1, Loss 0.0013666688464581966\n",
      "Training: Epoch 885, batch 2, Loss 0.001072671846486628\n",
      "Training: Epoch 885, batch 3, Loss 0.0009947291109710932\n",
      "Training: Epoch 888, batch 0, Loss 0.0010023383656516671\n",
      "Training: Epoch 888, batch 1, Loss 0.0013079403433948755\n",
      "Training: Epoch 888, batch 2, Loss 0.0015003366861492395\n",
      "Training: Epoch 888, batch 3, Loss 0.0013509087730199099\n",
      "Validation: Epoch 890, batch 0, Loss 0.0011029031593352556\n",
      "Training: Epoch 891, batch 0, Loss 0.0012265527620911598\n",
      "Training: Epoch 891, batch 1, Loss 0.0014722658088430762\n",
      "Training: Epoch 891, batch 2, Loss 0.0013425903161987662\n",
      "Training: Epoch 891, batch 3, Loss 0.0008657705038785934\n",
      "Training: Epoch 894, batch 0, Loss 0.0014366377145051956\n",
      "Training: Epoch 894, batch 1, Loss 0.0012465778272598982\n",
      "Training: Epoch 894, batch 2, Loss 0.001010329695418477\n",
      "Training: Epoch 894, batch 3, Loss 0.0006782570853829384\n",
      "Training: Epoch 897, batch 0, Loss 0.0011862674728035927\n",
      "Training: Epoch 897, batch 1, Loss 0.0012526363134384155\n",
      "Training: Epoch 897, batch 2, Loss 0.0015525645576417446\n",
      "Training: Epoch 897, batch 3, Loss 0.0009968477534130216\n",
      "Training: Epoch 900, batch 0, Loss 0.0010296670952811837\n",
      "Training: Epoch 900, batch 1, Loss 0.0011362854856997728\n",
      "Training: Epoch 900, batch 2, Loss 0.001475909142754972\n",
      "Training: Epoch 900, batch 3, Loss 0.0008777674520388246\n",
      "Validation: Epoch 900, batch 0, Loss 0.0008932046475820243\n",
      "Training: Epoch 903, batch 0, Loss 0.0011176386615261436\n",
      "Training: Epoch 903, batch 1, Loss 0.0015372089110314846\n",
      "Training: Epoch 903, batch 2, Loss 0.001093328115530312\n",
      "Training: Epoch 903, batch 3, Loss 0.0012383408611640334\n",
      "Training: Epoch 906, batch 0, Loss 0.0016454298747703433\n",
      "Training: Epoch 906, batch 1, Loss 0.0011529159964993596\n",
      "Training: Epoch 906, batch 2, Loss 0.0010744250612333417\n",
      "Training: Epoch 906, batch 3, Loss 0.0008817437919788063\n",
      "Training: Epoch 909, batch 0, Loss 0.0014060849789530039\n",
      "Training: Epoch 909, batch 1, Loss 0.0010235334048047662\n",
      "Training: Epoch 909, batch 2, Loss 0.001464937231503427\n",
      "Training: Epoch 909, batch 3, Loss 0.0012073899852111936\n",
      "Validation: Epoch 910, batch 0, Loss 0.0011485246941447258\n",
      "Training: Epoch 912, batch 0, Loss 0.0010998374782502651\n",
      "Training: Epoch 912, batch 1, Loss 0.001923356088809669\n",
      "Training: Epoch 912, batch 2, Loss 0.0013748544733971357\n",
      "Training: Epoch 912, batch 3, Loss 0.0008837567875161767\n",
      "Training: Epoch 915, batch 0, Loss 0.0012452075025066733\n",
      "Training: Epoch 915, batch 1, Loss 0.0016882340423762798\n",
      "Training: Epoch 915, batch 2, Loss 0.001205623266287148\n",
      "Training: Epoch 915, batch 3, Loss 0.0008114328375086188\n",
      "Training: Epoch 918, batch 0, Loss 0.0012147751403972507\n",
      "Training: Epoch 918, batch 1, Loss 0.0016157968202605844\n",
      "Training: Epoch 918, batch 2, Loss 0.0014481662074103951\n",
      "Training: Epoch 918, batch 3, Loss 0.0010843871859833598\n",
      "Validation: Epoch 920, batch 0, Loss 0.0009800122352316976\n",
      "Training: Epoch 921, batch 0, Loss 0.0012242331868037581\n",
      "Training: Epoch 921, batch 1, Loss 0.0012300207745283842\n",
      "Training: Epoch 921, batch 2, Loss 0.0013651479966938496\n",
      "Training: Epoch 921, batch 3, Loss 0.0013538249768316746\n",
      "Training: Epoch 924, batch 0, Loss 0.0026309541426599026\n",
      "Training: Epoch 924, batch 1, Loss 0.0026286370120942593\n",
      "Training: Epoch 924, batch 2, Loss 0.003035610308870673\n",
      "Training: Epoch 924, batch 3, Loss 0.002337685553357005\n",
      "Training: Epoch 927, batch 0, Loss 0.0019228667952120304\n",
      "Training: Epoch 927, batch 1, Loss 0.002499906811863184\n",
      "Training: Epoch 927, batch 2, Loss 0.0014979856787249446\n",
      "Training: Epoch 927, batch 3, Loss 0.0012299134396016598\n",
      "Training: Epoch 930, batch 0, Loss 0.0015037927078083158\n",
      "Training: Epoch 930, batch 1, Loss 0.0026422617956995964\n",
      "Training: Epoch 930, batch 2, Loss 0.0016786940395832062\n",
      "Training: Epoch 930, batch 3, Loss 0.0012677271151915193\n",
      "Validation: Epoch 930, batch 0, Loss 0.0015668561682105064\n",
      "Training: Epoch 933, batch 0, Loss 0.0014707669615745544\n",
      "Training: Epoch 933, batch 1, Loss 0.001624902943149209\n",
      "Training: Epoch 933, batch 2, Loss 0.0015185527736321092\n",
      "Training: Epoch 933, batch 3, Loss 0.0013795383274555206\n",
      "Training: Epoch 936, batch 0, Loss 0.0014358086045831442\n",
      "Training: Epoch 936, batch 1, Loss 0.0015136308502405882\n",
      "Training: Epoch 936, batch 2, Loss 0.0013288285117596388\n",
      "Training: Epoch 936, batch 3, Loss 0.0013016784796491265\n",
      "Training: Epoch 939, batch 0, Loss 0.0017393403686583042\n",
      "Training: Epoch 939, batch 1, Loss 0.0013101774966344237\n",
      "Training: Epoch 939, batch 2, Loss 0.0011214528931304812\n",
      "Training: Epoch 939, batch 3, Loss 0.0008216352434828877\n",
      "Validation: Epoch 940, batch 0, Loss 0.001011332729831338\n",
      "Training: Epoch 942, batch 0, Loss 0.0012418307596817613\n",
      "Training: Epoch 942, batch 1, Loss 0.0015612442512065172\n",
      "Training: Epoch 942, batch 2, Loss 0.001432832796126604\n",
      "Training: Epoch 942, batch 3, Loss 0.0011151634389534593\n",
      "Training: Epoch 945, batch 0, Loss 0.00123539415653795\n",
      "Training: Epoch 945, batch 1, Loss 0.0013063212390989065\n",
      "Training: Epoch 945, batch 2, Loss 0.0014400758082047105\n",
      "Training: Epoch 945, batch 3, Loss 0.0029353159479796886\n",
      "Training: Epoch 948, batch 0, Loss 0.0014317116001620889\n",
      "Training: Epoch 948, batch 1, Loss 0.0010269081685692072\n",
      "Training: Epoch 948, batch 2, Loss 0.001572020468302071\n",
      "Training: Epoch 948, batch 3, Loss 0.0009340417454950511\n",
      "Validation: Epoch 950, batch 0, Loss 0.001123058726079762\n",
      "Training: Epoch 951, batch 0, Loss 0.0013099249918013811\n",
      "Training: Epoch 951, batch 1, Loss 0.001404356095008552\n",
      "Training: Epoch 951, batch 2, Loss 0.0015127959195524454\n",
      "Training: Epoch 951, batch 3, Loss 0.0012501012533903122\n",
      "Training: Epoch 954, batch 0, Loss 0.0015217470936477184\n",
      "Training: Epoch 954, batch 1, Loss 0.001180558349005878\n",
      "Training: Epoch 954, batch 2, Loss 0.001154090161435306\n",
      "Training: Epoch 954, batch 3, Loss 0.0007749474607408047\n",
      "Training: Epoch 957, batch 0, Loss 0.001576725859194994\n",
      "Training: Epoch 957, batch 1, Loss 0.0012537383008748293\n",
      "Training: Epoch 957, batch 2, Loss 0.0011435213964432478\n",
      "Training: Epoch 957, batch 3, Loss 0.0010995634365826845\n",
      "Training: Epoch 960, batch 0, Loss 0.0017806378891691566\n",
      "Training: Epoch 960, batch 1, Loss 0.001262224861420691\n",
      "Training: Epoch 960, batch 2, Loss 0.0009963099146261811\n",
      "Training: Epoch 960, batch 3, Loss 0.0012393547222018242\n",
      "Validation: Epoch 960, batch 0, Loss 0.0011482308618724346\n",
      "Training: Epoch 963, batch 0, Loss 0.0019200973911210895\n",
      "Training: Epoch 963, batch 1, Loss 0.0018975979182869196\n",
      "Training: Epoch 963, batch 2, Loss 0.0020490395836532116\n",
      "Training: Epoch 963, batch 3, Loss 0.0012479351134970784\n",
      "Training: Epoch 966, batch 0, Loss 0.0018840787233784795\n",
      "Training: Epoch 966, batch 1, Loss 0.0018236894393339753\n",
      "Training: Epoch 966, batch 2, Loss 0.0016673244535923004\n",
      "Training: Epoch 966, batch 3, Loss 0.0020833418238908052\n",
      "Training: Epoch 969, batch 0, Loss 0.0014761568745598197\n",
      "Training: Epoch 969, batch 1, Loss 0.0015370446490123868\n",
      "Training: Epoch 969, batch 2, Loss 0.0010706022148951888\n",
      "Training: Epoch 969, batch 3, Loss 0.0012099824380129576\n",
      "Validation: Epoch 970, batch 0, Loss 0.0010239304974675179\n",
      "Training: Epoch 972, batch 0, Loss 0.0016217624070122838\n",
      "Training: Epoch 972, batch 1, Loss 0.0015331038739532232\n",
      "Training: Epoch 972, batch 2, Loss 0.0012379443505778909\n",
      "Training: Epoch 972, batch 3, Loss 0.0012128411326557398\n",
      "Training: Epoch 975, batch 0, Loss 0.0013352826936170459\n",
      "Training: Epoch 975, batch 1, Loss 0.001121259992942214\n",
      "Training: Epoch 975, batch 2, Loss 0.0014589159982278943\n",
      "Training: Epoch 975, batch 3, Loss 0.001558450749143958\n",
      "Training: Epoch 978, batch 0, Loss 0.0014422798994928598\n",
      "Training: Epoch 978, batch 1, Loss 0.00090106698917225\n",
      "Training: Epoch 978, batch 2, Loss 0.001222858321852982\n",
      "Training: Epoch 978, batch 3, Loss 0.0011485920986160636\n",
      "Validation: Epoch 980, batch 0, Loss 0.0010925537208095193\n",
      "Training: Epoch 981, batch 0, Loss 0.0012008692137897015\n",
      "Training: Epoch 981, batch 1, Loss 0.0012124450877308846\n",
      "Training: Epoch 981, batch 2, Loss 0.0009854714153334498\n",
      "Training: Epoch 981, batch 3, Loss 0.002361677587032318\n",
      "Training: Epoch 984, batch 0, Loss 0.0012651748256757855\n",
      "Training: Epoch 984, batch 1, Loss 0.00157241674605757\n",
      "Training: Epoch 984, batch 2, Loss 0.0010055546881631017\n",
      "Training: Epoch 984, batch 3, Loss 0.0019610123708844185\n",
      "Training: Epoch 987, batch 0, Loss 0.0018896014662459493\n",
      "Training: Epoch 987, batch 1, Loss 0.0018168549286201596\n",
      "Training: Epoch 987, batch 2, Loss 0.001493779825977981\n",
      "Training: Epoch 987, batch 3, Loss 0.001469482434913516\n",
      "Training: Epoch 990, batch 0, Loss 0.0015035871183499694\n",
      "Training: Epoch 990, batch 1, Loss 0.0017614237731322646\n",
      "Training: Epoch 990, batch 2, Loss 0.0012701409868896008\n",
      "Training: Epoch 990, batch 3, Loss 0.0015268974239006639\n",
      "Validation: Epoch 990, batch 0, Loss 0.0013620794052258134\n",
      "Training: Epoch 993, batch 0, Loss 0.0012108725495636463\n",
      "Training: Epoch 993, batch 1, Loss 0.0015956219285726547\n",
      "Training: Epoch 993, batch 2, Loss 0.0011642392491921782\n",
      "Training: Epoch 993, batch 3, Loss 0.0009024072205647826\n",
      "Training: Epoch 996, batch 0, Loss 0.0009764826972968876\n",
      "Training: Epoch 996, batch 1, Loss 0.0013348630163818598\n",
      "Training: Epoch 996, batch 2, Loss 0.0013204647693783045\n",
      "Training: Epoch 996, batch 3, Loss 0.0033794627524912357\n",
      "Training: Epoch 999, batch 0, Loss 0.0015412751818075776\n",
      "Training: Epoch 999, batch 1, Loss 0.0013521707151085138\n",
      "Training: Epoch 999, batch 2, Loss 0.0016281538410112262\n",
      "Training: Epoch 999, batch 3, Loss 0.0012187422253191471\n",
      "+++++ Training model 0 +++++\n",
      "Training: Epoch 0, batch 0, Loss 1.1456997394561768\n",
      "Training: Epoch 0, batch 1, Loss 0.6264216303825378\n",
      "Training: Epoch 0, batch 2, Loss 0.368741512298584\n",
      "Training: Epoch 0, batch 3, Loss 0.15607935190200806\n",
      "Validation: Epoch 0, batch 0, Loss 0.08520401269197464\n",
      "Training: Epoch 3, batch 0, Loss 0.05201224237680435\n",
      "Training: Epoch 3, batch 1, Loss 0.05975572392344475\n",
      "Training: Epoch 3, batch 2, Loss 0.05452706664800644\n",
      "Training: Epoch 3, batch 3, Loss 0.050553835928440094\n",
      "Training: Epoch 6, batch 0, Loss 0.054047998040914536\n",
      "Training: Epoch 6, batch 1, Loss 0.04867139086127281\n",
      "Training: Epoch 6, batch 2, Loss 0.05480694770812988\n",
      "Training: Epoch 6, batch 3, Loss 0.05737031251192093\n",
      "Training: Epoch 9, batch 0, Loss 0.051209405064582825\n",
      "Training: Epoch 9, batch 1, Loss 0.05357250198721886\n",
      "Training: Epoch 9, batch 2, Loss 0.05098900943994522\n",
      "Training: Epoch 9, batch 3, Loss 0.04649342969059944\n",
      "Validation: Epoch 10, batch 0, Loss 0.052764590829610825\n",
      "Training: Epoch 12, batch 0, Loss 0.04843814671039581\n",
      "Training: Epoch 12, batch 1, Loss 0.05279098078608513\n",
      "Training: Epoch 12, batch 2, Loss 0.04912141337990761\n",
      "Training: Epoch 12, batch 3, Loss 0.04913272708654404\n",
      "Training: Epoch 15, batch 0, Loss 0.04771886020898819\n",
      "Training: Epoch 15, batch 1, Loss 0.0496075265109539\n",
      "Training: Epoch 15, batch 2, Loss 0.04981482774019241\n",
      "Training: Epoch 15, batch 3, Loss 0.0470837838947773\n",
      "Training: Epoch 18, batch 0, Loss 0.04624349996447563\n",
      "Training: Epoch 18, batch 1, Loss 0.047957468777894974\n",
      "Training: Epoch 18, batch 2, Loss 0.04893100634217262\n",
      "Training: Epoch 18, batch 3, Loss 0.04888351261615753\n",
      "Validation: Epoch 20, batch 0, Loss 0.04764310643076897\n",
      "Training: Epoch 21, batch 0, Loss 0.04844653233885765\n",
      "Training: Epoch 21, batch 1, Loss 0.04477109760046005\n",
      "Training: Epoch 21, batch 2, Loss 0.04421909153461456\n",
      "Training: Epoch 21, batch 3, Loss 0.03820912167429924\n",
      "Training: Epoch 24, batch 0, Loss 0.04536495730280876\n",
      "Training: Epoch 24, batch 1, Loss 0.04172321408987045\n",
      "Training: Epoch 24, batch 2, Loss 0.04605045169591904\n",
      "Training: Epoch 24, batch 3, Loss 0.039828311651945114\n",
      "Training: Epoch 27, batch 0, Loss 0.041956398636102676\n",
      "Training: Epoch 27, batch 1, Loss 0.042067889124155045\n",
      "Training: Epoch 27, batch 2, Loss 0.04161978140473366\n",
      "Training: Epoch 27, batch 3, Loss 0.03980790823698044\n",
      "Training: Epoch 30, batch 0, Loss 0.04112553969025612\n",
      "Training: Epoch 30, batch 1, Loss 0.03727174922823906\n",
      "Training: Epoch 30, batch 2, Loss 0.042004264891147614\n",
      "Training: Epoch 30, batch 3, Loss 0.04007522016763687\n",
      "Validation: Epoch 30, batch 0, Loss 0.042959343641996384\n",
      "Training: Epoch 33, batch 0, Loss 0.03663291037082672\n",
      "Training: Epoch 33, batch 1, Loss 0.04099399968981743\n",
      "Training: Epoch 33, batch 2, Loss 0.04000493511557579\n",
      "Training: Epoch 33, batch 3, Loss 0.042418692260980606\n",
      "Training: Epoch 36, batch 0, Loss 0.0394926518201828\n",
      "Training: Epoch 36, batch 1, Loss 0.03743532672524452\n",
      "Training: Epoch 36, batch 2, Loss 0.03993270918726921\n",
      "Training: Epoch 36, batch 3, Loss 0.032122768461704254\n",
      "Training: Epoch 39, batch 0, Loss 0.03803195431828499\n",
      "Training: Epoch 39, batch 1, Loss 0.0393490232527256\n",
      "Training: Epoch 39, batch 2, Loss 0.03822088986635208\n",
      "Training: Epoch 39, batch 3, Loss 0.027684245258569717\n",
      "Validation: Epoch 40, batch 0, Loss 0.04193994775414467\n",
      "Training: Epoch 42, batch 0, Loss 0.037303145974874496\n",
      "Training: Epoch 42, batch 1, Loss 0.03855714946985245\n",
      "Training: Epoch 42, batch 2, Loss 0.03736705705523491\n",
      "Training: Epoch 42, batch 3, Loss 0.038419436663389206\n",
      "Training: Epoch 45, batch 0, Loss 0.03489776700735092\n",
      "Training: Epoch 45, batch 1, Loss 0.0375092513859272\n",
      "Training: Epoch 45, batch 2, Loss 0.03447601571679115\n",
      "Training: Epoch 45, batch 3, Loss 0.03240197151899338\n",
      "Training: Epoch 48, batch 0, Loss 0.03565138578414917\n",
      "Training: Epoch 48, batch 1, Loss 0.03333977237343788\n",
      "Training: Epoch 48, batch 2, Loss 0.036007657647132874\n",
      "Training: Epoch 48, batch 3, Loss 0.03824671730399132\n",
      "Validation: Epoch 50, batch 0, Loss 0.03776910901069641\n",
      "Training: Epoch 51, batch 0, Loss 0.038108065724372864\n",
      "Training: Epoch 51, batch 1, Loss 0.033898137509822845\n",
      "Training: Epoch 51, batch 2, Loss 0.034441571682691574\n",
      "Training: Epoch 51, batch 3, Loss 0.033997271209955215\n",
      "Training: Epoch 54, batch 0, Loss 0.03357985243201256\n",
      "Training: Epoch 54, batch 1, Loss 0.03406757116317749\n",
      "Training: Epoch 54, batch 2, Loss 0.03518682345747948\n",
      "Training: Epoch 54, batch 3, Loss 0.03194674476981163\n",
      "Training: Epoch 57, batch 0, Loss 0.033653393387794495\n",
      "Training: Epoch 57, batch 1, Loss 0.03483102470636368\n",
      "Training: Epoch 57, batch 2, Loss 0.0346694178879261\n",
      "Training: Epoch 57, batch 3, Loss 0.0323248915374279\n",
      "Training: Epoch 60, batch 0, Loss 0.032154545187950134\n",
      "Training: Epoch 60, batch 1, Loss 0.03440701588988304\n",
      "Training: Epoch 60, batch 2, Loss 0.03612354025244713\n",
      "Training: Epoch 60, batch 3, Loss 0.03478901833295822\n",
      "Validation: Epoch 60, batch 0, Loss 0.03576480597257614\n",
      "Training: Epoch 63, batch 0, Loss 0.03252825886011124\n",
      "Training: Epoch 63, batch 1, Loss 0.032594144344329834\n",
      "Training: Epoch 63, batch 2, Loss 0.03724018856883049\n",
      "Training: Epoch 63, batch 3, Loss 0.03270329535007477\n",
      "Training: Epoch 66, batch 0, Loss 0.03274647891521454\n",
      "Training: Epoch 66, batch 1, Loss 0.031292419880628586\n",
      "Training: Epoch 66, batch 2, Loss 0.031037429347634315\n",
      "Training: Epoch 66, batch 3, Loss 0.029671696946024895\n",
      "Training: Epoch 69, batch 0, Loss 0.032232705503702164\n",
      "Training: Epoch 69, batch 1, Loss 0.029244324192404747\n",
      "Training: Epoch 69, batch 2, Loss 0.031044675037264824\n",
      "Training: Epoch 69, batch 3, Loss 0.028457973152399063\n",
      "Validation: Epoch 70, batch 0, Loss 0.03104771114885807\n",
      "Training: Epoch 72, batch 0, Loss 0.028444651514291763\n",
      "Training: Epoch 72, batch 1, Loss 0.030379002913832664\n",
      "Training: Epoch 72, batch 2, Loss 0.028562815859913826\n",
      "Training: Epoch 72, batch 3, Loss 0.028903914615511894\n",
      "Training: Epoch 75, batch 0, Loss 0.026560461148619652\n",
      "Training: Epoch 75, batch 1, Loss 0.02732512541115284\n",
      "Training: Epoch 75, batch 2, Loss 0.027319924905896187\n",
      "Training: Epoch 75, batch 3, Loss 0.026412358507514\n",
      "Training: Epoch 78, batch 0, Loss 0.02390277571976185\n",
      "Training: Epoch 78, batch 1, Loss 0.024544721469283104\n",
      "Training: Epoch 78, batch 2, Loss 0.022277001291513443\n",
      "Training: Epoch 78, batch 3, Loss 0.02897605299949646\n",
      "Validation: Epoch 80, batch 0, Loss 0.022482173517346382\n",
      "Training: Epoch 81, batch 0, Loss 0.020359300076961517\n",
      "Training: Epoch 81, batch 1, Loss 0.0230559092015028\n",
      "Training: Epoch 81, batch 2, Loss 0.02182445302605629\n",
      "Training: Epoch 81, batch 3, Loss 0.02063055895268917\n",
      "Training: Epoch 84, batch 0, Loss 0.020452074706554413\n",
      "Training: Epoch 84, batch 1, Loss 0.02007426507771015\n",
      "Training: Epoch 84, batch 2, Loss 0.01989801600575447\n",
      "Training: Epoch 84, batch 3, Loss 0.02273251675069332\n",
      "Training: Epoch 87, batch 0, Loss 0.018459146842360497\n",
      "Training: Epoch 87, batch 1, Loss 0.019023938104510307\n",
      "Training: Epoch 87, batch 2, Loss 0.0189790278673172\n",
      "Training: Epoch 87, batch 3, Loss 0.024639055132865906\n",
      "Training: Epoch 90, batch 0, Loss 0.01980963535606861\n",
      "Training: Epoch 90, batch 1, Loss 0.019509287551045418\n",
      "Training: Epoch 90, batch 2, Loss 0.01804647594690323\n",
      "Training: Epoch 90, batch 3, Loss 0.016584277153015137\n",
      "Validation: Epoch 90, batch 0, Loss 0.019369853660464287\n",
      "Training: Epoch 93, batch 0, Loss 0.01874084398150444\n",
      "Training: Epoch 93, batch 1, Loss 0.01953914202749729\n",
      "Training: Epoch 93, batch 2, Loss 0.01614491082727909\n",
      "Training: Epoch 93, batch 3, Loss 0.015187816694378853\n",
      "Training: Epoch 96, batch 0, Loss 0.017919309437274933\n",
      "Training: Epoch 96, batch 1, Loss 0.017357895150780678\n",
      "Training: Epoch 96, batch 2, Loss 0.01803523115813732\n",
      "Training: Epoch 96, batch 3, Loss 0.017861247062683105\n",
      "Training: Epoch 99, batch 0, Loss 0.015376933850347996\n",
      "Training: Epoch 99, batch 1, Loss 0.015077043324708939\n",
      "Training: Epoch 99, batch 2, Loss 0.016319967806339264\n",
      "Training: Epoch 99, batch 3, Loss 0.011092640459537506\n",
      "Validation: Epoch 100, batch 0, Loss 0.014487653970718384\n",
      "Training: Epoch 102, batch 0, Loss 0.012385113164782524\n",
      "Training: Epoch 102, batch 1, Loss 0.013259653933346272\n",
      "Training: Epoch 102, batch 2, Loss 0.013132099993526936\n",
      "Training: Epoch 102, batch 3, Loss 0.011676668189466\n",
      "Training: Epoch 105, batch 0, Loss 0.010835734196007252\n",
      "Training: Epoch 105, batch 1, Loss 0.011595300398766994\n",
      "Training: Epoch 105, batch 2, Loss 0.011229407973587513\n",
      "Training: Epoch 105, batch 3, Loss 0.010441949591040611\n",
      "Training: Epoch 108, batch 0, Loss 0.010679105296730995\n",
      "Training: Epoch 108, batch 1, Loss 0.009968015365302563\n",
      "Training: Epoch 108, batch 2, Loss 0.009249448776245117\n",
      "Training: Epoch 108, batch 3, Loss 0.009948745369911194\n",
      "Validation: Epoch 110, batch 0, Loss 0.010247321799397469\n",
      "Training: Epoch 111, batch 0, Loss 0.009483755566179752\n",
      "Training: Epoch 111, batch 1, Loss 0.00948378536850214\n",
      "Training: Epoch 111, batch 2, Loss 0.009169251658022404\n",
      "Training: Epoch 111, batch 3, Loss 0.010726310312747955\n",
      "Training: Epoch 114, batch 0, Loss 0.010180842131376266\n",
      "Training: Epoch 114, batch 1, Loss 0.007852975279092789\n",
      "Training: Epoch 114, batch 2, Loss 0.00890254508703947\n",
      "Training: Epoch 114, batch 3, Loss 0.007162176538258791\n",
      "Training: Epoch 117, batch 0, Loss 0.010149852372705936\n",
      "Training: Epoch 117, batch 1, Loss 0.008310594595968723\n",
      "Training: Epoch 117, batch 2, Loss 0.009466035291552544\n",
      "Training: Epoch 117, batch 3, Loss 0.00840065348893404\n",
      "Training: Epoch 120, batch 0, Loss 0.0092232096940279\n",
      "Training: Epoch 120, batch 1, Loss 0.008567084558308125\n",
      "Training: Epoch 120, batch 2, Loss 0.008749503642320633\n",
      "Training: Epoch 120, batch 3, Loss 0.009720081463456154\n",
      "Validation: Epoch 120, batch 0, Loss 0.009246613830327988\n",
      "Training: Epoch 123, batch 0, Loss 0.0081705953925848\n",
      "Training: Epoch 123, batch 1, Loss 0.008018728345632553\n",
      "Training: Epoch 123, batch 2, Loss 0.007832709699869156\n",
      "Training: Epoch 123, batch 3, Loss 0.007873245514929295\n",
      "Training: Epoch 126, batch 0, Loss 0.007746005896478891\n",
      "Training: Epoch 126, batch 1, Loss 0.007881802506744862\n",
      "Training: Epoch 126, batch 2, Loss 0.0073891934007406235\n",
      "Training: Epoch 126, batch 3, Loss 0.006242055911570787\n",
      "Training: Epoch 129, batch 0, Loss 0.006957069970667362\n",
      "Training: Epoch 129, batch 1, Loss 0.007449062541127205\n",
      "Training: Epoch 129, batch 2, Loss 0.007975148037075996\n",
      "Training: Epoch 129, batch 3, Loss 0.007472760044038296\n",
      "Validation: Epoch 130, batch 0, Loss 0.00734571972861886\n",
      "Training: Epoch 132, batch 0, Loss 0.007140937261283398\n",
      "Training: Epoch 132, batch 1, Loss 0.006671582814306021\n",
      "Training: Epoch 132, batch 2, Loss 0.007353190332651138\n",
      "Training: Epoch 132, batch 3, Loss 0.00621637562289834\n",
      "Training: Epoch 135, batch 0, Loss 0.006583976559340954\n",
      "Training: Epoch 135, batch 1, Loss 0.006665352266281843\n",
      "Training: Epoch 135, batch 2, Loss 0.006813445128500462\n",
      "Training: Epoch 135, batch 3, Loss 0.0068241604603827\n",
      "Training: Epoch 138, batch 0, Loss 0.00815622229129076\n",
      "Training: Epoch 138, batch 1, Loss 0.009544691070914268\n",
      "Training: Epoch 138, batch 2, Loss 0.007505039218813181\n",
      "Training: Epoch 138, batch 3, Loss 0.009424719028174877\n",
      "Validation: Epoch 140, batch 0, Loss 0.008883611299097538\n",
      "Training: Epoch 141, batch 0, Loss 0.00966571643948555\n",
      "Training: Epoch 141, batch 1, Loss 0.005866510793566704\n",
      "Training: Epoch 141, batch 2, Loss 0.006851459387689829\n",
      "Training: Epoch 141, batch 3, Loss 0.010204460471868515\n",
      "Training: Epoch 144, batch 0, Loss 0.00684793246909976\n",
      "Training: Epoch 144, batch 1, Loss 0.0061917076818645\n",
      "Training: Epoch 144, batch 2, Loss 0.006280999165028334\n",
      "Training: Epoch 144, batch 3, Loss 0.009859745390713215\n",
      "Training: Epoch 147, batch 0, Loss 0.00565342977643013\n",
      "Training: Epoch 147, batch 1, Loss 0.00588027061894536\n",
      "Training: Epoch 147, batch 2, Loss 0.005474659614264965\n",
      "Training: Epoch 147, batch 3, Loss 0.005740246269851923\n",
      "Training: Epoch 150, batch 0, Loss 0.007901453413069248\n",
      "Training: Epoch 150, batch 1, Loss 0.006107793655246496\n",
      "Training: Epoch 150, batch 2, Loss 0.006392829120159149\n",
      "Training: Epoch 150, batch 3, Loss 0.006447869818657637\n",
      "Validation: Epoch 150, batch 0, Loss 0.006743498612195253\n",
      "Training: Epoch 153, batch 0, Loss 0.006186799146234989\n",
      "Training: Epoch 153, batch 1, Loss 0.006057819817215204\n",
      "Training: Epoch 153, batch 2, Loss 0.0068900869227945805\n",
      "Training: Epoch 153, batch 3, Loss 0.008039678446948528\n",
      "Training: Epoch 156, batch 0, Loss 0.006571484263986349\n",
      "Training: Epoch 156, batch 1, Loss 0.005831216927617788\n",
      "Training: Epoch 156, batch 2, Loss 0.005866854917258024\n",
      "Training: Epoch 156, batch 3, Loss 0.004608501214534044\n",
      "Training: Epoch 159, batch 0, Loss 0.006117053795605898\n",
      "Training: Epoch 159, batch 1, Loss 0.0051821144297719\n",
      "Training: Epoch 159, batch 2, Loss 0.0056179785169661045\n",
      "Training: Epoch 159, batch 3, Loss 0.004578371532261372\n",
      "Validation: Epoch 160, batch 0, Loss 0.005285516381263733\n",
      "Training: Epoch 162, batch 0, Loss 0.005909942090511322\n",
      "Training: Epoch 162, batch 1, Loss 0.004778385162353516\n",
      "Training: Epoch 162, batch 2, Loss 0.00480652553960681\n",
      "Training: Epoch 162, batch 3, Loss 0.005741296801716089\n",
      "Training: Epoch 165, batch 0, Loss 0.004384004045277834\n",
      "Training: Epoch 165, batch 1, Loss 0.005586171057075262\n",
      "Training: Epoch 165, batch 2, Loss 0.005872485227882862\n",
      "Training: Epoch 165, batch 3, Loss 0.004756807815283537\n",
      "Training: Epoch 168, batch 0, Loss 0.005681868176907301\n",
      "Training: Epoch 168, batch 1, Loss 0.005032259039580822\n",
      "Training: Epoch 168, batch 2, Loss 0.0042171115055680275\n",
      "Training: Epoch 168, batch 3, Loss 0.004428241401910782\n",
      "Validation: Epoch 170, batch 0, Loss 0.00521423714235425\n",
      "Training: Epoch 171, batch 0, Loss 0.005223740357905626\n",
      "Training: Epoch 171, batch 1, Loss 0.0043913074769079685\n",
      "Training: Epoch 171, batch 2, Loss 0.005753557197749615\n",
      "Training: Epoch 171, batch 3, Loss 0.004394611809402704\n",
      "Training: Epoch 174, batch 0, Loss 0.004256213083863258\n",
      "Training: Epoch 174, batch 1, Loss 0.004852938931435347\n",
      "Training: Epoch 174, batch 2, Loss 0.004562129266560078\n",
      "Training: Epoch 174, batch 3, Loss 0.007453268859535456\n",
      "Training: Epoch 177, batch 0, Loss 0.004403045866638422\n",
      "Training: Epoch 177, batch 1, Loss 0.005292363464832306\n",
      "Training: Epoch 177, batch 2, Loss 0.004463544115424156\n",
      "Training: Epoch 177, batch 3, Loss 0.0037680224049836397\n",
      "Training: Epoch 180, batch 0, Loss 0.004925933666527271\n",
      "Training: Epoch 180, batch 1, Loss 0.004232496954500675\n",
      "Training: Epoch 180, batch 2, Loss 0.0043612029403448105\n",
      "Training: Epoch 180, batch 3, Loss 0.0033598316367715597\n",
      "Validation: Epoch 180, batch 0, Loss 0.004196750000119209\n",
      "Training: Epoch 183, batch 0, Loss 0.005098417866975069\n",
      "Training: Epoch 183, batch 1, Loss 0.0038810076657682657\n",
      "Training: Epoch 183, batch 2, Loss 0.004210966639220715\n",
      "Training: Epoch 183, batch 3, Loss 0.004120031837373972\n",
      "Training: Epoch 186, batch 0, Loss 0.004919492639601231\n",
      "Training: Epoch 186, batch 1, Loss 0.0050180889666080475\n",
      "Training: Epoch 186, batch 2, Loss 0.00408179173246026\n",
      "Training: Epoch 186, batch 3, Loss 0.0042036548256874084\n",
      "Training: Epoch 189, batch 0, Loss 0.003891708329319954\n",
      "Training: Epoch 189, batch 1, Loss 0.004390134010463953\n",
      "Training: Epoch 189, batch 2, Loss 0.00419199001044035\n",
      "Training: Epoch 189, batch 3, Loss 0.006155916023999453\n",
      "Validation: Epoch 190, batch 0, Loss 0.00430044811218977\n",
      "Training: Epoch 192, batch 0, Loss 0.004041766282171011\n",
      "Training: Epoch 192, batch 1, Loss 0.0055261398665606976\n",
      "Training: Epoch 192, batch 2, Loss 0.003983062691986561\n",
      "Training: Epoch 192, batch 3, Loss 0.003906567580997944\n",
      "Training: Epoch 195, batch 0, Loss 0.003986512776464224\n",
      "Training: Epoch 195, batch 1, Loss 0.004766789730638266\n",
      "Training: Epoch 195, batch 2, Loss 0.0047602783888578415\n",
      "Training: Epoch 195, batch 3, Loss 0.003831777023151517\n",
      "Training: Epoch 198, batch 0, Loss 0.003935080487281084\n",
      "Training: Epoch 198, batch 1, Loss 0.004034530837088823\n",
      "Training: Epoch 198, batch 2, Loss 0.004885602742433548\n",
      "Training: Epoch 198, batch 3, Loss 0.003469112329185009\n",
      "Validation: Epoch 200, batch 0, Loss 0.003879606956616044\n",
      "Training: Epoch 201, batch 0, Loss 0.004430513363331556\n",
      "Training: Epoch 201, batch 1, Loss 0.004910391289740801\n",
      "Training: Epoch 201, batch 2, Loss 0.004024970345199108\n",
      "Training: Epoch 201, batch 3, Loss 0.003519676858559251\n",
      "Training: Epoch 204, batch 0, Loss 0.0038125566206872463\n",
      "Training: Epoch 204, batch 1, Loss 0.005538168828934431\n",
      "Training: Epoch 204, batch 2, Loss 0.003732495242729783\n",
      "Training: Epoch 204, batch 3, Loss 0.0046798973344266415\n",
      "Training: Epoch 207, batch 0, Loss 0.003718035528436303\n",
      "Training: Epoch 207, batch 1, Loss 0.004391492810100317\n",
      "Training: Epoch 207, batch 2, Loss 0.004423585254698992\n",
      "Training: Epoch 207, batch 3, Loss 0.004674473311752081\n",
      "Training: Epoch 210, batch 0, Loss 0.004207512363791466\n",
      "Training: Epoch 210, batch 1, Loss 0.004667189437896013\n",
      "Training: Epoch 210, batch 2, Loss 0.0034540293272584677\n",
      "Training: Epoch 210, batch 3, Loss 0.005509527400135994\n",
      "Validation: Epoch 210, batch 0, Loss 0.0036248043179512024\n",
      "Training: Epoch 213, batch 0, Loss 0.0036974966060370207\n",
      "Training: Epoch 213, batch 1, Loss 0.003391074948012829\n",
      "Training: Epoch 213, batch 2, Loss 0.004553448408842087\n",
      "Training: Epoch 213, batch 3, Loss 0.003632269101217389\n",
      "Training: Epoch 216, batch 0, Loss 0.003554844530299306\n",
      "Training: Epoch 216, batch 1, Loss 0.004236692562699318\n",
      "Training: Epoch 216, batch 2, Loss 0.00411994056776166\n",
      "Training: Epoch 216, batch 3, Loss 0.005008102860301733\n",
      "Training: Epoch 219, batch 0, Loss 0.004561665002256632\n",
      "Training: Epoch 219, batch 1, Loss 0.003888806328177452\n",
      "Training: Epoch 219, batch 2, Loss 0.004131254740059376\n",
      "Training: Epoch 219, batch 3, Loss 0.003797289449721575\n",
      "Validation: Epoch 220, batch 0, Loss 0.003543763654306531\n",
      "Training: Epoch 222, batch 0, Loss 0.0035327475052326918\n",
      "Training: Epoch 222, batch 1, Loss 0.003841666504740715\n",
      "Training: Epoch 222, batch 2, Loss 0.004257215652614832\n",
      "Training: Epoch 222, batch 3, Loss 0.0031235404312610626\n",
      "Training: Epoch 225, batch 0, Loss 0.0036030150949954987\n",
      "Training: Epoch 225, batch 1, Loss 0.0034217548090964556\n",
      "Training: Epoch 225, batch 2, Loss 0.004189916420727968\n",
      "Training: Epoch 225, batch 3, Loss 0.006327520124614239\n",
      "Training: Epoch 228, batch 0, Loss 0.004121303558349609\n",
      "Training: Epoch 228, batch 1, Loss 0.004271262791007757\n",
      "Training: Epoch 228, batch 2, Loss 0.004242271184921265\n",
      "Training: Epoch 228, batch 3, Loss 0.0061914450488984585\n",
      "Validation: Epoch 230, batch 0, Loss 0.0035386004019528627\n",
      "Training: Epoch 231, batch 0, Loss 0.0038867956027388573\n",
      "Training: Epoch 231, batch 1, Loss 0.0032381811179220676\n",
      "Training: Epoch 231, batch 2, Loss 0.0042561935260891914\n",
      "Training: Epoch 231, batch 3, Loss 0.0036928958725184202\n",
      "Training: Epoch 234, batch 0, Loss 0.00376079767011106\n",
      "Training: Epoch 234, batch 1, Loss 0.003935779444873333\n",
      "Training: Epoch 234, batch 2, Loss 0.003338791662827134\n",
      "Training: Epoch 234, batch 3, Loss 0.004214978776872158\n",
      "Training: Epoch 237, batch 0, Loss 0.004557605367153883\n",
      "Training: Epoch 237, batch 1, Loss 0.0033449174370616674\n",
      "Training: Epoch 237, batch 2, Loss 0.0034937781747430563\n",
      "Training: Epoch 237, batch 3, Loss 0.0037607119884341955\n",
      "Training: Epoch 240, batch 0, Loss 0.004054030869156122\n",
      "Training: Epoch 240, batch 1, Loss 0.0031578296329826117\n",
      "Training: Epoch 240, batch 2, Loss 0.0032096109353005886\n",
      "Training: Epoch 240, batch 3, Loss 0.00710423244163394\n",
      "Validation: Epoch 240, batch 0, Loss 0.004427830222994089\n",
      "Training: Epoch 243, batch 0, Loss 0.003761678235605359\n",
      "Training: Epoch 243, batch 1, Loss 0.004510471597313881\n",
      "Training: Epoch 243, batch 2, Loss 0.003763658693060279\n",
      "Training: Epoch 243, batch 3, Loss 0.0040142773650586605\n",
      "Training: Epoch 246, batch 0, Loss 0.0034850332885980606\n",
      "Training: Epoch 246, batch 1, Loss 0.003283765632659197\n",
      "Training: Epoch 246, batch 2, Loss 0.004316185601055622\n",
      "Training: Epoch 246, batch 3, Loss 0.0043218862265348434\n",
      "Training: Epoch 249, batch 0, Loss 0.003569892141968012\n",
      "Training: Epoch 249, batch 1, Loss 0.004350605886429548\n",
      "Training: Epoch 249, batch 2, Loss 0.0030017464887350798\n",
      "Training: Epoch 249, batch 3, Loss 0.0038929979782551527\n",
      "Validation: Epoch 250, batch 0, Loss 0.003510079812258482\n",
      "Training: Epoch 252, batch 0, Loss 0.004181697033345699\n",
      "Training: Epoch 252, batch 1, Loss 0.003317398950457573\n",
      "Training: Epoch 252, batch 2, Loss 0.003779779188334942\n",
      "Training: Epoch 252, batch 3, Loss 0.0028256948571652174\n",
      "Training: Epoch 255, batch 0, Loss 0.00359165808185935\n",
      "Training: Epoch 255, batch 1, Loss 0.0038026783149689436\n",
      "Training: Epoch 255, batch 2, Loss 0.003396806074306369\n",
      "Training: Epoch 255, batch 3, Loss 0.0032084498088806868\n",
      "Training: Epoch 258, batch 0, Loss 0.003600155469030142\n",
      "Training: Epoch 258, batch 1, Loss 0.003504230873659253\n",
      "Training: Epoch 258, batch 2, Loss 0.003725287737324834\n",
      "Training: Epoch 258, batch 3, Loss 0.0036211139522492886\n",
      "Validation: Epoch 260, batch 0, Loss 0.003188112284988165\n",
      "Training: Epoch 261, batch 0, Loss 0.0031212083995342255\n",
      "Training: Epoch 261, batch 1, Loss 0.0037016631104052067\n",
      "Training: Epoch 261, batch 2, Loss 0.004190717823803425\n",
      "Training: Epoch 261, batch 3, Loss 0.003379717469215393\n",
      "Training: Epoch 264, batch 0, Loss 0.003213845659047365\n",
      "Training: Epoch 264, batch 1, Loss 0.003750427160412073\n",
      "Training: Epoch 264, batch 2, Loss 0.003524945816025138\n",
      "Training: Epoch 264, batch 3, Loss 0.0036449648905545473\n",
      "Training: Epoch 267, batch 0, Loss 0.0035553108900785446\n",
      "Training: Epoch 267, batch 1, Loss 0.0034989856649190187\n",
      "Training: Epoch 267, batch 2, Loss 0.0030145326163619757\n",
      "Training: Epoch 267, batch 3, Loss 0.005045326892286539\n",
      "Training: Epoch 270, batch 0, Loss 0.0030731798615306616\n",
      "Training: Epoch 270, batch 1, Loss 0.00321293156594038\n",
      "Training: Epoch 270, batch 2, Loss 0.003963682800531387\n",
      "Training: Epoch 270, batch 3, Loss 0.00373339606449008\n",
      "Validation: Epoch 270, batch 0, Loss 0.0033265172969549894\n",
      "Training: Epoch 273, batch 0, Loss 0.003738636150956154\n",
      "Training: Epoch 273, batch 1, Loss 0.0036675985902547836\n",
      "Training: Epoch 273, batch 2, Loss 0.003950085956603289\n",
      "Training: Epoch 273, batch 3, Loss 0.0030419128015637398\n",
      "Training: Epoch 276, batch 0, Loss 0.0035373561549931765\n",
      "Training: Epoch 276, batch 1, Loss 0.0037118548061698675\n",
      "Training: Epoch 276, batch 2, Loss 0.00395944295451045\n",
      "Training: Epoch 276, batch 3, Loss 0.0026866260450333357\n",
      "Training: Epoch 279, batch 0, Loss 0.0038470507133752108\n",
      "Training: Epoch 279, batch 1, Loss 0.0038551909383386374\n",
      "Training: Epoch 279, batch 2, Loss 0.003409808035939932\n",
      "Training: Epoch 279, batch 3, Loss 0.0033646072261035442\n",
      "Validation: Epoch 280, batch 0, Loss 0.003293299116194248\n",
      "Training: Epoch 282, batch 0, Loss 0.003344510681927204\n",
      "Training: Epoch 282, batch 1, Loss 0.004001274239271879\n",
      "Training: Epoch 282, batch 2, Loss 0.0037694862112402916\n",
      "Training: Epoch 282, batch 3, Loss 0.002803365932777524\n",
      "Training: Epoch 285, batch 0, Loss 0.0030687861144542694\n",
      "Training: Epoch 285, batch 1, Loss 0.004021358676254749\n",
      "Training: Epoch 285, batch 2, Loss 0.003301887307316065\n",
      "Training: Epoch 285, batch 3, Loss 0.003185616573318839\n",
      "Training: Epoch 288, batch 0, Loss 0.003576491726562381\n",
      "Training: Epoch 288, batch 1, Loss 0.0034331385977566242\n",
      "Training: Epoch 288, batch 2, Loss 0.003594437148422003\n",
      "Training: Epoch 288, batch 3, Loss 0.0033909857738763094\n",
      "Validation: Epoch 290, batch 0, Loss 0.003373250598087907\n",
      "Training: Epoch 291, batch 0, Loss 0.003753452794626355\n",
      "Training: Epoch 291, batch 1, Loss 0.0030749160796403885\n",
      "Training: Epoch 291, batch 2, Loss 0.00413996959105134\n",
      "Training: Epoch 291, batch 3, Loss 0.003062350442633033\n",
      "Training: Epoch 294, batch 0, Loss 0.0037608258426189423\n",
      "Training: Epoch 294, batch 1, Loss 0.0033029830083251\n",
      "Training: Epoch 294, batch 2, Loss 0.00384187838062644\n",
      "Training: Epoch 294, batch 3, Loss 0.003200151724740863\n",
      "Training: Epoch 297, batch 0, Loss 0.004289593547582626\n",
      "Training: Epoch 297, batch 1, Loss 0.0029799737967550755\n",
      "Training: Epoch 297, batch 2, Loss 0.002699906239286065\n",
      "Training: Epoch 297, batch 3, Loss 0.0038752832915633917\n",
      "Training: Epoch 300, batch 0, Loss 0.0032545386347919703\n",
      "Training: Epoch 300, batch 1, Loss 0.003991415258497\n",
      "Training: Epoch 300, batch 2, Loss 0.0038438173942267895\n",
      "Training: Epoch 300, batch 3, Loss 0.004015848971903324\n",
      "Validation: Epoch 300, batch 0, Loss 0.003220077371224761\n",
      "Training: Epoch 303, batch 0, Loss 0.0036728528793901205\n",
      "Training: Epoch 303, batch 1, Loss 0.003407002193853259\n",
      "Training: Epoch 303, batch 2, Loss 0.004198039416223764\n",
      "Training: Epoch 303, batch 3, Loss 0.003250070381909609\n",
      "Training: Epoch 306, batch 0, Loss 0.003621539566665888\n",
      "Training: Epoch 306, batch 1, Loss 0.003448107512667775\n",
      "Training: Epoch 306, batch 2, Loss 0.003927163779735565\n",
      "Training: Epoch 306, batch 3, Loss 0.0026088689919561148\n",
      "Training: Epoch 309, batch 0, Loss 0.0032658942509442568\n",
      "Training: Epoch 309, batch 1, Loss 0.0036189646925777197\n",
      "Training: Epoch 309, batch 2, Loss 0.0032023629173636436\n",
      "Training: Epoch 309, batch 3, Loss 0.0037376906257122755\n",
      "Validation: Epoch 310, batch 0, Loss 0.0031507578678429127\n",
      "Training: Epoch 312, batch 0, Loss 0.004439407028257847\n",
      "Training: Epoch 312, batch 1, Loss 0.003940617199987173\n",
      "Training: Epoch 312, batch 2, Loss 0.0035193213261663914\n",
      "Training: Epoch 312, batch 3, Loss 0.0037107665557414293\n",
      "Training: Epoch 315, batch 0, Loss 0.0040572285652160645\n",
      "Training: Epoch 315, batch 1, Loss 0.002989750122651458\n",
      "Training: Epoch 315, batch 2, Loss 0.003364852862432599\n",
      "Training: Epoch 315, batch 3, Loss 0.004676257260143757\n",
      "Training: Epoch 318, batch 0, Loss 0.004072400741279125\n",
      "Training: Epoch 318, batch 1, Loss 0.004725773353129625\n",
      "Training: Epoch 318, batch 2, Loss 0.003542263526469469\n",
      "Training: Epoch 318, batch 3, Loss 0.0052619436755776405\n",
      "Validation: Epoch 320, batch 0, Loss 0.003486724104732275\n",
      "Training: Epoch 321, batch 0, Loss 0.0040528844110667706\n",
      "Training: Epoch 321, batch 1, Loss 0.004180773161351681\n",
      "Training: Epoch 321, batch 2, Loss 0.006262399256229401\n",
      "Training: Epoch 321, batch 3, Loss 0.0038366338703781366\n",
      "Training: Epoch 324, batch 0, Loss 0.003794705029577017\n",
      "Training: Epoch 324, batch 1, Loss 0.004664392676204443\n",
      "Training: Epoch 324, batch 2, Loss 0.004526425153017044\n",
      "Training: Epoch 324, batch 3, Loss 0.004856368061155081\n",
      "Training: Epoch 327, batch 0, Loss 0.003587187733501196\n",
      "Training: Epoch 327, batch 1, Loss 0.003169106552377343\n",
      "Training: Epoch 327, batch 2, Loss 0.0038524288684129715\n",
      "Training: Epoch 327, batch 3, Loss 0.004307308234274387\n",
      "Training: Epoch 330, batch 0, Loss 0.0033864930737763643\n",
      "Training: Epoch 330, batch 1, Loss 0.0037817719858139753\n",
      "Training: Epoch 330, batch 2, Loss 0.0033286663237959146\n",
      "Training: Epoch 330, batch 3, Loss 0.0026474585756659508\n",
      "Validation: Epoch 330, batch 0, Loss 0.0032425483223050833\n",
      "Training: Epoch 333, batch 0, Loss 0.003946071956306696\n",
      "Training: Epoch 333, batch 1, Loss 0.0028383349999785423\n",
      "Training: Epoch 333, batch 2, Loss 0.0034858910366892815\n",
      "Training: Epoch 333, batch 3, Loss 0.002691167639568448\n",
      "Training: Epoch 336, batch 0, Loss 0.00367475813254714\n",
      "Training: Epoch 336, batch 1, Loss 0.0032758740708231926\n",
      "Training: Epoch 336, batch 2, Loss 0.0027622412890195847\n",
      "Training: Epoch 336, batch 3, Loss 0.003357752226293087\n",
      "Training: Epoch 339, batch 0, Loss 0.003625917248427868\n",
      "Training: Epoch 339, batch 1, Loss 0.0026560057885944843\n",
      "Training: Epoch 339, batch 2, Loss 0.0031852542888373137\n",
      "Training: Epoch 339, batch 3, Loss 0.002537944819778204\n",
      "Validation: Epoch 340, batch 0, Loss 0.003435185644775629\n",
      "Training: Epoch 342, batch 0, Loss 0.003120105480775237\n",
      "Training: Epoch 342, batch 1, Loss 0.0036742594093084335\n",
      "Training: Epoch 342, batch 2, Loss 0.0033585091587156057\n",
      "Training: Epoch 342, batch 3, Loss 0.002266680123284459\n",
      "Training: Epoch 345, batch 0, Loss 0.0029844597447663546\n",
      "Training: Epoch 345, batch 1, Loss 0.0027892461512237787\n",
      "Training: Epoch 345, batch 2, Loss 0.003634336870163679\n",
      "Training: Epoch 345, batch 3, Loss 0.0028815018013119698\n",
      "Training: Epoch 348, batch 0, Loss 0.0036796079948544502\n",
      "Training: Epoch 348, batch 1, Loss 0.002730741398409009\n",
      "Training: Epoch 348, batch 2, Loss 0.0034066601656377316\n",
      "Training: Epoch 348, batch 3, Loss 0.003408035496249795\n",
      "Validation: Epoch 350, batch 0, Loss 0.002952651586383581\n",
      "Training: Epoch 351, batch 0, Loss 0.002758862217888236\n",
      "Training: Epoch 351, batch 1, Loss 0.002763202181085944\n",
      "Training: Epoch 351, batch 2, Loss 0.003717387095093727\n",
      "Training: Epoch 351, batch 3, Loss 0.005141925066709518\n",
      "Training: Epoch 354, batch 0, Loss 0.002832519356161356\n",
      "Training: Epoch 354, batch 1, Loss 0.003971688915044069\n",
      "Training: Epoch 354, batch 2, Loss 0.002806072821840644\n",
      "Training: Epoch 354, batch 3, Loss 0.00279464409686625\n",
      "Training: Epoch 357, batch 0, Loss 0.0031425512861460447\n",
      "Training: Epoch 357, batch 1, Loss 0.0029138599056750536\n",
      "Training: Epoch 357, batch 2, Loss 0.003464159555733204\n",
      "Training: Epoch 357, batch 3, Loss 0.0034122969955205917\n",
      "Training: Epoch 360, batch 0, Loss 0.003122479422017932\n",
      "Training: Epoch 360, batch 1, Loss 0.0035599071998149157\n",
      "Training: Epoch 360, batch 2, Loss 0.0026130140759050846\n",
      "Training: Epoch 360, batch 3, Loss 0.0025149229913949966\n",
      "Validation: Epoch 360, batch 0, Loss 0.0027424583677202463\n",
      "Training: Epoch 363, batch 0, Loss 0.002776759210973978\n",
      "Training: Epoch 363, batch 1, Loss 0.003681257599964738\n",
      "Training: Epoch 363, batch 2, Loss 0.0034419780131429434\n",
      "Training: Epoch 363, batch 3, Loss 0.0022494636941701174\n",
      "Training: Epoch 366, batch 0, Loss 0.0026822166983038187\n",
      "Training: Epoch 366, batch 1, Loss 0.002983673242852092\n",
      "Training: Epoch 366, batch 2, Loss 0.003976922482252121\n",
      "Training: Epoch 366, batch 3, Loss 0.002954461146146059\n",
      "Training: Epoch 369, batch 0, Loss 0.0026840863283723593\n",
      "Training: Epoch 369, batch 1, Loss 0.0032389352563768625\n",
      "Training: Epoch 369, batch 2, Loss 0.002929822774603963\n",
      "Training: Epoch 369, batch 3, Loss 0.004996017087250948\n",
      "Validation: Epoch 370, batch 0, Loss 0.0032744735945016146\n",
      "Training: Epoch 372, batch 0, Loss 0.0037406401243060827\n",
      "Training: Epoch 372, batch 1, Loss 0.0036301766522228718\n",
      "Training: Epoch 372, batch 2, Loss 0.0031217215582728386\n",
      "Training: Epoch 372, batch 3, Loss 0.003126933239400387\n",
      "Training: Epoch 375, batch 0, Loss 0.003012058325111866\n",
      "Training: Epoch 375, batch 1, Loss 0.0032647254411131144\n",
      "Training: Epoch 375, batch 2, Loss 0.00310985348187387\n",
      "Training: Epoch 375, batch 3, Loss 0.0032013391610234976\n",
      "Training: Epoch 378, batch 0, Loss 0.0029930565506219864\n",
      "Training: Epoch 378, batch 1, Loss 0.0027056289836764336\n",
      "Training: Epoch 378, batch 2, Loss 0.002966597443446517\n",
      "Training: Epoch 378, batch 3, Loss 0.004793991334736347\n",
      "Validation: Epoch 380, batch 0, Loss 0.0028041107580065727\n",
      "Training: Epoch 381, batch 0, Loss 0.0025376328267157078\n",
      "Training: Epoch 381, batch 1, Loss 0.0032683340832591057\n",
      "Training: Epoch 381, batch 2, Loss 0.0033784389961510897\n",
      "Training: Epoch 381, batch 3, Loss 0.0026254833210259676\n",
      "Training: Epoch 384, batch 0, Loss 0.0029445195104926825\n",
      "Training: Epoch 384, batch 1, Loss 0.002516508335247636\n",
      "Training: Epoch 384, batch 2, Loss 0.003453388111665845\n",
      "Training: Epoch 384, batch 3, Loss 0.00227239727973938\n",
      "Training: Epoch 387, batch 0, Loss 0.0028618392534554005\n",
      "Training: Epoch 387, batch 1, Loss 0.002643248764798045\n",
      "Training: Epoch 387, batch 2, Loss 0.003429975127801299\n",
      "Training: Epoch 387, batch 3, Loss 0.002677484881132841\n",
      "Training: Epoch 390, batch 0, Loss 0.002925514942035079\n",
      "Training: Epoch 390, batch 1, Loss 0.003036906709894538\n",
      "Training: Epoch 390, batch 2, Loss 0.002943487837910652\n",
      "Training: Epoch 390, batch 3, Loss 0.002854826394468546\n",
      "Validation: Epoch 390, batch 0, Loss 0.0026701679453253746\n",
      "Training: Epoch 393, batch 0, Loss 0.003075229236856103\n",
      "Training: Epoch 393, batch 1, Loss 0.002489017089828849\n",
      "Training: Epoch 393, batch 2, Loss 0.0033915331587195396\n",
      "Training: Epoch 393, batch 3, Loss 0.003188800998032093\n",
      "Training: Epoch 396, batch 0, Loss 0.0027895409148186445\n",
      "Training: Epoch 396, batch 1, Loss 0.0033606654033064842\n",
      "Training: Epoch 396, batch 2, Loss 0.003068867139518261\n",
      "Training: Epoch 396, batch 3, Loss 0.002179838949814439\n",
      "Training: Epoch 399, batch 0, Loss 0.0032856371253728867\n",
      "Training: Epoch 399, batch 1, Loss 0.003153916448354721\n",
      "Training: Epoch 399, batch 2, Loss 0.002788033103570342\n",
      "Training: Epoch 399, batch 3, Loss 0.0030229694675654173\n",
      "Validation: Epoch 400, batch 0, Loss 0.0025834583211690187\n",
      "Training: Epoch 402, batch 0, Loss 0.003130515804514289\n",
      "Training: Epoch 402, batch 1, Loss 0.0028884457424283028\n",
      "Training: Epoch 402, batch 2, Loss 0.0029360014013946056\n",
      "Training: Epoch 402, batch 3, Loss 0.0026226199697703123\n",
      "Training: Epoch 405, batch 0, Loss 0.003589846659451723\n",
      "Training: Epoch 405, batch 1, Loss 0.0024544906336814165\n",
      "Training: Epoch 405, batch 2, Loss 0.0029726908542215824\n",
      "Training: Epoch 405, batch 3, Loss 0.0025156645569950342\n",
      "Training: Epoch 408, batch 0, Loss 0.003748971503227949\n",
      "Training: Epoch 408, batch 1, Loss 0.002938541118055582\n",
      "Training: Epoch 408, batch 2, Loss 0.002872091718018055\n",
      "Training: Epoch 408, batch 3, Loss 0.0024256946053355932\n",
      "Validation: Epoch 410, batch 0, Loss 0.002612912328913808\n",
      "Training: Epoch 411, batch 0, Loss 0.0026849238201975822\n",
      "Training: Epoch 411, batch 1, Loss 0.003296698210760951\n",
      "Training: Epoch 411, batch 2, Loss 0.0025354837998747826\n",
      "Training: Epoch 411, batch 3, Loss 0.0031041186302900314\n",
      "Training: Epoch 414, batch 0, Loss 0.0032354481518268585\n",
      "Training: Epoch 414, batch 1, Loss 0.002575583988800645\n",
      "Training: Epoch 414, batch 2, Loss 0.0029237375129014254\n",
      "Training: Epoch 414, batch 3, Loss 0.0024139462038874626\n",
      "Training: Epoch 417, batch 0, Loss 0.0028877623844891787\n",
      "Training: Epoch 417, batch 1, Loss 0.0031694795470684767\n",
      "Training: Epoch 417, batch 2, Loss 0.0024600746110081673\n",
      "Training: Epoch 417, batch 3, Loss 0.002915679244324565\n",
      "Training: Epoch 420, batch 0, Loss 0.002859274623915553\n",
      "Training: Epoch 420, batch 1, Loss 0.002412005327641964\n",
      "Training: Epoch 420, batch 2, Loss 0.0033752734307199717\n",
      "Training: Epoch 420, batch 3, Loss 0.001983557129278779\n",
      "Validation: Epoch 420, batch 0, Loss 0.002846743445843458\n",
      "Training: Epoch 423, batch 0, Loss 0.002359158592298627\n",
      "Training: Epoch 423, batch 1, Loss 0.002656891942024231\n",
      "Training: Epoch 423, batch 2, Loss 0.003894808469340205\n",
      "Training: Epoch 423, batch 3, Loss 0.0026230073999613523\n",
      "Training: Epoch 426, batch 0, Loss 0.002669167472049594\n",
      "Training: Epoch 426, batch 1, Loss 0.0032802957575768232\n",
      "Training: Epoch 426, batch 2, Loss 0.0028867728542536497\n",
      "Training: Epoch 426, batch 3, Loss 0.0018543056212365627\n",
      "Training: Epoch 429, batch 0, Loss 0.0028812706004828215\n",
      "Training: Epoch 429, batch 1, Loss 0.0026285380590707064\n",
      "Training: Epoch 429, batch 2, Loss 0.00272433296777308\n",
      "Training: Epoch 429, batch 3, Loss 0.0034077807795256376\n",
      "Validation: Epoch 430, batch 0, Loss 0.0027669097762554884\n",
      "Training: Epoch 432, batch 0, Loss 0.003498053178191185\n",
      "Training: Epoch 432, batch 1, Loss 0.0029946479480713606\n",
      "Training: Epoch 432, batch 2, Loss 0.00400572782382369\n",
      "Training: Epoch 432, batch 3, Loss 0.002343587577342987\n",
      "Training: Epoch 435, batch 0, Loss 0.0034638780634850264\n",
      "Training: Epoch 435, batch 1, Loss 0.0031813490204513073\n",
      "Training: Epoch 435, batch 2, Loss 0.0027356198988854885\n",
      "Training: Epoch 435, batch 3, Loss 0.002552742138504982\n",
      "Training: Epoch 438, batch 0, Loss 0.002903037704527378\n",
      "Training: Epoch 438, batch 1, Loss 0.0031331703066825867\n",
      "Training: Epoch 438, batch 2, Loss 0.004427642561495304\n",
      "Training: Epoch 438, batch 3, Loss 0.0033318125642836094\n",
      "Validation: Epoch 440, batch 0, Loss 0.003271860769018531\n",
      "Training: Epoch 441, batch 0, Loss 0.00301409256644547\n",
      "Training: Epoch 441, batch 1, Loss 0.004729786887764931\n",
      "Training: Epoch 441, batch 2, Loss 0.0029236015398055315\n",
      "Training: Epoch 441, batch 3, Loss 0.00370760727673769\n",
      "Training: Epoch 444, batch 0, Loss 0.0037466560024768114\n",
      "Training: Epoch 444, batch 1, Loss 0.0028893991839140654\n",
      "Training: Epoch 444, batch 2, Loss 0.0037865624763071537\n",
      "Training: Epoch 444, batch 3, Loss 0.004462928511202335\n",
      "Training: Epoch 447, batch 0, Loss 0.0036672900896519423\n",
      "Training: Epoch 447, batch 1, Loss 0.0034325483720749617\n",
      "Training: Epoch 447, batch 2, Loss 0.003959413152188063\n",
      "Training: Epoch 447, batch 3, Loss 0.0025563195813447237\n",
      "Training: Epoch 450, batch 0, Loss 0.0029911333695054054\n",
      "Training: Epoch 450, batch 1, Loss 0.0029662009328603745\n",
      "Training: Epoch 450, batch 2, Loss 0.002551538636907935\n",
      "Training: Epoch 450, batch 3, Loss 0.00478351628407836\n",
      "Validation: Epoch 450, batch 0, Loss 0.003362814197316766\n",
      "Training: Epoch 453, batch 0, Loss 0.0029895142652094364\n",
      "Training: Epoch 453, batch 1, Loss 0.003409777767956257\n",
      "Training: Epoch 453, batch 2, Loss 0.0032222154550254345\n",
      "Training: Epoch 453, batch 3, Loss 0.0023499317467212677\n",
      "Training: Epoch 456, batch 0, Loss 0.002424172591418028\n",
      "Training: Epoch 456, batch 1, Loss 0.002893727505579591\n",
      "Training: Epoch 456, batch 2, Loss 0.003110518679022789\n",
      "Training: Epoch 456, batch 3, Loss 0.002481691772118211\n",
      "Training: Epoch 459, batch 0, Loss 0.0026495836209505796\n",
      "Training: Epoch 459, batch 1, Loss 0.0033337450586259365\n",
      "Training: Epoch 459, batch 2, Loss 0.0024514843244105577\n",
      "Training: Epoch 459, batch 3, Loss 0.002638649893924594\n",
      "Validation: Epoch 460, batch 0, Loss 0.0023662035819143057\n",
      "Training: Epoch 462, batch 0, Loss 0.003178704297170043\n",
      "Training: Epoch 462, batch 1, Loss 0.002643353072926402\n",
      "Training: Epoch 462, batch 2, Loss 0.002210823819041252\n",
      "Training: Epoch 462, batch 3, Loss 0.002417183481156826\n",
      "Training: Epoch 465, batch 0, Loss 0.003146759932860732\n",
      "Training: Epoch 465, batch 1, Loss 0.002825021045282483\n",
      "Training: Epoch 465, batch 2, Loss 0.002344829263165593\n",
      "Training: Epoch 465, batch 3, Loss 0.0025920909829437733\n",
      "Training: Epoch 468, batch 0, Loss 0.0027455526869744062\n",
      "Training: Epoch 468, batch 1, Loss 0.002278743777424097\n",
      "Training: Epoch 468, batch 2, Loss 0.0026185796596109867\n",
      "Training: Epoch 468, batch 3, Loss 0.005407084710896015\n",
      "Validation: Epoch 470, batch 0, Loss 0.002667768858373165\n",
      "Training: Epoch 471, batch 0, Loss 0.0025822222232818604\n",
      "Training: Epoch 471, batch 1, Loss 0.0034632969181984663\n",
      "Training: Epoch 471, batch 2, Loss 0.002142912708222866\n",
      "Training: Epoch 471, batch 3, Loss 0.005046715959906578\n",
      "Training: Epoch 474, batch 0, Loss 0.002733612898737192\n",
      "Training: Epoch 474, batch 1, Loss 0.0031723773572593927\n",
      "Training: Epoch 474, batch 2, Loss 0.0029642966110259295\n",
      "Training: Epoch 474, batch 3, Loss 0.0026297939475625753\n",
      "Training: Epoch 477, batch 0, Loss 0.0024175425060093403\n",
      "Training: Epoch 477, batch 1, Loss 0.0030685928650200367\n",
      "Training: Epoch 477, batch 2, Loss 0.002843437949195504\n",
      "Training: Epoch 477, batch 3, Loss 0.0022882684133946896\n",
      "Training: Epoch 480, batch 0, Loss 0.002723457757383585\n",
      "Training: Epoch 480, batch 1, Loss 0.002638070145621896\n",
      "Training: Epoch 480, batch 2, Loss 0.0027474048547446728\n",
      "Training: Epoch 480, batch 3, Loss 0.002947199158370495\n",
      "Validation: Epoch 480, batch 0, Loss 0.0024450861383229494\n",
      "Training: Epoch 483, batch 0, Loss 0.0024101226590573788\n",
      "Training: Epoch 483, batch 1, Loss 0.0029788559768348932\n",
      "Training: Epoch 483, batch 2, Loss 0.0030031618662178516\n",
      "Training: Epoch 483, batch 3, Loss 0.0022915571462363005\n",
      "Training: Epoch 486, batch 0, Loss 0.002762284129858017\n",
      "Training: Epoch 486, batch 1, Loss 0.0024017230607569218\n",
      "Training: Epoch 486, batch 2, Loss 0.003143126145005226\n",
      "Training: Epoch 486, batch 3, Loss 0.0018140069441869855\n",
      "Training: Epoch 489, batch 0, Loss 0.002767684403806925\n",
      "Training: Epoch 489, batch 1, Loss 0.0027185077778995037\n",
      "Training: Epoch 489, batch 2, Loss 0.0025934556033462286\n",
      "Training: Epoch 489, batch 3, Loss 0.0022870521061122417\n",
      "Validation: Epoch 490, batch 0, Loss 0.002461270662024617\n",
      "Training: Epoch 492, batch 0, Loss 0.0027340897358953953\n",
      "Training: Epoch 492, batch 1, Loss 0.002548624062910676\n",
      "Training: Epoch 492, batch 2, Loss 0.0023081840481609106\n",
      "Training: Epoch 492, batch 3, Loss 0.0031659456435590982\n",
      "Training: Epoch 495, batch 0, Loss 0.0026607222389429808\n",
      "Training: Epoch 495, batch 1, Loss 0.002512456616386771\n",
      "Training: Epoch 495, batch 2, Loss 0.002504390198737383\n",
      "Training: Epoch 495, batch 3, Loss 0.0019804369658231735\n",
      "Training: Epoch 498, batch 0, Loss 0.002397179836407304\n",
      "Training: Epoch 498, batch 1, Loss 0.0027365570422261953\n",
      "Training: Epoch 498, batch 2, Loss 0.0027500393334776163\n",
      "Training: Epoch 498, batch 3, Loss 0.001729832380078733\n",
      "Validation: Epoch 500, batch 0, Loss 0.0024062308948487043\n",
      "Training: Epoch 501, batch 0, Loss 0.002332208910956979\n",
      "Training: Epoch 501, batch 1, Loss 0.0026556102093309164\n",
      "Training: Epoch 501, batch 2, Loss 0.0029869002755731344\n",
      "Training: Epoch 501, batch 3, Loss 0.00269843521527946\n",
      "Training: Epoch 504, batch 0, Loss 0.0033313457388430834\n",
      "Training: Epoch 504, batch 1, Loss 0.0025280844420194626\n",
      "Training: Epoch 504, batch 2, Loss 0.0023072524927556515\n",
      "Training: Epoch 504, batch 3, Loss 0.002435198985040188\n",
      "Training: Epoch 507, batch 0, Loss 0.0029893647879362106\n",
      "Training: Epoch 507, batch 1, Loss 0.0025062174536287785\n",
      "Training: Epoch 507, batch 2, Loss 0.002826158655807376\n",
      "Training: Epoch 507, batch 3, Loss 0.0030354144982993603\n",
      "Training: Epoch 510, batch 0, Loss 0.0035559453535825014\n",
      "Training: Epoch 510, batch 1, Loss 0.0022428983356803656\n",
      "Training: Epoch 510, batch 2, Loss 0.002517072018235922\n",
      "Training: Epoch 510, batch 3, Loss 0.0026796646416187286\n",
      "Validation: Epoch 510, batch 0, Loss 0.00242233881726861\n",
      "Training: Epoch 513, batch 0, Loss 0.002901541767641902\n",
      "Training: Epoch 513, batch 1, Loss 0.0026037839706987143\n",
      "Training: Epoch 513, batch 2, Loss 0.00236119725741446\n",
      "Training: Epoch 513, batch 3, Loss 0.0030759007204324007\n",
      "Training: Epoch 516, batch 0, Loss 0.0024957528803497553\n",
      "Training: Epoch 516, batch 1, Loss 0.0031338208355009556\n",
      "Training: Epoch 516, batch 2, Loss 0.0022321180440485477\n",
      "Training: Epoch 516, batch 3, Loss 0.0024282473605126143\n",
      "Training: Epoch 519, batch 0, Loss 0.003815527306869626\n",
      "Training: Epoch 519, batch 1, Loss 0.0026232695672661066\n",
      "Training: Epoch 519, batch 2, Loss 0.002564400900155306\n",
      "Training: Epoch 519, batch 3, Loss 0.0024239146150648594\n",
      "Validation: Epoch 520, batch 0, Loss 0.0026163344737142324\n",
      "Training: Epoch 522, batch 0, Loss 0.0022888206876814365\n",
      "Training: Epoch 522, batch 1, Loss 0.0029782368801534176\n",
      "Training: Epoch 522, batch 2, Loss 0.003357537556439638\n",
      "Training: Epoch 522, batch 3, Loss 0.0023343677166849375\n",
      "Training: Epoch 525, batch 0, Loss 0.0033520522993057966\n",
      "Training: Epoch 525, batch 1, Loss 0.0026144608855247498\n",
      "Training: Epoch 525, batch 2, Loss 0.002937088720500469\n",
      "Training: Epoch 525, batch 3, Loss 0.001967659220099449\n",
      "Training: Epoch 528, batch 0, Loss 0.0025420819874852896\n",
      "Training: Epoch 528, batch 1, Loss 0.003179997205734253\n",
      "Training: Epoch 528, batch 2, Loss 0.002493860898539424\n",
      "Training: Epoch 528, batch 3, Loss 0.0026768234092742205\n",
      "Validation: Epoch 530, batch 0, Loss 0.0021387897431850433\n",
      "Training: Epoch 531, batch 0, Loss 0.0031335230451077223\n",
      "Training: Epoch 531, batch 1, Loss 0.002304705558344722\n",
      "Training: Epoch 531, batch 2, Loss 0.00215966091491282\n",
      "Training: Epoch 531, batch 3, Loss 0.0020257693249732256\n",
      "Training: Epoch 534, batch 0, Loss 0.002743100980296731\n",
      "Training: Epoch 534, batch 1, Loss 0.002310817362740636\n",
      "Training: Epoch 534, batch 2, Loss 0.002639962825924158\n",
      "Training: Epoch 534, batch 3, Loss 0.0042508263140916824\n",
      "Training: Epoch 537, batch 0, Loss 0.002337014302611351\n",
      "Training: Epoch 537, batch 1, Loss 0.002913923002779484\n",
      "Training: Epoch 537, batch 2, Loss 0.002918172860518098\n",
      "Training: Epoch 537, batch 3, Loss 0.0018250300781801343\n",
      "Training: Epoch 540, batch 0, Loss 0.0031008217483758926\n",
      "Training: Epoch 540, batch 1, Loss 0.002575345803052187\n",
      "Training: Epoch 540, batch 2, Loss 0.002305978210642934\n",
      "Training: Epoch 540, batch 3, Loss 0.0021782461553812027\n",
      "Validation: Epoch 540, batch 0, Loss 0.0024034033995121717\n",
      "Training: Epoch 543, batch 0, Loss 0.0024901137221604586\n",
      "Training: Epoch 543, batch 1, Loss 0.002754664048552513\n",
      "Training: Epoch 543, batch 2, Loss 0.0029116226360201836\n",
      "Training: Epoch 543, batch 3, Loss 0.002367145847529173\n",
      "Training: Epoch 546, batch 0, Loss 0.0029601699206978083\n",
      "Training: Epoch 546, batch 1, Loss 0.002267818432301283\n",
      "Training: Epoch 546, batch 2, Loss 0.0024528715293854475\n",
      "Training: Epoch 546, batch 3, Loss 0.002341260202229023\n",
      "Training: Epoch 549, batch 0, Loss 0.002666679210960865\n",
      "Training: Epoch 549, batch 1, Loss 0.0023351297713816166\n",
      "Training: Epoch 549, batch 2, Loss 0.002392966765910387\n",
      "Training: Epoch 549, batch 3, Loss 0.0020609472412616014\n",
      "Validation: Epoch 550, batch 0, Loss 0.002276021521538496\n",
      "Training: Epoch 552, batch 0, Loss 0.002403007820248604\n",
      "Training: Epoch 552, batch 1, Loss 0.0023834779858589172\n",
      "Training: Epoch 552, batch 2, Loss 0.0026674792170524597\n",
      "Training: Epoch 552, batch 3, Loss 0.005827523767948151\n",
      "Training: Epoch 555, batch 0, Loss 0.003176179016008973\n",
      "Training: Epoch 555, batch 1, Loss 0.0035704600159078836\n",
      "Training: Epoch 555, batch 2, Loss 0.0033258411567658186\n",
      "Training: Epoch 555, batch 3, Loss 0.002510864520445466\n",
      "Training: Epoch 558, batch 0, Loss 0.0025638462975621223\n",
      "Training: Epoch 558, batch 1, Loss 0.002889190800487995\n",
      "Training: Epoch 558, batch 2, Loss 0.0026889475993812084\n",
      "Training: Epoch 558, batch 3, Loss 0.00285896984860301\n",
      "Validation: Epoch 560, batch 0, Loss 0.002321776933968067\n",
      "Training: Epoch 561, batch 0, Loss 0.0025854206178337336\n",
      "Training: Epoch 561, batch 1, Loss 0.002212995197623968\n",
      "Training: Epoch 561, batch 2, Loss 0.002285647438839078\n",
      "Training: Epoch 561, batch 3, Loss 0.004279904067516327\n",
      "Training: Epoch 564, batch 0, Loss 0.0026305108331143856\n",
      "Training: Epoch 564, batch 1, Loss 0.0024839674588292837\n",
      "Training: Epoch 564, batch 2, Loss 0.0025749565102159977\n",
      "Training: Epoch 564, batch 3, Loss 0.0022799328435212374\n",
      "Training: Epoch 567, batch 0, Loss 0.0024808936286717653\n",
      "Training: Epoch 567, batch 1, Loss 0.002718063071370125\n",
      "Training: Epoch 567, batch 2, Loss 0.0027896836400032043\n",
      "Training: Epoch 567, batch 3, Loss 0.0017653065733611584\n",
      "Training: Epoch 570, batch 0, Loss 0.002143376972526312\n",
      "Training: Epoch 570, batch 1, Loss 0.002328196307644248\n",
      "Training: Epoch 570, batch 2, Loss 0.002905044471845031\n",
      "Training: Epoch 570, batch 3, Loss 0.0016595427878201008\n",
      "Validation: Epoch 570, batch 0, Loss 0.002103096805512905\n",
      "Training: Epoch 573, batch 0, Loss 0.002095121657475829\n",
      "Training: Epoch 573, batch 1, Loss 0.00230805273167789\n",
      "Training: Epoch 573, batch 2, Loss 0.0028685268480330706\n",
      "Training: Epoch 573, batch 3, Loss 0.0019432823173701763\n",
      "Training: Epoch 576, batch 0, Loss 0.0026192059740424156\n",
      "Training: Epoch 576, batch 1, Loss 0.0022941099014133215\n",
      "Training: Epoch 576, batch 2, Loss 0.002081509679555893\n",
      "Training: Epoch 576, batch 3, Loss 0.0023671637754887342\n",
      "Training: Epoch 579, batch 0, Loss 0.0018889716593548656\n",
      "Training: Epoch 579, batch 1, Loss 0.003041519084945321\n",
      "Training: Epoch 579, batch 2, Loss 0.00233083451166749\n",
      "Training: Epoch 579, batch 3, Loss 0.002030341885983944\n",
      "Validation: Epoch 580, batch 0, Loss 0.002100084675475955\n",
      "Training: Epoch 582, batch 0, Loss 0.002542447531595826\n",
      "Training: Epoch 582, batch 1, Loss 0.002464652992784977\n",
      "Training: Epoch 582, batch 2, Loss 0.0022551484871655703\n",
      "Training: Epoch 582, batch 3, Loss 0.0017842171946540475\n",
      "Training: Epoch 585, batch 0, Loss 0.00275140767917037\n",
      "Training: Epoch 585, batch 1, Loss 0.0020071659237146378\n",
      "Training: Epoch 585, batch 2, Loss 0.0026023087557405233\n",
      "Training: Epoch 585, batch 3, Loss 0.0019827482756227255\n",
      "Training: Epoch 588, batch 0, Loss 0.0032838957849889994\n",
      "Training: Epoch 588, batch 1, Loss 0.002093781251460314\n",
      "Training: Epoch 588, batch 2, Loss 0.0024635924492031336\n",
      "Training: Epoch 588, batch 3, Loss 0.002573627745732665\n",
      "Validation: Epoch 590, batch 0, Loss 0.002436678623780608\n",
      "Training: Epoch 591, batch 0, Loss 0.002701848279684782\n",
      "Training: Epoch 591, batch 1, Loss 0.0029176564421504736\n",
      "Training: Epoch 591, batch 2, Loss 0.002562375506386161\n",
      "Training: Epoch 591, batch 3, Loss 0.0023688124492764473\n",
      "Training: Epoch 594, batch 0, Loss 0.0035051133017987013\n",
      "Training: Epoch 594, batch 1, Loss 0.00438147084787488\n",
      "Training: Epoch 594, batch 2, Loss 0.0025688353925943375\n",
      "Training: Epoch 594, batch 3, Loss 0.002902096137404442\n",
      "Training: Epoch 597, batch 0, Loss 0.003554342780262232\n",
      "Training: Epoch 597, batch 1, Loss 0.00413229176774621\n",
      "Training: Epoch 597, batch 2, Loss 0.0025286683812737465\n",
      "Training: Epoch 597, batch 3, Loss 0.002900044433772564\n",
      "Training: Epoch 600, batch 0, Loss 0.002683969447389245\n",
      "Training: Epoch 600, batch 1, Loss 0.002748850965872407\n",
      "Training: Epoch 600, batch 2, Loss 0.0027986669447273016\n",
      "Training: Epoch 600, batch 3, Loss 0.0023176902905106544\n",
      "Validation: Epoch 600, batch 0, Loss 0.0023060585372149944\n",
      "Training: Epoch 603, batch 0, Loss 0.003137385006994009\n",
      "Training: Epoch 603, batch 1, Loss 0.002478391630575061\n",
      "Training: Epoch 603, batch 2, Loss 0.001906168763525784\n",
      "Training: Epoch 603, batch 3, Loss 0.0024707175325602293\n",
      "Training: Epoch 606, batch 0, Loss 0.0022910344414412975\n",
      "Training: Epoch 606, batch 1, Loss 0.0024501674342900515\n",
      "Training: Epoch 606, batch 2, Loss 0.002850055694580078\n",
      "Training: Epoch 606, batch 3, Loss 0.001801803125999868\n",
      "Training: Epoch 609, batch 0, Loss 0.0023820679634809494\n",
      "Training: Epoch 609, batch 1, Loss 0.002347436035051942\n",
      "Training: Epoch 609, batch 2, Loss 0.0020206039771437645\n",
      "Training: Epoch 609, batch 3, Loss 0.003652809886261821\n",
      "Validation: Epoch 610, batch 0, Loss 0.0021181413903832436\n",
      "Training: Epoch 612, batch 0, Loss 0.0025895072612911463\n",
      "Training: Epoch 612, batch 1, Loss 0.002213271800428629\n",
      "Training: Epoch 612, batch 2, Loss 0.001923968200571835\n",
      "Training: Epoch 612, batch 3, Loss 0.0036649007815867662\n",
      "Training: Epoch 615, batch 0, Loss 0.00210894295014441\n",
      "Training: Epoch 615, batch 1, Loss 0.002859695116057992\n",
      "Training: Epoch 615, batch 2, Loss 0.0025397243443876505\n",
      "Training: Epoch 615, batch 3, Loss 0.0021115366835147142\n",
      "Training: Epoch 618, batch 0, Loss 0.0022066254168748856\n",
      "Training: Epoch 618, batch 1, Loss 0.0028514082077890635\n",
      "Training: Epoch 618, batch 2, Loss 0.002400966826826334\n",
      "Training: Epoch 618, batch 3, Loss 0.0020496868528425694\n",
      "Validation: Epoch 620, batch 0, Loss 0.0021081585437059402\n",
      "Training: Epoch 621, batch 0, Loss 0.002425504382699728\n",
      "Training: Epoch 621, batch 1, Loss 0.002104332437738776\n",
      "Training: Epoch 621, batch 2, Loss 0.0022572078742086887\n",
      "Training: Epoch 621, batch 3, Loss 0.0035605204757303\n",
      "Training: Epoch 624, batch 0, Loss 0.002217245986685157\n",
      "Training: Epoch 624, batch 1, Loss 0.002544770482927561\n",
      "Training: Epoch 624, batch 2, Loss 0.002289608819410205\n",
      "Training: Epoch 624, batch 3, Loss 0.0019087997497990727\n",
      "Training: Epoch 627, batch 0, Loss 0.0026855196338146925\n",
      "Training: Epoch 627, batch 1, Loss 0.002603271510452032\n",
      "Training: Epoch 627, batch 2, Loss 0.0019934019073843956\n",
      "Training: Epoch 627, batch 3, Loss 0.002092212438583374\n",
      "Training: Epoch 630, batch 0, Loss 0.002705831779167056\n",
      "Training: Epoch 630, batch 1, Loss 0.0022992705926299095\n",
      "Training: Epoch 630, batch 2, Loss 0.0019383091712370515\n",
      "Training: Epoch 630, batch 3, Loss 0.00230621756054461\n",
      "Validation: Epoch 630, batch 0, Loss 0.0020483131520450115\n",
      "Training: Epoch 633, batch 0, Loss 0.002644456923007965\n",
      "Training: Epoch 633, batch 1, Loss 0.002145044971257448\n",
      "Training: Epoch 633, batch 2, Loss 0.002122027799487114\n",
      "Training: Epoch 633, batch 3, Loss 0.0018764544511213899\n",
      "Training: Epoch 636, batch 0, Loss 0.0023385274689644575\n",
      "Training: Epoch 636, batch 1, Loss 0.00201563723385334\n",
      "Training: Epoch 636, batch 2, Loss 0.0026566737797111273\n",
      "Training: Epoch 636, batch 3, Loss 0.0014571164501830935\n",
      "Training: Epoch 639, batch 0, Loss 0.0029821121133863926\n",
      "Training: Epoch 639, batch 1, Loss 0.0025682083796709776\n",
      "Training: Epoch 639, batch 2, Loss 0.0023637523408979177\n",
      "Training: Epoch 639, batch 3, Loss 0.0018413877114653587\n",
      "Validation: Epoch 640, batch 0, Loss 0.002092296490445733\n",
      "Training: Epoch 642, batch 0, Loss 0.0019827173091471195\n",
      "Training: Epoch 642, batch 1, Loss 0.0020684227347373962\n",
      "Training: Epoch 642, batch 2, Loss 0.0025873149279505014\n",
      "Training: Epoch 642, batch 3, Loss 0.003270492423325777\n",
      "Training: Epoch 645, batch 0, Loss 0.0023296261206269264\n",
      "Training: Epoch 645, batch 1, Loss 0.0020390779245644808\n",
      "Training: Epoch 645, batch 2, Loss 0.0029338542371988297\n",
      "Training: Epoch 645, batch 3, Loss 0.0019014270510524511\n",
      "Training: Epoch 648, batch 0, Loss 0.0023413877934217453\n",
      "Training: Epoch 648, batch 1, Loss 0.002555494662374258\n",
      "Training: Epoch 648, batch 2, Loss 0.0018934101099148393\n",
      "Training: Epoch 648, batch 3, Loss 0.0035118949599564075\n",
      "Validation: Epoch 650, batch 0, Loss 0.00250782398506999\n",
      "Training: Epoch 651, batch 0, Loss 0.003267140593379736\n",
      "Training: Epoch 651, batch 1, Loss 0.0022088750265538692\n",
      "Training: Epoch 651, batch 2, Loss 0.0026055159978568554\n",
      "Training: Epoch 651, batch 3, Loss 0.002189305378124118\n",
      "Training: Epoch 654, batch 0, Loss 0.0020653691608458757\n",
      "Training: Epoch 654, batch 1, Loss 0.0025271372869610786\n",
      "Training: Epoch 654, batch 2, Loss 0.0025569992139935493\n",
      "Training: Epoch 654, batch 3, Loss 0.0034924750216305256\n",
      "Training: Epoch 657, batch 0, Loss 0.0025577342603355646\n",
      "Training: Epoch 657, batch 1, Loss 0.0031908215023577213\n",
      "Training: Epoch 657, batch 2, Loss 0.0025196473579853773\n",
      "Training: Epoch 657, batch 3, Loss 0.002045485656708479\n",
      "Training: Epoch 660, batch 0, Loss 0.0017879154765978456\n",
      "Training: Epoch 660, batch 1, Loss 0.0030800136737525463\n",
      "Training: Epoch 660, batch 2, Loss 0.0018657277105376124\n",
      "Training: Epoch 660, batch 3, Loss 0.00187153578735888\n",
      "Validation: Epoch 660, batch 0, Loss 0.0019124493701383471\n",
      "Training: Epoch 663, batch 0, Loss 0.0019268220057711005\n",
      "Training: Epoch 663, batch 1, Loss 0.002097621327266097\n",
      "Training: Epoch 663, batch 2, Loss 0.0024132856633514166\n",
      "Training: Epoch 663, batch 3, Loss 0.0017308825626969337\n",
      "Training: Epoch 666, batch 0, Loss 0.0031090567354112864\n",
      "Training: Epoch 666, batch 1, Loss 0.001719334744848311\n",
      "Training: Epoch 666, batch 2, Loss 0.0017818904016166925\n",
      "Training: Epoch 666, batch 3, Loss 0.001678973319940269\n",
      "Training: Epoch 669, batch 0, Loss 0.0024576818104833364\n",
      "Training: Epoch 669, batch 1, Loss 0.0017399665666744113\n",
      "Training: Epoch 669, batch 2, Loss 0.0017704691272228956\n",
      "Training: Epoch 669, batch 3, Loss 0.0035152274649590254\n",
      "Validation: Epoch 670, batch 0, Loss 0.001969341654330492\n",
      "Training: Epoch 672, batch 0, Loss 0.0024690532591193914\n",
      "Training: Epoch 672, batch 1, Loss 0.0018898400012403727\n",
      "Training: Epoch 672, batch 2, Loss 0.0023269199300557375\n",
      "Training: Epoch 672, batch 3, Loss 0.001925539574585855\n",
      "Training: Epoch 675, batch 0, Loss 0.002250832039862871\n",
      "Training: Epoch 675, batch 1, Loss 0.0025563782546669245\n",
      "Training: Epoch 675, batch 2, Loss 0.00232246913947165\n",
      "Training: Epoch 675, batch 3, Loss 0.003159905783832073\n",
      "Training: Epoch 678, batch 0, Loss 0.0021982211619615555\n",
      "Training: Epoch 678, batch 1, Loss 0.0019910188857465982\n",
      "Training: Epoch 678, batch 2, Loss 0.0017919644014909863\n",
      "Training: Epoch 678, batch 3, Loss 0.0038575059734284878\n",
      "Validation: Epoch 680, batch 0, Loss 0.0018584125209599733\n",
      "Training: Epoch 681, batch 0, Loss 0.0021505646873265505\n",
      "Training: Epoch 681, batch 1, Loss 0.002459157258272171\n",
      "Training: Epoch 681, batch 2, Loss 0.0018731941236183047\n",
      "Training: Epoch 681, batch 3, Loss 0.0013966604601591825\n",
      "Training: Epoch 684, batch 0, Loss 0.0021311137825250626\n",
      "Training: Epoch 684, batch 1, Loss 0.0018794225761666894\n",
      "Training: Epoch 684, batch 2, Loss 0.0021475316025316715\n",
      "Training: Epoch 684, batch 3, Loss 0.0029230043292045593\n",
      "Training: Epoch 687, batch 0, Loss 0.002552403835579753\n",
      "Training: Epoch 687, batch 1, Loss 0.0028852401301264763\n",
      "Training: Epoch 687, batch 2, Loss 0.0023673763498663902\n",
      "Training: Epoch 687, batch 3, Loss 0.0014776024036109447\n",
      "Training: Epoch 690, batch 0, Loss 0.0023774325381964445\n",
      "Training: Epoch 690, batch 1, Loss 0.002440715441480279\n",
      "Training: Epoch 690, batch 2, Loss 0.0026320458855479956\n",
      "Training: Epoch 690, batch 3, Loss 0.0020023963879793882\n",
      "Validation: Epoch 690, batch 0, Loss 0.002158086048439145\n",
      "Training: Epoch 693, batch 0, Loss 0.0017613652162253857\n",
      "Training: Epoch 693, batch 1, Loss 0.002927883993834257\n",
      "Training: Epoch 693, batch 2, Loss 0.0019013724522665143\n",
      "Training: Epoch 693, batch 3, Loss 0.0014487426960840821\n",
      "Training: Epoch 696, batch 0, Loss 0.0021056344266980886\n",
      "Training: Epoch 696, batch 1, Loss 0.002255054423585534\n",
      "Training: Epoch 696, batch 2, Loss 0.002709083491936326\n",
      "Training: Epoch 696, batch 3, Loss 0.0038590708281844854\n",
      "Training: Epoch 699, batch 0, Loss 0.0017992431530728936\n",
      "Training: Epoch 699, batch 1, Loss 0.002216123742982745\n",
      "Training: Epoch 699, batch 2, Loss 0.0024624639190733433\n",
      "Training: Epoch 699, batch 3, Loss 0.002053702250123024\n",
      "Validation: Epoch 700, batch 0, Loss 0.0024940655566751957\n",
      "Training: Epoch 702, batch 0, Loss 0.002058972604572773\n",
      "Training: Epoch 702, batch 1, Loss 0.002710620639845729\n",
      "Training: Epoch 702, batch 2, Loss 0.0020986793097108603\n",
      "Training: Epoch 702, batch 3, Loss 0.0020225741900503635\n",
      "Training: Epoch 705, batch 0, Loss 0.002168608596548438\n",
      "Training: Epoch 705, batch 1, Loss 0.00219952710904181\n",
      "Training: Epoch 705, batch 2, Loss 0.002235500141978264\n",
      "Training: Epoch 705, batch 3, Loss 0.003400624031201005\n",
      "Training: Epoch 708, batch 0, Loss 0.002781711984425783\n",
      "Training: Epoch 708, batch 1, Loss 0.0018935324624180794\n",
      "Training: Epoch 708, batch 2, Loss 0.0024182905908674\n",
      "Training: Epoch 708, batch 3, Loss 0.0017218064749613404\n",
      "Validation: Epoch 710, batch 0, Loss 0.0019727314356714487\n",
      "Training: Epoch 711, batch 0, Loss 0.002849171869456768\n",
      "Training: Epoch 711, batch 1, Loss 0.002379024401307106\n",
      "Training: Epoch 711, batch 2, Loss 0.0019272054778411984\n",
      "Training: Epoch 711, batch 3, Loss 0.0017610398354008794\n",
      "Training: Epoch 714, batch 0, Loss 0.002652837196364999\n",
      "Training: Epoch 714, batch 1, Loss 0.0020185448229312897\n",
      "Training: Epoch 714, batch 2, Loss 0.001826176536269486\n",
      "Training: Epoch 714, batch 3, Loss 0.004001067019999027\n",
      "Training: Epoch 717, batch 0, Loss 0.0021358642261475325\n",
      "Training: Epoch 717, batch 1, Loss 0.0023749705869704485\n",
      "Training: Epoch 717, batch 2, Loss 0.0022256732918322086\n",
      "Training: Epoch 717, batch 3, Loss 0.002073493553325534\n",
      "Training: Epoch 720, batch 0, Loss 0.002028161194175482\n",
      "Training: Epoch 720, batch 1, Loss 0.0018516865093261003\n",
      "Training: Epoch 720, batch 2, Loss 0.0020384208764880896\n",
      "Training: Epoch 720, batch 3, Loss 0.0034647348802536726\n",
      "Validation: Epoch 720, batch 0, Loss 0.0022058833856135607\n",
      "Training: Epoch 723, batch 0, Loss 0.0021449497435241938\n",
      "Training: Epoch 723, batch 1, Loss 0.0018498987192288041\n",
      "Training: Epoch 723, batch 2, Loss 0.0023877730127424\n",
      "Training: Epoch 723, batch 3, Loss 0.0017481858376413584\n",
      "Training: Epoch 726, batch 0, Loss 0.0019790870137512684\n",
      "Training: Epoch 726, batch 1, Loss 0.0021715371403843164\n",
      "Training: Epoch 726, batch 2, Loss 0.0017795468447729945\n",
      "Training: Epoch 726, batch 3, Loss 0.0033968291245400906\n",
      "Training: Epoch 729, batch 0, Loss 0.0018135629361495376\n",
      "Training: Epoch 729, batch 1, Loss 0.002628181129693985\n",
      "Training: Epoch 729, batch 2, Loss 0.002194242784753442\n",
      "Training: Epoch 729, batch 3, Loss 0.0019010026007890701\n",
      "Validation: Epoch 730, batch 0, Loss 0.001953083323314786\n",
      "Training: Epoch 732, batch 0, Loss 0.002829229459166527\n",
      "Training: Epoch 732, batch 1, Loss 0.001845626742579043\n",
      "Training: Epoch 732, batch 2, Loss 0.0022511626593768597\n",
      "Training: Epoch 732, batch 3, Loss 0.001874057692475617\n",
      "Training: Epoch 735, batch 0, Loss 0.0023384278174489737\n",
      "Training: Epoch 735, batch 1, Loss 0.0019532982259988785\n",
      "Training: Epoch 735, batch 2, Loss 0.0020179867278784513\n",
      "Training: Epoch 735, batch 3, Loss 0.0019812944810837507\n",
      "Training: Epoch 738, batch 0, Loss 0.0039705936796963215\n",
      "Training: Epoch 738, batch 1, Loss 0.0028912804555147886\n",
      "Training: Epoch 738, batch 2, Loss 0.002930662827566266\n",
      "Training: Epoch 738, batch 3, Loss 0.0026624498423188925\n",
      "Validation: Epoch 740, batch 0, Loss 0.0020716518629342318\n",
      "Training: Epoch 741, batch 0, Loss 0.002413948066532612\n",
      "Training: Epoch 741, batch 1, Loss 0.0023845473770052195\n",
      "Training: Epoch 741, batch 2, Loss 0.0019329459173604846\n",
      "Training: Epoch 741, batch 3, Loss 0.0016869369428604841\n",
      "Training: Epoch 744, batch 0, Loss 0.0024888161569833755\n",
      "Training: Epoch 744, batch 1, Loss 0.0022522306535393\n",
      "Training: Epoch 744, batch 2, Loss 0.002089746994897723\n",
      "Training: Epoch 744, batch 3, Loss 0.0014942660927772522\n",
      "Training: Epoch 747, batch 0, Loss 0.0023486376740038395\n",
      "Training: Epoch 747, batch 1, Loss 0.0025355638936161995\n",
      "Training: Epoch 747, batch 2, Loss 0.0022976112086325884\n",
      "Training: Epoch 747, batch 3, Loss 0.0028501853812485933\n",
      "Training: Epoch 750, batch 0, Loss 0.0021735415793955326\n",
      "Training: Epoch 750, batch 1, Loss 0.0024778631050139666\n",
      "Training: Epoch 750, batch 2, Loss 0.00186122243758291\n",
      "Training: Epoch 750, batch 3, Loss 0.004347560927271843\n",
      "Validation: Epoch 750, batch 0, Loss 0.0017182854935526848\n",
      "Training: Epoch 753, batch 0, Loss 0.0017363829538226128\n",
      "Training: Epoch 753, batch 1, Loss 0.002229065401479602\n",
      "Training: Epoch 753, batch 2, Loss 0.002054882002994418\n",
      "Training: Epoch 753, batch 3, Loss 0.00318886642344296\n",
      "Training: Epoch 756, batch 0, Loss 0.0022694433573633432\n",
      "Training: Epoch 756, batch 1, Loss 0.002277649939060211\n",
      "Training: Epoch 756, batch 2, Loss 0.0022454215213656425\n",
      "Training: Epoch 756, batch 3, Loss 0.0028843278996646404\n",
      "Training: Epoch 759, batch 0, Loss 0.0024818931706249714\n",
      "Training: Epoch 759, batch 1, Loss 0.002614394063130021\n",
      "Training: Epoch 759, batch 2, Loss 0.002337140729650855\n",
      "Training: Epoch 759, batch 3, Loss 0.0014415879268199205\n",
      "Validation: Epoch 760, batch 0, Loss 0.002327799564227462\n",
      "Training: Epoch 762, batch 0, Loss 0.0020997689571231604\n",
      "Training: Epoch 762, batch 1, Loss 0.0026817319449037313\n",
      "Training: Epoch 762, batch 2, Loss 0.0020347610116004944\n",
      "Training: Epoch 762, batch 3, Loss 0.001559200813062489\n",
      "Training: Epoch 765, batch 0, Loss 0.0022162399254739285\n",
      "Training: Epoch 765, batch 1, Loss 0.0034154229797422886\n",
      "Training: Epoch 765, batch 2, Loss 0.0018542594043537974\n",
      "Training: Epoch 765, batch 3, Loss 0.002338490914553404\n",
      "Training: Epoch 768, batch 0, Loss 0.002430340275168419\n",
      "Training: Epoch 768, batch 1, Loss 0.0032020341604948044\n",
      "Training: Epoch 768, batch 2, Loss 0.002525590592995286\n",
      "Training: Epoch 768, batch 3, Loss 0.001984748523682356\n",
      "Validation: Epoch 770, batch 0, Loss 0.002034213626757264\n",
      "Training: Epoch 771, batch 0, Loss 0.002002355409786105\n",
      "Training: Epoch 771, batch 1, Loss 0.002056729979813099\n",
      "Training: Epoch 771, batch 2, Loss 0.002452193759381771\n",
      "Training: Epoch 771, batch 3, Loss 0.002219516783952713\n",
      "Training: Epoch 774, batch 0, Loss 0.001994292251765728\n",
      "Training: Epoch 774, batch 1, Loss 0.0022771675139665604\n",
      "Training: Epoch 774, batch 2, Loss 0.002016643527895212\n",
      "Training: Epoch 774, batch 3, Loss 0.0016057303873822093\n",
      "Training: Epoch 777, batch 0, Loss 0.0019182770047336817\n",
      "Training: Epoch 777, batch 1, Loss 0.0022972191218286753\n",
      "Training: Epoch 777, batch 2, Loss 0.001710058655589819\n",
      "Training: Epoch 777, batch 3, Loss 0.0028524009976536036\n",
      "Training: Epoch 780, batch 0, Loss 0.0016742325387895107\n",
      "Training: Epoch 780, batch 1, Loss 0.0019706804305315018\n",
      "Training: Epoch 780, batch 2, Loss 0.002471253974363208\n",
      "Training: Epoch 780, batch 3, Loss 0.002053908072412014\n",
      "Validation: Epoch 780, batch 0, Loss 0.0017431697342544794\n",
      "Training: Epoch 783, batch 0, Loss 0.0021912967786192894\n",
      "Training: Epoch 783, batch 1, Loss 0.00169310939963907\n",
      "Training: Epoch 783, batch 2, Loss 0.0019861506298184395\n",
      "Training: Epoch 783, batch 3, Loss 0.001378523069433868\n",
      "Training: Epoch 786, batch 0, Loss 0.0019301573047414422\n",
      "Training: Epoch 786, batch 1, Loss 0.0016756056575104594\n",
      "Training: Epoch 786, batch 2, Loss 0.002020425396040082\n",
      "Training: Epoch 786, batch 3, Loss 0.002033977070823312\n",
      "Training: Epoch 789, batch 0, Loss 0.0021720565855503082\n",
      "Training: Epoch 789, batch 1, Loss 0.0021467325277626514\n",
      "Training: Epoch 789, batch 2, Loss 0.0015853899531066418\n",
      "Training: Epoch 789, batch 3, Loss 0.0013777135172858834\n",
      "Validation: Epoch 790, batch 0, Loss 0.001966349082067609\n",
      "Training: Epoch 792, batch 0, Loss 0.002279386157169938\n",
      "Training: Epoch 792, batch 1, Loss 0.002021261490881443\n",
      "Training: Epoch 792, batch 2, Loss 0.0016460242914035916\n",
      "Training: Epoch 792, batch 3, Loss 0.0036730479914695024\n",
      "Training: Epoch 795, batch 0, Loss 0.0019711449276655912\n",
      "Training: Epoch 795, batch 1, Loss 0.0021115674171596766\n",
      "Training: Epoch 795, batch 2, Loss 0.0023687100037932396\n",
      "Training: Epoch 795, batch 3, Loss 0.0019098003394901752\n",
      "Training: Epoch 798, batch 0, Loss 0.0023111302871257067\n",
      "Training: Epoch 798, batch 1, Loss 0.0025151518639177084\n",
      "Training: Epoch 798, batch 2, Loss 0.0015951689565554261\n",
      "Training: Epoch 798, batch 3, Loss 0.0017825145041570067\n",
      "Validation: Epoch 800, batch 0, Loss 0.001931339967995882\n",
      "Training: Epoch 801, batch 0, Loss 0.001979307271540165\n",
      "Training: Epoch 801, batch 1, Loss 0.0018874206580221653\n",
      "Training: Epoch 801, batch 2, Loss 0.001887390622869134\n",
      "Training: Epoch 801, batch 3, Loss 0.004313689656555653\n",
      "Training: Epoch 804, batch 0, Loss 0.0017148065380752087\n",
      "Training: Epoch 804, batch 1, Loss 0.0022866667713969946\n",
      "Training: Epoch 804, batch 2, Loss 0.00231202132999897\n",
      "Training: Epoch 804, batch 3, Loss 0.0014994305092841387\n",
      "Training: Epoch 807, batch 0, Loss 0.0028518058825284243\n",
      "Training: Epoch 807, batch 1, Loss 0.002436084672808647\n",
      "Training: Epoch 807, batch 2, Loss 0.0021342455875128508\n",
      "Training: Epoch 807, batch 3, Loss 0.002885988214984536\n",
      "Training: Epoch 810, batch 0, Loss 0.002684959676116705\n",
      "Training: Epoch 810, batch 1, Loss 0.0020432197488844395\n",
      "Training: Epoch 810, batch 2, Loss 0.001839861855842173\n",
      "Training: Epoch 810, batch 3, Loss 0.0031458777375519276\n",
      "Validation: Epoch 810, batch 0, Loss 0.0020136507228016853\n",
      "Training: Epoch 813, batch 0, Loss 0.00257088802754879\n",
      "Training: Epoch 813, batch 1, Loss 0.0026843249797821045\n",
      "Training: Epoch 813, batch 2, Loss 0.0018614328000694513\n",
      "Training: Epoch 813, batch 3, Loss 0.002016044920310378\n",
      "Training: Epoch 816, batch 0, Loss 0.0024192959535866976\n",
      "Training: Epoch 816, batch 1, Loss 0.00294019328430295\n",
      "Training: Epoch 816, batch 2, Loss 0.0025169765576720238\n",
      "Training: Epoch 816, batch 3, Loss 0.0018627289682626724\n",
      "Training: Epoch 819, batch 0, Loss 0.002384090330451727\n",
      "Training: Epoch 819, batch 1, Loss 0.0019472773419693112\n",
      "Training: Epoch 819, batch 2, Loss 0.0021647291723638773\n",
      "Training: Epoch 819, batch 3, Loss 0.004321563057601452\n",
      "Validation: Epoch 820, batch 0, Loss 0.001847569947130978\n",
      "Training: Epoch 822, batch 0, Loss 0.002667519962415099\n",
      "Training: Epoch 822, batch 1, Loss 0.0018874704837799072\n",
      "Training: Epoch 822, batch 2, Loss 0.0018911195220425725\n",
      "Training: Epoch 822, batch 3, Loss 0.002060660859569907\n",
      "Training: Epoch 825, batch 0, Loss 0.001712105586193502\n",
      "Training: Epoch 825, batch 1, Loss 0.002429201500490308\n",
      "Training: Epoch 825, batch 2, Loss 0.0017914241179823875\n",
      "Training: Epoch 825, batch 3, Loss 0.0017115490045398474\n",
      "Training: Epoch 828, batch 0, Loss 0.0020600068382918835\n",
      "Training: Epoch 828, batch 1, Loss 0.0023710613604635\n",
      "Training: Epoch 828, batch 2, Loss 0.001515470794402063\n",
      "Training: Epoch 828, batch 3, Loss 0.0013565975241363049\n",
      "Validation: Epoch 830, batch 0, Loss 0.0017562059219926596\n",
      "Training: Epoch 831, batch 0, Loss 0.0017332796705886722\n",
      "Training: Epoch 831, batch 1, Loss 0.0023155228700488806\n",
      "Training: Epoch 831, batch 2, Loss 0.0015095991548150778\n",
      "Training: Epoch 831, batch 3, Loss 0.0029306975193321705\n",
      "Training: Epoch 834, batch 0, Loss 0.0019187751458957791\n",
      "Training: Epoch 834, batch 1, Loss 0.003133668564260006\n",
      "Training: Epoch 834, batch 2, Loss 0.0022010430693626404\n",
      "Training: Epoch 834, batch 3, Loss 0.006667495705187321\n",
      "Training: Epoch 837, batch 0, Loss 0.01190163567662239\n",
      "Training: Epoch 837, batch 1, Loss 0.017891976982355118\n",
      "Training: Epoch 837, batch 2, Loss 0.01443449966609478\n",
      "Training: Epoch 837, batch 3, Loss 0.00998468417674303\n",
      "Training: Epoch 840, batch 0, Loss 0.007960629649460316\n",
      "Training: Epoch 840, batch 1, Loss 0.006132081151008606\n",
      "Training: Epoch 840, batch 2, Loss 0.004680635407567024\n",
      "Training: Epoch 840, batch 3, Loss 0.004692893475294113\n",
      "Validation: Epoch 840, batch 0, Loss 0.005961763672530651\n",
      "Training: Epoch 843, batch 0, Loss 0.004410244058817625\n",
      "Training: Epoch 843, batch 1, Loss 0.0037785072345286608\n",
      "Training: Epoch 843, batch 2, Loss 0.003842690959572792\n",
      "Training: Epoch 843, batch 3, Loss 0.0036099443677812815\n",
      "Training: Epoch 846, batch 0, Loss 0.0034527345560491085\n",
      "Training: Epoch 846, batch 1, Loss 0.0028685834258794785\n",
      "Training: Epoch 846, batch 2, Loss 0.0029330886900424957\n",
      "Training: Epoch 846, batch 3, Loss 0.0038817599415779114\n",
      "Training: Epoch 849, batch 0, Loss 0.0024092840030789375\n",
      "Training: Epoch 849, batch 1, Loss 0.003515455173328519\n",
      "Training: Epoch 849, batch 2, Loss 0.0022391865495592356\n",
      "Training: Epoch 849, batch 3, Loss 0.0025473113637417555\n",
      "Validation: Epoch 850, batch 0, Loss 0.002280811546370387\n",
      "Training: Epoch 852, batch 0, Loss 0.002361166523769498\n",
      "Training: Epoch 852, batch 1, Loss 0.0026662563905119896\n",
      "Training: Epoch 852, batch 2, Loss 0.0022956656757742167\n",
      "Training: Epoch 852, batch 3, Loss 0.0020303630735725164\n",
      "Training: Epoch 855, batch 0, Loss 0.002688746200874448\n",
      "Training: Epoch 855, batch 1, Loss 0.002211947226896882\n",
      "Training: Epoch 855, batch 2, Loss 0.0029161665588617325\n",
      "Training: Epoch 855, batch 3, Loss 0.0033152569085359573\n",
      "Training: Epoch 858, batch 0, Loss 0.002813773462548852\n",
      "Training: Epoch 858, batch 1, Loss 0.002042332896962762\n",
      "Training: Epoch 858, batch 2, Loss 0.0021607924718409777\n",
      "Training: Epoch 858, batch 3, Loss 0.0014471720205619931\n",
      "Validation: Epoch 860, batch 0, Loss 0.001806903979741037\n",
      "Training: Epoch 861, batch 0, Loss 0.0022167915012687445\n",
      "Training: Epoch 861, batch 1, Loss 0.002194096567109227\n",
      "Training: Epoch 861, batch 2, Loss 0.0020014436449855566\n",
      "Training: Epoch 861, batch 3, Loss 0.0018782397964969277\n",
      "Training: Epoch 864, batch 0, Loss 0.001991527620702982\n",
      "Training: Epoch 864, batch 1, Loss 0.0027815918438136578\n",
      "Training: Epoch 864, batch 2, Loss 0.001634324318729341\n",
      "Training: Epoch 864, batch 3, Loss 0.0020375591702759266\n",
      "Training: Epoch 867, batch 0, Loss 0.0021331857424229383\n",
      "Training: Epoch 867, batch 1, Loss 0.0016926000826060772\n",
      "Training: Epoch 867, batch 2, Loss 0.0024710260331630707\n",
      "Training: Epoch 867, batch 3, Loss 0.0020116548985242844\n",
      "Training: Epoch 870, batch 0, Loss 0.0017997572431340814\n",
      "Training: Epoch 870, batch 1, Loss 0.00250662537291646\n",
      "Training: Epoch 870, batch 2, Loss 0.0019029394024983048\n",
      "Training: Epoch 870, batch 3, Loss 0.00169366504997015\n",
      "Validation: Epoch 870, batch 0, Loss 0.0017174665117636323\n",
      "Training: Epoch 873, batch 0, Loss 0.0018570893444120884\n",
      "Training: Epoch 873, batch 1, Loss 0.0026787198148667812\n",
      "Training: Epoch 873, batch 2, Loss 0.001880677416920662\n",
      "Training: Epoch 873, batch 3, Loss 0.001661266665905714\n",
      "Training: Epoch 876, batch 0, Loss 0.0021840352565050125\n",
      "Training: Epoch 876, batch 1, Loss 0.0017515604849904776\n",
      "Training: Epoch 876, batch 2, Loss 0.0018758631777018309\n",
      "Training: Epoch 876, batch 3, Loss 0.0036388454027473927\n",
      "Training: Epoch 879, batch 0, Loss 0.0022570083383470774\n",
      "Training: Epoch 879, batch 1, Loss 0.0020137305837124586\n",
      "Training: Epoch 879, batch 2, Loss 0.0024595619179308414\n",
      "Training: Epoch 879, batch 3, Loss 0.003192387754097581\n",
      "Validation: Epoch 880, batch 0, Loss 0.00212533725425601\n",
      "Training: Epoch 882, batch 0, Loss 0.0016149202128872275\n",
      "Training: Epoch 882, batch 1, Loss 0.00262643164023757\n",
      "Training: Epoch 882, batch 2, Loss 0.002249474637210369\n",
      "Training: Epoch 882, batch 3, Loss 0.0038111708126962185\n",
      "Training: Epoch 885, batch 0, Loss 0.002093304181471467\n",
      "Training: Epoch 885, batch 1, Loss 0.0022184012923389673\n",
      "Training: Epoch 885, batch 2, Loss 0.0023859248030930758\n",
      "Training: Epoch 885, batch 3, Loss 0.003518388606607914\n",
      "Training: Epoch 888, batch 0, Loss 0.0017282202607020736\n",
      "Training: Epoch 888, batch 1, Loss 0.002163901459425688\n",
      "Training: Epoch 888, batch 2, Loss 0.002392666880041361\n",
      "Training: Epoch 888, batch 3, Loss 0.0017047583824023604\n",
      "Validation: Epoch 890, batch 0, Loss 0.0016740824794396758\n",
      "Training: Epoch 891, batch 0, Loss 0.001615150598809123\n",
      "Training: Epoch 891, batch 1, Loss 0.0019170932937413454\n",
      "Training: Epoch 891, batch 2, Loss 0.0021797209046781063\n",
      "Training: Epoch 891, batch 3, Loss 0.0015493488172069192\n",
      "Training: Epoch 894, batch 0, Loss 0.0015556951984763145\n",
      "Training: Epoch 894, batch 1, Loss 0.002156088827177882\n",
      "Training: Epoch 894, batch 2, Loss 0.0018755889032036066\n",
      "Training: Epoch 894, batch 3, Loss 0.0013551403535529971\n",
      "Training: Epoch 897, batch 0, Loss 0.002040710998699069\n",
      "Training: Epoch 897, batch 1, Loss 0.002074278425425291\n",
      "Training: Epoch 897, batch 2, Loss 0.001616002875380218\n",
      "Training: Epoch 897, batch 3, Loss 0.0015117821749299765\n",
      "Training: Epoch 900, batch 0, Loss 0.002161936368793249\n",
      "Training: Epoch 900, batch 1, Loss 0.0020002457313239574\n",
      "Training: Epoch 900, batch 2, Loss 0.0016315134707838297\n",
      "Training: Epoch 900, batch 3, Loss 0.0012281270464882255\n",
      "Validation: Epoch 900, batch 0, Loss 0.0016476039309054613\n",
      "Training: Epoch 903, batch 0, Loss 0.00232972321100533\n",
      "Training: Epoch 903, batch 1, Loss 0.002047580201178789\n",
      "Training: Epoch 903, batch 2, Loss 0.0016604484990239143\n",
      "Training: Epoch 903, batch 3, Loss 0.0023937944788485765\n",
      "Training: Epoch 906, batch 0, Loss 0.0018478468991816044\n",
      "Training: Epoch 906, batch 1, Loss 0.0019719547126442194\n",
      "Training: Epoch 906, batch 2, Loss 0.0017607130575925112\n",
      "Training: Epoch 906, batch 3, Loss 0.0030618379823863506\n",
      "Training: Epoch 909, batch 0, Loss 0.001651419443078339\n",
      "Training: Epoch 909, batch 1, Loss 0.001979793654754758\n",
      "Training: Epoch 909, batch 2, Loss 0.001599416951648891\n",
      "Training: Epoch 909, batch 3, Loss 0.003805907443165779\n",
      "Validation: Epoch 910, batch 0, Loss 0.001792038674466312\n",
      "Training: Epoch 912, batch 0, Loss 0.0017350673442706466\n",
      "Training: Epoch 912, batch 1, Loss 0.001997062237933278\n",
      "Training: Epoch 912, batch 2, Loss 0.0023637220729142427\n",
      "Training: Epoch 912, batch 3, Loss 0.0015973309054970741\n",
      "Training: Epoch 915, batch 0, Loss 0.0016502267681062222\n",
      "Training: Epoch 915, batch 1, Loss 0.002182944444939494\n",
      "Training: Epoch 915, batch 2, Loss 0.00171088392380625\n",
      "Training: Epoch 915, batch 3, Loss 0.0028812685050070286\n",
      "Training: Epoch 918, batch 0, Loss 0.0015467922203242779\n",
      "Training: Epoch 918, batch 1, Loss 0.0017852798337116838\n",
      "Training: Epoch 918, batch 2, Loss 0.0025307033210992813\n",
      "Training: Epoch 918, batch 3, Loss 0.0013244064757600427\n",
      "Validation: Epoch 920, batch 0, Loss 0.0015416847309097648\n",
      "Training: Epoch 921, batch 0, Loss 0.001690488774329424\n",
      "Training: Epoch 921, batch 1, Loss 0.0017484541749581695\n",
      "Training: Epoch 921, batch 2, Loss 0.0021277221385389566\n",
      "Training: Epoch 921, batch 3, Loss 0.0018238164484500885\n",
      "Training: Epoch 924, batch 0, Loss 0.0018284567631781101\n",
      "Training: Epoch 924, batch 1, Loss 0.0016269807238131762\n",
      "Training: Epoch 924, batch 2, Loss 0.0021750680170953274\n",
      "Training: Epoch 924, batch 3, Loss 0.0016050871927291155\n",
      "Training: Epoch 927, batch 0, Loss 0.0024445252493023872\n",
      "Training: Epoch 927, batch 1, Loss 0.001732144970446825\n",
      "Training: Epoch 927, batch 2, Loss 0.001423806301318109\n",
      "Training: Epoch 927, batch 3, Loss 0.001571485074236989\n",
      "Training: Epoch 930, batch 0, Loss 0.0014554259832948446\n",
      "Training: Epoch 930, batch 1, Loss 0.001546363695524633\n",
      "Training: Epoch 930, batch 2, Loss 0.0024781739339232445\n",
      "Training: Epoch 930, batch 3, Loss 0.001588822458870709\n",
      "Validation: Epoch 930, batch 0, Loss 0.0015304444823414087\n",
      "Training: Epoch 933, batch 0, Loss 0.002339489758014679\n",
      "Training: Epoch 933, batch 1, Loss 0.0014127767644822598\n",
      "Training: Epoch 933, batch 2, Loss 0.0016817303840070963\n",
      "Training: Epoch 933, batch 3, Loss 0.0015490417135879397\n",
      "Training: Epoch 936, batch 0, Loss 0.0015898675192147493\n",
      "Training: Epoch 936, batch 1, Loss 0.0017095002112910151\n",
      "Training: Epoch 936, batch 2, Loss 0.00183202070184052\n",
      "Training: Epoch 936, batch 3, Loss 0.0030339777003973722\n",
      "Training: Epoch 939, batch 0, Loss 0.0019556840416043997\n",
      "Training: Epoch 939, batch 1, Loss 0.002788648707792163\n",
      "Training: Epoch 939, batch 2, Loss 0.00199381192214787\n",
      "Training: Epoch 939, batch 3, Loss 0.0015735881170257926\n",
      "Validation: Epoch 940, batch 0, Loss 0.0017139110714197159\n",
      "Training: Epoch 942, batch 0, Loss 0.0017388268606737256\n",
      "Training: Epoch 942, batch 1, Loss 0.0018026906764134765\n",
      "Training: Epoch 942, batch 2, Loss 0.002377189928665757\n",
      "Training: Epoch 942, batch 3, Loss 0.00409938208758831\n",
      "Training: Epoch 945, batch 0, Loss 0.0028241423424333334\n",
      "Training: Epoch 945, batch 1, Loss 0.0017406815895810723\n",
      "Training: Epoch 945, batch 2, Loss 0.0025561871007084846\n",
      "Training: Epoch 945, batch 3, Loss 0.001273549161851406\n",
      "Training: Epoch 948, batch 0, Loss 0.001937618711963296\n",
      "Training: Epoch 948, batch 1, Loss 0.0019561289809644222\n",
      "Training: Epoch 948, batch 2, Loss 0.0023835075553506613\n",
      "Training: Epoch 948, batch 3, Loss 0.001531612011604011\n",
      "Validation: Epoch 950, batch 0, Loss 0.001477217418141663\n",
      "Training: Epoch 951, batch 0, Loss 0.0018898252164945006\n",
      "Training: Epoch 951, batch 1, Loss 0.001896182424388826\n",
      "Training: Epoch 951, batch 2, Loss 0.0019472265848889947\n",
      "Training: Epoch 951, batch 3, Loss 0.0012751334579661489\n",
      "Training: Epoch 954, batch 0, Loss 0.0017259246669709682\n",
      "Training: Epoch 954, batch 1, Loss 0.001475999248214066\n",
      "Training: Epoch 954, batch 2, Loss 0.0022524716332554817\n",
      "Training: Epoch 954, batch 3, Loss 0.001418550731614232\n",
      "Training: Epoch 957, batch 0, Loss 0.0014566625468432903\n",
      "Training: Epoch 957, batch 1, Loss 0.002049695933237672\n",
      "Training: Epoch 957, batch 2, Loss 0.0019038200844079256\n",
      "Training: Epoch 957, batch 3, Loss 0.0016148772556334734\n",
      "Training: Epoch 960, batch 0, Loss 0.0018123926129192114\n",
      "Training: Epoch 960, batch 1, Loss 0.0015183191280812025\n",
      "Training: Epoch 960, batch 2, Loss 0.002043004846200347\n",
      "Training: Epoch 960, batch 3, Loss 0.0015234688762575388\n",
      "Validation: Epoch 960, batch 0, Loss 0.0015223065856844187\n",
      "Training: Epoch 963, batch 0, Loss 0.0019212124170735478\n",
      "Training: Epoch 963, batch 1, Loss 0.0018090136582031846\n",
      "Training: Epoch 963, batch 2, Loss 0.0017502570990473032\n",
      "Training: Epoch 963, batch 3, Loss 0.0010512069566175342\n",
      "Training: Epoch 966, batch 0, Loss 0.002102379221469164\n",
      "Training: Epoch 966, batch 1, Loss 0.001920066075399518\n",
      "Training: Epoch 966, batch 2, Loss 0.0014985445886850357\n",
      "Training: Epoch 966, batch 3, Loss 0.001631612190976739\n",
      "Training: Epoch 969, batch 0, Loss 0.0017487844452261925\n",
      "Training: Epoch 969, batch 1, Loss 0.0020467217545956373\n",
      "Training: Epoch 969, batch 2, Loss 0.0016586100682616234\n",
      "Training: Epoch 969, batch 3, Loss 0.0014613349922001362\n",
      "Validation: Epoch 970, batch 0, Loss 0.0014931022888049483\n",
      "Training: Epoch 972, batch 0, Loss 0.0015567125519737601\n",
      "Training: Epoch 972, batch 1, Loss 0.0017513401107862592\n",
      "Training: Epoch 972, batch 2, Loss 0.0017470784951001406\n",
      "Training: Epoch 972, batch 3, Loss 0.0019936144817620516\n",
      "Training: Epoch 975, batch 0, Loss 0.0014098372776061296\n",
      "Training: Epoch 975, batch 1, Loss 0.001561601529829204\n",
      "Training: Epoch 975, batch 2, Loss 0.0019535725004971027\n",
      "Training: Epoch 975, batch 3, Loss 0.002690526656806469\n",
      "Training: Epoch 978, batch 0, Loss 0.001985998125746846\n",
      "Training: Epoch 978, batch 1, Loss 0.0016512905713170767\n",
      "Training: Epoch 978, batch 2, Loss 0.001696850871667266\n",
      "Training: Epoch 978, batch 3, Loss 0.0016648441087454557\n",
      "Validation: Epoch 980, batch 0, Loss 0.0015443403972312808\n",
      "Training: Epoch 981, batch 0, Loss 0.0014156714314594865\n",
      "Training: Epoch 981, batch 1, Loss 0.002305532805621624\n",
      "Training: Epoch 981, batch 2, Loss 0.0019366707419976592\n",
      "Training: Epoch 981, batch 3, Loss 0.0013634543865919113\n",
      "Training: Epoch 984, batch 0, Loss 0.0017339321784675121\n",
      "Training: Epoch 984, batch 1, Loss 0.0018758876249194145\n",
      "Training: Epoch 984, batch 2, Loss 0.002051091520115733\n",
      "Training: Epoch 984, batch 3, Loss 0.0016931911231949925\n",
      "Training: Epoch 987, batch 0, Loss 0.0016141051892191172\n",
      "Training: Epoch 987, batch 1, Loss 0.0018281120574101806\n",
      "Training: Epoch 987, batch 2, Loss 0.0017852565506473184\n",
      "Training: Epoch 987, batch 3, Loss 0.0038411277346313\n",
      "Training: Epoch 990, batch 0, Loss 0.001689164200797677\n",
      "Training: Epoch 990, batch 1, Loss 0.0018365009455010295\n",
      "Training: Epoch 990, batch 2, Loss 0.002128718886524439\n",
      "Training: Epoch 990, batch 3, Loss 0.0012051023077219725\n",
      "Validation: Epoch 990, batch 0, Loss 0.0016572814201936126\n",
      "Training: Epoch 993, batch 0, Loss 0.0017276828875765204\n",
      "Training: Epoch 993, batch 1, Loss 0.0021240811329334974\n",
      "Training: Epoch 993, batch 2, Loss 0.0014386557741090655\n",
      "Training: Epoch 993, batch 3, Loss 0.0012320426758378744\n",
      "Training: Epoch 996, batch 0, Loss 0.001745418063364923\n",
      "Training: Epoch 996, batch 1, Loss 0.0018422354478389025\n",
      "Training: Epoch 996, batch 2, Loss 0.0017952233320102096\n",
      "Training: Epoch 996, batch 3, Loss 0.0013902949867770076\n",
      "Training: Epoch 999, batch 0, Loss 0.0020044322591274977\n",
      "Training: Epoch 999, batch 1, Loss 0.0016731171635910869\n",
      "Training: Epoch 999, batch 2, Loss 0.0017426798585802317\n",
      "Training: Epoch 999, batch 3, Loss 0.001740152481943369\n"
     ]
    }
   ],
   "source": [
    "def deepCME_training_loop(model, train_loader, optimizer, n_epochs, validation_loader=None, scheduler=None, sigma_decay_factor=0.5, sigma_decay_every=200, sigmamin=0.1):\n",
    "    training_loss = []\n",
    "    validation_loss = []\n",
    "    for epoch in range(n_epochs):\n",
    "        for i, (X, Y, R) in enumerate(train_loader):\n",
    "            X = X.to(model.device)\n",
    "            Y = Y.to(model.device)\n",
    "            R = R.to(model.device)\n",
    "            def closure():\n",
    "                optimizer.zero_grad()\n",
    "                loss = model.loss(X, Y, R)\n",
    "                training_loss.append((epoch*len(train_loader) + i, loss.item()))\n",
    "                loss.backward()\n",
    "                return loss\n",
    "            optimizer.step(closure)\n",
    "            if epoch % 3 == 0 :\n",
    "                print(f'Training: Epoch {epoch}, batch {i}, Loss {training_loss[-1][1]}')\n",
    "        if scheduler is not None:\n",
    "            scheduler.step()\n",
    "        if epoch % sigma_decay_every == 0 and epoch > 0:\n",
    "            if model.likelihood_parameters[\"noise_covariance\"][0,0] > sigmamin:\n",
    "                model.likelihood_parameters[\"noise_covariance\"] *= sigma_decay_factor\n",
    "                print(f\"Decaying sigma to {model.likelihood_parameters['noise_covariance']}\")\n",
    "\n",
    "        if epoch % 10 == 0 and validation_loader is not None:\n",
    "            for i, (X, Y, R) in enumerate(validation_loader):\n",
    "                with torch.no_grad():\n",
    "                    X = X.to(model.device)\n",
    "                    Y = Y.to(model.device)\n",
    "                    R = R.to(model.device)\n",
    "                    loss = model.loss(X, Y, R)\n",
    "                    validation_loss.append((epoch*len(validation_loader) + i, loss.item()))\n",
    "                    if i % 1 == 0:\n",
    "                        print(f'Validation: Epoch {epoch}, batch {i}, Loss {loss.item()}')\n",
    "    return training_loss, validation_loss\n",
    "\n",
    "def chain_training_loop(chain, train_loader, n_epochs, validation_loader=None, lr=0.01, sigma_decay_factor=0.5, sigma_decay_every=200, sigmamin=0.1):\n",
    "    for i, model in enumerate(chain):\n",
    "        print(f\"+++++ Training model {model.position_in_the_chain} +++++\")\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "        #optimizer = torch.optim.LBFGS(model.parameters(), lr=lr)\n",
    "        training_loss, validation_loss = deepCME_training_loop(model, train_loader, optimizer, n_epochs, validation_loader=validation_loader, sigma_decay_factor=sigma_decay_factor, sigma_decay_every=sigma_decay_every, sigmamin=sigmamin)\n",
    "        model.freeze()\n",
    "\n",
    "\n",
    "chain_training_loop(chain, train_dataset, 1000, validation_loader=val_dataset, lr=0.01, sigma_decay_factor=0.5, sigma_decay_every=3, sigmamin=sigma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "## single model training\n",
    "# FilteringDeepCME = FilteringDeepCME(backbone, x_encoder, y_encoder, baseline_net, tau_times, measurement_times, g_functions, temporal_feature_extractor, R, K, O, position_in_the_chain=n_Y_measurements-2, n_NN_in_chain=n_Y_measurements-1, device=device, h_transform=h_fun, likelihood=likelihood, likelihood_parameters=likelihood_parameters)\n",
    "# optimizer = torch.optim.Adam(filteringDeepCME.parameters(), lr=0.01)\n",
    "# n_epochs = 1000\n",
    "# training_loss, validation_loss = deepCME_training_loop(filteringDeepCME, train_dataset, optimizer, n_epochs, validation_loader=val_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "G0: tensor(0.2933, device='cuda:0')\n",
      "G1: tensor(0.6662, device='cuda:0')\n",
      "mRNA: tensor(2.1122, device='cuda:0')\n",
      "results: tensor([0.1054, 0.2393, 0.7588, 0.3593], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "results = chain[-2].eval_baseline(0., torch.tensor([[0., 1., 3.]]).to(device), torch.tensor([[[1.], [2.]]]).to(device))[0]\n",
    "\n",
    "print(\"G0:\", results[0]/results[3])\n",
    "print(\"G1:\", results[1]/results[3])\n",
    "print(\"mRNA:\", results[2]/results[3])\n",
    "print(\"results:\", results)\n",
    "\n",
    "# results = chain[-1].eval_baseline(0., torch.tensor([[1.]]).to(device), torch.tensor([[[3.]]]).to(device))[0]\n",
    "\n",
    "# print(\"G0:\", results[0]/results[1])\n",
    "# print(\"results:\", results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "G0: tensor(0.1615, device='cuda:0')\n",
      "G1: tensor(0.8533, device='cuda:0')\n",
      "mRNA: tensor(6.0901, device='cuda:0')\n",
      "results: tensor([0.0013, 0.0066, 0.0473, 0.0078], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "def vectorize(initial_states):\n",
    "    out = []\n",
    "    for d in initial_states:\n",
    "        out.append([[d['G0'], d['G1'], d['mRNA']]])\n",
    "    return torch.tensor(out).float()\n",
    "\n",
    "initial_conditions = vectorize(initial_states)\n",
    "\n",
    "def avg_martingale(model, initial_states, Y):\n",
    "    initial_conditions = vectorize(initial_states).to(Y)\n",
    "    results = None\n",
    "    for ic in initial_conditions:\n",
    "        if results is None:\n",
    "            results = model.eval_baseline(0., ic, Y)[0]\n",
    "        else:\n",
    "            results += model.eval_baseline(0., ic, Y)[0]\n",
    "\n",
    "    return results/len(initial_conditions)\n",
    "\n",
    "results = avg_martingale(chain[-2], initial_states, torch.tensor([[[3.], [4.]]]).to(device))\n",
    "\n",
    "print(\"G0:\", results[0]/results[3])\n",
    "print(\"G1:\", results[1]/results[3])\n",
    "print(\"mRNA:\", results[2]/results[3])\n",
    "print(\"results:\", results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.0013, device='cuda:0')"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.211000</td>\n",
       "      <td>0.157667</td>\n",
       "      <td>0.000667</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.179333</td>\n",
       "      <td>0.212667</td>\n",
       "      <td>0.007000</td>\n",
       "      <td>0.002333</td>\n",
       "      <td>0.000333</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.025000</td>\n",
       "      <td>0.042333</td>\n",
       "      <td>0.015667</td>\n",
       "      <td>0.003000</td>\n",
       "      <td>0.000333</td>\n",
       "      <td>0.000667</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.020333</td>\n",
       "      <td>0.038000</td>\n",
       "      <td>0.028000</td>\n",
       "      <td>0.009000</td>\n",
       "      <td>0.001333</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.007333</td>\n",
       "      <td>0.015333</td>\n",
       "      <td>0.012333</td>\n",
       "      <td>0.007667</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.000333</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000333</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          0         1         2         3         4         5    6         7\n",
       "0  0.211000  0.157667  0.000667  0.001000  0.000000  0.000000  0.0  0.000000\n",
       "1  0.179333  0.212667  0.007000  0.002333  0.000333  0.000000  0.0  0.000000\n",
       "2  0.025000  0.042333  0.015667  0.003000  0.000333  0.000667  0.0  0.000000\n",
       "3  0.020333  0.038000  0.028000  0.009000  0.001333  0.000000  0.0  0.000000\n",
       "4  0.007333  0.015333  0.012333  0.007667  0.001000  0.000333  0.0  0.000333"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0013, 0.0066, 0.0473, 0.0078], device='cuda:0')"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "ename": "Exception",
     "evalue": "Stop here",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mException\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[33], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mStop here\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mException\u001b[0m: Stop here"
     ]
    }
   ],
   "source": [
    "raise Exception(\"Stop here\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural Network setup\n",
    "\n",
    "We omit the definition of the CRN, which is done at the beginning of the file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total number of parameters:  6322\n"
     ]
    }
   ],
   "source": [
    "from ElenaNetworks import NeuralMartingale, NeuralMartingaleChain\n",
    "from OtherNetworks import RNNEncoder, MLP\n",
    "\n",
    "from ElenaLosses import likelihood_GaussianNoise_unbatched, likelihood_constant_one\n",
    "\n",
    "Delta = torch.tensor(MI.stoichiometric_matrix)\n",
    "\n",
    "# we need a version of the h_function that is efficient for torch computations (maybe?)\n",
    "\n",
    "hx_number_of_dimensions = 1\n",
    "gx_number_of_dimensions = 2\n",
    "\n",
    "class MLP_projecting(MLP):\n",
    "\n",
    "    def __init__(self, *argv, **kwargs):\n",
    "        super(MLP_projecting, self).__init__(*argv, **kwargs)\n",
    "        pass\n",
    "\n",
    "    def forward(self, x):\n",
    "        # remove the 'k related' dimensions\n",
    "        return super(MLP_projecting, self).forward(x[:, 0])\n",
    "    \n",
    "\n",
    "XEncoder = MLP(MI.get_number_of_species(), 32, 32, 2, activation=torch.nn.Tanh)\n",
    "YEncoder = MLP_projecting(1, 32, 32, 2, activation=torch.nn.Tanh) #RNNEncoder(len(h_function), 64, activation=torch.nn.Tanh)\n",
    "backbone = MLP(1+XEncoder.output_size+YEncoder.output_size, 16, gx_number_of_dimensions, 4, postprocessing_layer=torch.nn.Softplus(), activation=torch.nn.Tanh)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "NN = NeuralMartingale(\n",
    "    XEncoder, YEncoder, backbone,\n",
    "    likelihood_GaussianNoise_unbatched,\n",
    "    {\"noise_covariance\": 0.1*torch.eye(hx_number_of_dimensions)}, # covariance matrix (here the identity)\n",
    "    lambda x: x[2],             # h_transform\n",
    "    lambda x: x[0:2],        # g_function\n",
    "    torch.tensor(MI.stoichiometric_matrix).to(device), # stoichiometry matrix\n",
    "    n_X_measurements_between_Y_measurements, n_Y_measurements,\n",
    "    Xtimes, Ytimes,\n",
    "    gx_number_of_dimensions,\n",
    "    device = device,\n",
    "    paired_batch_size = 10,\n",
    "    batch_size = batch_size\n",
    ")\n",
    "\n",
    "NN_chain = NeuralMartingaleChain(n_Y_measurements-1, NN.to(device)).to(device)\n",
    "# print model size\n",
    "print(\"total number of parameters: \", sum(p.numel() for p in NN_chain.chain[-1].parameters() if p.requires_grad))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 203, 3])"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[1:3].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 2, 102, 3])\n"
     ]
    }
   ],
   "source": [
    "from DeepCME import split_overalp\n",
    "\n",
    "X = next(iter(train_dataset))[0]\n",
    "\n",
    "print(split_overalp(X[1:3], 3).shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training loop\n",
    "\n",
    "now we have to define the training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "Training: Epoch 0, batch 0, Loss 2.005944013595581\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "per epoch training loss (batch average): 0.859007966518402\n",
      "Validation: Epoch 0, batch 0, Loss 0.7401360273361206\n",
      "Validation: Epoch 0, batch 1, Loss 0.4614514410495758\n",
      "Validation: Epoch 0, batch 2, Loss 0.7796158194541931\n",
      "Validation: Epoch 0, batch 3, Loss 0.4553094506263733\n",
      "Validation: Epoch 0, batch 4, Loss 0.6632491946220398\n",
      "per epoch validation loss (batch average): 0.6199523866176605\n",
      "0\n",
      "Training: Epoch 1, batch 0, Loss 0.5184557437896729\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "per epoch training loss (batch average): 0.5098745703697205\n",
      "Validation: Epoch 1, batch 0, Loss 0.6359116435050964\n",
      "Validation: Epoch 1, batch 1, Loss 0.39916789531707764\n",
      "Validation: Epoch 1, batch 2, Loss 0.6391560435295105\n",
      "Validation: Epoch 1, batch 3, Loss 0.1773383915424347\n",
      "Validation: Epoch 1, batch 4, Loss 0.5949072241783142\n",
      "per epoch validation loss (batch average): 0.4892962396144867\n",
      "0\n",
      "Training: Epoch 2, batch 0, Loss 0.9360252618789673\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "per epoch training loss (batch average): 0.5954047739505768\n",
      "Validation: Epoch 2, batch 0, Loss 0.33229032158851624\n",
      "Validation: Epoch 2, batch 1, Loss 0.8186933398246765\n",
      "Validation: Epoch 2, batch 2, Loss 0.8510885238647461\n",
      "Validation: Epoch 2, batch 3, Loss 0.37399062514305115\n",
      "Validation: Epoch 2, batch 4, Loss 0.35438889265060425\n",
      "per epoch validation loss (batch average): 0.5460903406143188\n",
      "0\n",
      "Training: Epoch 3, batch 0, Loss 0.5869789719581604\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "per epoch training loss (batch average): 0.479741758108139\n",
      "Validation: Epoch 3, batch 0, Loss 0.6203782558441162\n",
      "Validation: Epoch 3, batch 1, Loss 0.6950621008872986\n",
      "Validation: Epoch 3, batch 2, Loss 0.45023640990257263\n",
      "Validation: Epoch 3, batch 3, Loss 0.45929455757141113\n",
      "Validation: Epoch 3, batch 4, Loss 0.22948479652404785\n",
      "per epoch validation loss (batch average): 0.49089122414588926\n",
      "0\n",
      "Training: Epoch 4, batch 0, Loss 0.3549942672252655\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "per epoch training loss (batch average): 0.4258118063211441\n",
      "Validation: Epoch 4, batch 0, Loss 0.7262492179870605\n",
      "Validation: Epoch 4, batch 1, Loss 0.5897561311721802\n",
      "Validation: Epoch 4, batch 2, Loss 0.6046406626701355\n",
      "Validation: Epoch 4, batch 3, Loss 0.43445640802383423\n",
      "Validation: Epoch 4, batch 4, Loss 0.48993197083473206\n",
      "per epoch validation loss (batch average): 0.5690068781375885\n",
      "0\n",
      "Training: Epoch 5, batch 0, Loss 0.5317601561546326\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "per epoch training loss (batch average): 0.570420628786087\n",
      "Validation: Epoch 5, batch 0, Loss 0.674667239189148\n",
      "Validation: Epoch 5, batch 1, Loss 0.49661985039711\n",
      "Validation: Epoch 5, batch 2, Loss 0.6428298950195312\n",
      "Validation: Epoch 5, batch 3, Loss 0.39614835381507874\n",
      "Validation: Epoch 5, batch 4, Loss 0.4211387634277344\n",
      "per epoch validation loss (batch average): 0.5262808203697205\n",
      "0\n",
      "Training: Epoch 6, batch 0, Loss 0.4392574429512024\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "per epoch training loss (batch average): 0.4221917450428009\n",
      "Validation: Epoch 6, batch 0, Loss 0.5408920645713806\n",
      "Validation: Epoch 6, batch 1, Loss 0.3829050660133362\n",
      "Validation: Epoch 6, batch 2, Loss 0.6225062608718872\n",
      "Validation: Epoch 6, batch 3, Loss 0.2758398950099945\n",
      "Validation: Epoch 6, batch 4, Loss 0.8795462846755981\n",
      "per epoch validation loss (batch average): 0.5403379142284394\n",
      "0\n",
      "Training: Epoch 7, batch 0, Loss 0.12484600394964218\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "per epoch training loss (batch average): 0.3735793516039848\n",
      "Validation: Epoch 7, batch 0, Loss 0.5112210512161255\n",
      "Validation: Epoch 7, batch 1, Loss 0.5139003992080688\n",
      "Validation: Epoch 7, batch 2, Loss 0.7751647233963013\n",
      "Validation: Epoch 7, batch 3, Loss 0.5510993003845215\n",
      "Validation: Epoch 7, batch 4, Loss 0.21890845894813538\n",
      "per epoch validation loss (batch average): 0.5140587866306305\n",
      "0\n",
      "Training: Epoch 8, batch 0, Loss 0.43582674860954285\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "per epoch training loss (batch average): 0.48749750256538393\n",
      "Validation: Epoch 8, batch 0, Loss 0.45198580622673035\n",
      "Validation: Epoch 8, batch 1, Loss 0.39214250445365906\n",
      "Validation: Epoch 8, batch 2, Loss 0.4937744140625\n",
      "Validation: Epoch 8, batch 3, Loss 0.48984405398368835\n",
      "Validation: Epoch 8, batch 4, Loss 0.5301297307014465\n",
      "per epoch validation loss (batch average): 0.47157530188560487\n",
      "0\n",
      "Training: Epoch 9, batch 0, Loss 0.7221775650978088\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "per epoch training loss (batch average): 0.5000457346439362\n",
      "Validation: Epoch 9, batch 0, Loss 0.4363865852355957\n",
      "Validation: Epoch 9, batch 1, Loss 0.18005827069282532\n",
      "Validation: Epoch 9, batch 2, Loss 0.5956091284751892\n",
      "Validation: Epoch 9, batch 3, Loss 0.19325461983680725\n",
      "Validation: Epoch 9, batch 4, Loss 0.36644822359085083\n",
      "per epoch validation loss (batch average): 0.3543513655662537\n",
      "0\n",
      "Training: Epoch 10, batch 0, Loss 0.5195460915565491\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "per epoch training loss (batch average): 0.5113803029060364\n",
      "Validation: Epoch 10, batch 0, Loss 0.6201577186584473\n",
      "Validation: Epoch 10, batch 1, Loss 0.4599515497684479\n",
      "Validation: Epoch 10, batch 2, Loss 0.4696643054485321\n",
      "Validation: Epoch 10, batch 3, Loss 0.5091463923454285\n",
      "Validation: Epoch 10, batch 4, Loss 0.39395323395729065\n",
      "per epoch validation loss (batch average): 0.4905746400356293\n",
      "0\n",
      "Training: Epoch 11, batch 0, Loss 0.6236757636070251\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "per epoch training loss (batch average): 0.47454552352428436\n",
      "Validation: Epoch 11, batch 0, Loss 0.44375839829444885\n",
      "Validation: Epoch 11, batch 1, Loss 0.5058855414390564\n",
      "Validation: Epoch 11, batch 2, Loss 0.41553035378456116\n",
      "Validation: Epoch 11, batch 3, Loss 0.35150766372680664\n",
      "Validation: Epoch 11, batch 4, Loss 0.7662710547447205\n",
      "per epoch validation loss (batch average): 0.4965906023979187\n",
      "0\n",
      "Training: Epoch 12, batch 0, Loss 0.4762025773525238\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "per epoch training loss (batch average): 0.48684207201004026\n",
      "Validation: Epoch 12, batch 0, Loss 0.6282636523246765\n",
      "Validation: Epoch 12, batch 1, Loss 0.46643123030662537\n",
      "Validation: Epoch 12, batch 2, Loss 0.23886261880397797\n",
      "Validation: Epoch 12, batch 3, Loss 0.3940599858760834\n",
      "Validation: Epoch 12, batch 4, Loss 0.6571519374847412\n",
      "per epoch validation loss (batch average): 0.47695388495922086\n",
      "0\n",
      "Training: Epoch 13, batch 0, Loss 0.5793480277061462\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "per epoch training loss (batch average): 0.42348493337631227\n",
      "Validation: Epoch 13, batch 0, Loss 0.6919525861740112\n",
      "Validation: Epoch 13, batch 1, Loss 0.6284509301185608\n",
      "Validation: Epoch 13, batch 2, Loss 0.42382746934890747\n",
      "Validation: Epoch 13, batch 3, Loss 0.46363434195518494\n",
      "Validation: Epoch 13, batch 4, Loss 0.7285096049308777\n",
      "per epoch validation loss (batch average): 0.5872749865055085\n",
      "0\n",
      "Training: Epoch 14, batch 0, Loss 0.6054360866546631\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "per epoch training loss (batch average): 0.5339546859264374\n",
      "Validation: Epoch 14, batch 0, Loss 0.43988749384880066\n",
      "Validation: Epoch 14, batch 1, Loss 0.7151334285736084\n",
      "Validation: Epoch 14, batch 2, Loss 0.5306055545806885\n",
      "Validation: Epoch 14, batch 3, Loss 0.4055183529853821\n",
      "Validation: Epoch 14, batch 4, Loss 0.5055941343307495\n",
      "per epoch validation loss (batch average): 0.5193477928638458\n",
      "0\n",
      "Training: Epoch 15, batch 0, Loss 0.4249520003795624\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "per epoch training loss (batch average): 0.4201193630695343\n",
      "Validation: Epoch 15, batch 0, Loss 0.35700687766075134\n",
      "Validation: Epoch 15, batch 1, Loss 0.29217490553855896\n",
      "Validation: Epoch 15, batch 2, Loss 0.387825608253479\n",
      "Validation: Epoch 15, batch 3, Loss 0.3802507817745209\n",
      "Validation: Epoch 15, batch 4, Loss 0.5480612516403198\n",
      "per epoch validation loss (batch average): 0.393063884973526\n",
      "0\n",
      "Training: Epoch 16, batch 0, Loss 0.3435271978378296\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "per epoch training loss (batch average): 0.47160531878471373\n",
      "Validation: Epoch 16, batch 0, Loss 0.4240472912788391\n",
      "Validation: Epoch 16, batch 1, Loss 0.32905444502830505\n",
      "Validation: Epoch 16, batch 2, Loss 0.4719526767730713\n",
      "Validation: Epoch 16, batch 3, Loss 0.08987867087125778\n",
      "Validation: Epoch 16, batch 4, Loss 0.288822740316391\n",
      "per epoch validation loss (batch average): 0.32075116485357286\n",
      "0\n",
      "Training: Epoch 17, batch 0, Loss 0.6311073303222656\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "per epoch training loss (batch average): 0.4670816630125046\n",
      "Validation: Epoch 17, batch 0, Loss 0.5262356400489807\n",
      "Validation: Epoch 17, batch 1, Loss 0.5603127479553223\n",
      "Validation: Epoch 17, batch 2, Loss 0.3366734981536865\n",
      "Validation: Epoch 17, batch 3, Loss 0.6166006326675415\n",
      "Validation: Epoch 17, batch 4, Loss 0.6181015372276306\n",
      "per epoch validation loss (batch average): 0.5315848112106323\n",
      "0\n",
      "Training: Epoch 18, batch 0, Loss 0.39089518785476685\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "per epoch training loss (batch average): 0.527212792634964\n",
      "Validation: Epoch 18, batch 0, Loss 0.3830261826515198\n",
      "Validation: Epoch 18, batch 1, Loss 0.3586403429508209\n",
      "Validation: Epoch 18, batch 2, Loss 0.609275221824646\n",
      "Validation: Epoch 18, batch 3, Loss 0.3507872223854065\n",
      "Validation: Epoch 18, batch 4, Loss 0.66059809923172\n",
      "per epoch validation loss (batch average): 0.4724654138088226\n",
      "0\n",
      "Training: Epoch 19, batch 0, Loss 0.4762082099914551\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "per epoch training loss (batch average): 0.4805824339389801\n",
      "Validation: Epoch 19, batch 0, Loss 0.4830838739871979\n",
      "Validation: Epoch 19, batch 1, Loss 0.7057459950447083\n",
      "Validation: Epoch 19, batch 2, Loss 0.375498503446579\n",
      "Validation: Epoch 19, batch 3, Loss 0.4190632998943329\n",
      "Validation: Epoch 19, batch 4, Loss 0.6401301622390747\n",
      "per epoch validation loss (batch average): 0.5247043669223785\n",
      "0\n",
      "Training: Epoch 20, batch 0, Loss 0.6691381335258484\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "per epoch training loss (batch average): 0.6562128782272338\n",
      "Validation: Epoch 20, batch 0, Loss 0.4793584942817688\n",
      "Validation: Epoch 20, batch 1, Loss 0.4814940392971039\n",
      "Validation: Epoch 20, batch 2, Loss 0.37366145849227905\n",
      "Validation: Epoch 20, batch 3, Loss 0.7825965285301208\n",
      "Validation: Epoch 20, batch 4, Loss 0.47562819719314575\n",
      "per epoch validation loss (batch average): 0.5185477435588837\n",
      "0\n",
      "Training: Epoch 21, batch 0, Loss 0.28438571095466614\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "per epoch training loss (batch average): 0.44038805961608884\n",
      "Validation: Epoch 21, batch 0, Loss 0.6716785430908203\n",
      "Validation: Epoch 21, batch 1, Loss 0.5974971055984497\n",
      "Validation: Epoch 21, batch 2, Loss 0.7288958430290222\n",
      "Validation: Epoch 21, batch 3, Loss 0.5538803935050964\n",
      "Validation: Epoch 21, batch 4, Loss 0.4896242320537567\n",
      "per epoch validation loss (batch average): 0.6083152234554291\n",
      "0\n",
      "Training: Epoch 22, batch 0, Loss 0.6364331245422363\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "per epoch training loss (batch average): 0.4347769320011139\n",
      "Validation: Epoch 22, batch 0, Loss 0.42702779173851013\n",
      "Validation: Epoch 22, batch 1, Loss 0.5312075614929199\n",
      "Validation: Epoch 22, batch 2, Loss 0.5737979412078857\n",
      "Validation: Epoch 22, batch 3, Loss 0.3938771188259125\n",
      "Validation: Epoch 22, batch 4, Loss 0.588128924369812\n",
      "per epoch validation loss (batch average): 0.502807867527008\n",
      "0\n",
      "Training: Epoch 23, batch 0, Loss 0.32862159609794617\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "per epoch training loss (batch average): 0.5132721364498138\n",
      "Validation: Epoch 23, batch 0, Loss 0.19846314191818237\n",
      "Validation: Epoch 23, batch 1, Loss 0.37179824709892273\n",
      "Validation: Epoch 23, batch 2, Loss 0.3976023197174072\n",
      "Validation: Epoch 23, batch 3, Loss 0.4231838881969452\n",
      "Validation: Epoch 23, batch 4, Loss 0.30657529830932617\n",
      "per epoch validation loss (batch average): 0.33952457904815675\n",
      "0\n",
      "Training: Epoch 24, batch 0, Loss 0.3416309654712677\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "per epoch training loss (batch average): 0.3468340516090393\n",
      "Validation: Epoch 24, batch 0, Loss 0.5148154497146606\n",
      "Validation: Epoch 24, batch 1, Loss 0.374796599149704\n",
      "Validation: Epoch 24, batch 2, Loss 0.553011417388916\n",
      "Validation: Epoch 24, batch 3, Loss 0.36820513010025024\n",
      "Validation: Epoch 24, batch 4, Loss 0.5352922081947327\n",
      "per epoch validation loss (batch average): 0.4692241609096527\n",
      "0\n",
      "Training: Epoch 25, batch 0, Loss 0.5206962823867798\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "per epoch training loss (batch average): 0.45548667907714846\n",
      "Validation: Epoch 25, batch 0, Loss 0.4267568290233612\n",
      "Validation: Epoch 25, batch 1, Loss 0.7104611396789551\n",
      "Validation: Epoch 25, batch 2, Loss 0.7575140595436096\n",
      "Validation: Epoch 25, batch 3, Loss 0.6534887552261353\n",
      "Validation: Epoch 25, batch 4, Loss 0.2168280929327011\n",
      "per epoch validation loss (batch average): 0.5530097752809524\n",
      "0\n",
      "Training: Epoch 26, batch 0, Loss 0.4688623547554016\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "per epoch training loss (batch average): 0.46076456308364866\n",
      "Validation: Epoch 26, batch 0, Loss 0.5397555828094482\n",
      "Validation: Epoch 26, batch 1, Loss 0.6381509900093079\n",
      "Validation: Epoch 26, batch 2, Loss 0.5417157411575317\n",
      "Validation: Epoch 26, batch 3, Loss 0.6594323515892029\n",
      "Validation: Epoch 26, batch 4, Loss 0.34385791420936584\n",
      "per epoch validation loss (batch average): 0.5445825159549713\n",
      "0\n",
      "Training: Epoch 27, batch 0, Loss 0.5975888967514038\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "per epoch training loss (batch average): 0.47635178864002226\n",
      "Validation: Epoch 27, batch 0, Loss 0.5832828879356384\n",
      "Validation: Epoch 27, batch 1, Loss 0.5740674138069153\n",
      "Validation: Epoch 27, batch 2, Loss 0.4073662757873535\n",
      "Validation: Epoch 27, batch 3, Loss 0.5042011141777039\n",
      "Validation: Epoch 27, batch 4, Loss 0.3691560924053192\n",
      "per epoch validation loss (batch average): 0.48761475682258604\n",
      "0\n",
      "Training: Epoch 28, batch 0, Loss 0.44387656450271606\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "per epoch training loss (batch average): 0.4109121561050415\n",
      "Validation: Epoch 28, batch 0, Loss 0.40056678652763367\n",
      "Validation: Epoch 28, batch 1, Loss 0.5744709968566895\n",
      "Validation: Epoch 28, batch 2, Loss 0.45052310824394226\n",
      "Validation: Epoch 28, batch 3, Loss 0.16464070975780487\n",
      "Validation: Epoch 28, batch 4, Loss 0.36515340209007263\n",
      "per epoch validation loss (batch average): 0.3910710006952286\n",
      "0\n",
      "Training: Epoch 29, batch 0, Loss 0.40820184350013733\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "per epoch training loss (batch average): 0.39464496076107025\n",
      "Validation: Epoch 29, batch 0, Loss 0.4800325334072113\n",
      "Validation: Epoch 29, batch 1, Loss 0.5740702748298645\n",
      "Validation: Epoch 29, batch 2, Loss 0.5391711592674255\n",
      "Validation: Epoch 29, batch 3, Loss 0.22196541726589203\n",
      "Validation: Epoch 29, batch 4, Loss 0.5345671772956848\n",
      "per epoch validation loss (batch average): 0.46996131241321565\n",
      "0\n",
      "Training: Epoch 30, batch 0, Loss 0.32422205805778503\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "per epoch training loss (batch average): 0.47552382946014404\n",
      "Validation: Epoch 30, batch 0, Loss 0.6831459999084473\n",
      "Validation: Epoch 30, batch 1, Loss 0.3921373188495636\n",
      "Validation: Epoch 30, batch 2, Loss 0.3216165602207184\n",
      "Validation: Epoch 30, batch 3, Loss 0.17174629867076874\n",
      "Validation: Epoch 30, batch 4, Loss 0.29658976197242737\n",
      "per epoch validation loss (batch average): 0.37304718792438507\n",
      "0\n",
      "Training: Epoch 31, batch 0, Loss 0.4702613055706024\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "per epoch training loss (batch average): 0.45628684759140015\n",
      "Validation: Epoch 31, batch 0, Loss 0.5775438547134399\n",
      "Validation: Epoch 31, batch 1, Loss 0.616054892539978\n",
      "Validation: Epoch 31, batch 2, Loss 0.3224160671234131\n",
      "Validation: Epoch 31, batch 3, Loss 0.33770278096199036\n",
      "Validation: Epoch 31, batch 4, Loss 0.3101732134819031\n",
      "per epoch validation loss (batch average): 0.4327781617641449\n",
      "0\n",
      "Training: Epoch 32, batch 0, Loss 0.23877175152301788\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "per epoch training loss (batch average): 0.4137244373559952\n",
      "Validation: Epoch 32, batch 0, Loss 0.537976086139679\n",
      "Validation: Epoch 32, batch 1, Loss 0.4531019628047943\n",
      "Validation: Epoch 32, batch 2, Loss 0.3446879982948303\n",
      "Validation: Epoch 32, batch 3, Loss 0.41410133242607117\n",
      "Validation: Epoch 32, batch 4, Loss 0.26610997319221497\n",
      "per epoch validation loss (batch average): 0.40319547057151794\n",
      "0\n",
      "Training: Epoch 33, batch 0, Loss 0.46768951416015625\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "per epoch training loss (batch average): 0.3643626093864441\n",
      "Validation: Epoch 33, batch 0, Loss 0.27258118987083435\n",
      "Validation: Epoch 33, batch 1, Loss 0.5718313455581665\n",
      "Validation: Epoch 33, batch 2, Loss 0.2638353407382965\n",
      "Validation: Epoch 33, batch 3, Loss 0.16548724472522736\n",
      "Validation: Epoch 33, batch 4, Loss 0.6545793414115906\n",
      "per epoch validation loss (batch average): 0.3856628924608231\n",
      "0\n",
      "Training: Epoch 34, batch 0, Loss 0.375676691532135\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "per epoch training loss (batch average): 0.43406776785850526\n",
      "Validation: Epoch 34, batch 0, Loss 0.43629980087280273\n",
      "Validation: Epoch 34, batch 1, Loss 0.5372509360313416\n",
      "Validation: Epoch 34, batch 2, Loss 0.3782542645931244\n",
      "Validation: Epoch 34, batch 3, Loss 0.7266713976860046\n",
      "Validation: Epoch 34, batch 4, Loss 0.3761111795902252\n",
      "per epoch validation loss (batch average): 0.4909175157546997\n",
      "0\n",
      "Training: Epoch 35, batch 0, Loss 0.6817494630813599\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "per epoch training loss (batch average): 0.49715219140052797\n",
      "Validation: Epoch 35, batch 0, Loss 0.44324150681495667\n",
      "Validation: Epoch 35, batch 1, Loss 0.7108319401741028\n",
      "Validation: Epoch 35, batch 2, Loss 0.3638048768043518\n",
      "Validation: Epoch 35, batch 3, Loss 0.22722689807415009\n",
      "Validation: Epoch 35, batch 4, Loss 0.5539001822471619\n",
      "per epoch validation loss (batch average): 0.45980108082294463\n",
      "0\n",
      "Training: Epoch 36, batch 0, Loss 0.6456407904624939\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "per epoch training loss (batch average): 0.4166280925273895\n",
      "Validation: Epoch 36, batch 0, Loss 0.43313342332839966\n",
      "Validation: Epoch 36, batch 1, Loss 0.218556746840477\n",
      "Validation: Epoch 36, batch 2, Loss 0.3621580898761749\n",
      "Validation: Epoch 36, batch 3, Loss 0.43291187286376953\n",
      "Validation: Epoch 36, batch 4, Loss 0.29491063952445984\n",
      "per epoch validation loss (batch average): 0.34833415448665617\n",
      "0\n",
      "Training: Epoch 37, batch 0, Loss 0.37713372707366943\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "per epoch training loss (batch average): 0.47732028365135193\n",
      "Validation: Epoch 37, batch 0, Loss 0.37584564089775085\n",
      "Validation: Epoch 37, batch 1, Loss 0.45910292863845825\n",
      "Validation: Epoch 37, batch 2, Loss 0.39006856083869934\n",
      "Validation: Epoch 37, batch 3, Loss 0.45998820662498474\n",
      "Validation: Epoch 37, batch 4, Loss 0.6878615617752075\n",
      "per epoch validation loss (batch average): 0.4745733797550201\n",
      "0\n",
      "Training: Epoch 38, batch 0, Loss 0.5666354894638062\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "per epoch training loss (batch average): 0.5356263518333435\n",
      "Validation: Epoch 38, batch 0, Loss 0.6206044554710388\n",
      "Validation: Epoch 38, batch 1, Loss 0.7301698923110962\n",
      "Validation: Epoch 38, batch 2, Loss 0.34365183115005493\n",
      "Validation: Epoch 38, batch 3, Loss 0.5435909032821655\n",
      "Validation: Epoch 38, batch 4, Loss 0.39562833309173584\n",
      "per epoch validation loss (batch average): 0.5267290830612182\n",
      "0\n",
      "Training: Epoch 39, batch 0, Loss 0.44648605585098267\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "per epoch training loss (batch average): 0.37142339944839475\n",
      "Validation: Epoch 39, batch 0, Loss 0.47952207922935486\n",
      "Validation: Epoch 39, batch 1, Loss 0.28424039483070374\n",
      "Validation: Epoch 39, batch 2, Loss 0.6397794485092163\n",
      "Validation: Epoch 39, batch 3, Loss 0.6638821363449097\n",
      "Validation: Epoch 39, batch 4, Loss 0.5895649790763855\n",
      "per epoch validation loss (batch average): 0.5313978075981141\n",
      "0\n",
      "Training: Epoch 40, batch 0, Loss 0.22680626809597015\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "per epoch training loss (batch average): 0.28232814073562623\n",
      "Validation: Epoch 40, batch 0, Loss 0.39029690623283386\n",
      "Validation: Epoch 40, batch 1, Loss 0.6076675653457642\n",
      "Validation: Epoch 40, batch 2, Loss 0.42382630705833435\n",
      "Validation: Epoch 40, batch 3, Loss 0.592819333076477\n",
      "Validation: Epoch 40, batch 4, Loss 0.12846188247203827\n",
      "per epoch validation loss (batch average): 0.4286143988370895\n",
      "0\n",
      "Training: Epoch 41, batch 0, Loss 0.13993775844573975\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "per epoch training loss (batch average): 0.4298365473747253\n",
      "Validation: Epoch 41, batch 0, Loss 0.6075567603111267\n",
      "Validation: Epoch 41, batch 1, Loss 0.38160014152526855\n",
      "Validation: Epoch 41, batch 2, Loss 0.30673256516456604\n",
      "Validation: Epoch 41, batch 3, Loss 0.45400306582450867\n",
      "Validation: Epoch 41, batch 4, Loss 0.6926150321960449\n",
      "per epoch validation loss (batch average): 0.488501513004303\n",
      "0\n",
      "Training: Epoch 42, batch 0, Loss 0.38767334818840027\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "per epoch training loss (batch average): 0.4519529640674591\n",
      "Validation: Epoch 42, batch 0, Loss 0.7446193695068359\n",
      "Validation: Epoch 42, batch 1, Loss 0.3490069806575775\n",
      "Validation: Epoch 42, batch 2, Loss 0.305004358291626\n",
      "Validation: Epoch 42, batch 3, Loss 0.48345479369163513\n",
      "Validation: Epoch 42, batch 4, Loss 0.3028033673763275\n",
      "per epoch validation loss (batch average): 0.4369777739048004\n",
      "0\n",
      "Training: Epoch 43, batch 0, Loss 0.5034944415092468\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "per epoch training loss (batch average): 0.5252160727977753\n",
      "Validation: Epoch 43, batch 0, Loss 0.2880830466747284\n",
      "Validation: Epoch 43, batch 1, Loss 0.5407413840293884\n",
      "Validation: Epoch 43, batch 2, Loss 0.4677867591381073\n",
      "Validation: Epoch 43, batch 3, Loss 0.3778781592845917\n",
      "Validation: Epoch 43, batch 4, Loss 0.3705451190471649\n",
      "per epoch validation loss (batch average): 0.40900689363479614\n",
      "0\n",
      "Training: Epoch 44, batch 0, Loss 0.23739604651927948\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "per epoch training loss (batch average): 0.418076953291893\n",
      "Validation: Epoch 44, batch 0, Loss 0.6349490284919739\n",
      "Validation: Epoch 44, batch 1, Loss 0.39434921741485596\n",
      "Validation: Epoch 44, batch 2, Loss 0.2341856062412262\n",
      "Validation: Epoch 44, batch 3, Loss 0.33302533626556396\n",
      "Validation: Epoch 44, batch 4, Loss 0.7901567816734314\n",
      "per epoch validation loss (batch average): 0.47733319401741026\n",
      "0\n",
      "Training: Epoch 45, batch 0, Loss 0.4941530227661133\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "per epoch training loss (batch average): 0.47999587953090667\n",
      "Validation: Epoch 45, batch 0, Loss 0.41312941908836365\n",
      "Validation: Epoch 45, batch 1, Loss 0.8052988052368164\n",
      "Validation: Epoch 45, batch 2, Loss 0.26121988892555237\n",
      "Validation: Epoch 45, batch 3, Loss 0.4992372691631317\n",
      "Validation: Epoch 45, batch 4, Loss 0.5170680284500122\n",
      "per epoch validation loss (batch average): 0.4991906821727753\n",
      "0\n",
      "Training: Epoch 46, batch 0, Loss 0.2055215686559677\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "per epoch training loss (batch average): 0.3959397941827774\n",
      "Validation: Epoch 46, batch 0, Loss 0.5676571130752563\n",
      "Validation: Epoch 46, batch 1, Loss 0.3052242398262024\n",
      "Validation: Epoch 46, batch 2, Loss 0.2595791816711426\n",
      "Validation: Epoch 46, batch 3, Loss 0.5284246802330017\n",
      "Validation: Epoch 46, batch 4, Loss 0.5983602404594421\n",
      "per epoch validation loss (batch average): 0.451849091053009\n",
      "0\n",
      "Training: Epoch 47, batch 0, Loss 0.4153313636779785\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "per epoch training loss (batch average): 0.4922504186630249\n",
      "Validation: Epoch 47, batch 0, Loss 0.14913885295391083\n",
      "Validation: Epoch 47, batch 1, Loss 0.5899323225021362\n",
      "Validation: Epoch 47, batch 2, Loss 0.32210466265678406\n",
      "Validation: Epoch 47, batch 3, Loss 0.38831743597984314\n",
      "Validation: Epoch 47, batch 4, Loss 0.5884057879447937\n",
      "per epoch validation loss (batch average): 0.4075798124074936\n",
      "0\n",
      "Training: Epoch 48, batch 0, Loss 0.2987990379333496\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "per epoch training loss (batch average): 0.38483368158340453\n",
      "Validation: Epoch 48, batch 0, Loss 0.2081453800201416\n",
      "Validation: Epoch 48, batch 1, Loss 0.7139667868614197\n",
      "Validation: Epoch 48, batch 2, Loss 0.2563309073448181\n",
      "Validation: Epoch 48, batch 3, Loss 0.21015949547290802\n",
      "Validation: Epoch 48, batch 4, Loss 0.34225165843963623\n",
      "per epoch validation loss (batch average): 0.3461708456277847\n",
      "0\n",
      "Training: Epoch 49, batch 0, Loss 0.15239058434963226\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "per epoch training loss (batch average): 0.45473491251468656\n",
      "Validation: Epoch 49, batch 0, Loss 0.3030931353569031\n",
      "Validation: Epoch 49, batch 1, Loss 0.4312235414981842\n",
      "Validation: Epoch 49, batch 2, Loss 0.4078027904033661\n",
      "Validation: Epoch 49, batch 3, Loss 0.2241305410861969\n",
      "Validation: Epoch 49, batch 4, Loss 0.4890765845775604\n",
      "per epoch validation loss (batch average): 0.37106531858444214\n",
      "0\n",
      "Training: Epoch 50, batch 0, Loss 0.3841993510723114\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "per epoch training loss (batch average): 0.45533464550971986\n",
      "Validation: Epoch 50, batch 0, Loss 0.5261307954788208\n",
      "Validation: Epoch 50, batch 1, Loss 0.24229608476161957\n",
      "Validation: Epoch 50, batch 2, Loss 0.6808953285217285\n",
      "Validation: Epoch 50, batch 3, Loss 0.5425162315368652\n",
      "Validation: Epoch 50, batch 4, Loss 0.3000255525112152\n",
      "per epoch validation loss (batch average): 0.4583727985620499\n",
      "0\n",
      "Training: Epoch 51, batch 0, Loss 0.2828407883644104\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "per epoch training loss (batch average): 0.3758862793445587\n",
      "Validation: Epoch 51, batch 0, Loss 0.4076552987098694\n",
      "Validation: Epoch 51, batch 1, Loss 0.3744671046733856\n",
      "Validation: Epoch 51, batch 2, Loss 0.37999987602233887\n",
      "Validation: Epoch 51, batch 3, Loss 0.3967398703098297\n",
      "Validation: Epoch 51, batch 4, Loss 0.33203125\n",
      "per epoch validation loss (batch average): 0.37817867994308474\n",
      "0\n",
      "Training: Epoch 52, batch 0, Loss 0.45631369948387146\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "per epoch training loss (batch average): 0.3988587975502014\n",
      "Validation: Epoch 52, batch 0, Loss 0.6812290549278259\n",
      "Validation: Epoch 52, batch 1, Loss 0.5070997476577759\n",
      "Validation: Epoch 52, batch 2, Loss 0.5610601305961609\n",
      "Validation: Epoch 52, batch 3, Loss 0.35182371735572815\n",
      "Validation: Epoch 52, batch 4, Loss 0.2854393422603607\n",
      "per epoch validation loss (batch average): 0.4773303985595703\n",
      "0\n",
      "Training: Epoch 53, batch 0, Loss 0.29880407452583313\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "per epoch training loss (batch average): 0.33484409749507904\n",
      "Validation: Epoch 53, batch 0, Loss 0.3293032646179199\n",
      "Validation: Epoch 53, batch 1, Loss 0.33450475335121155\n",
      "Validation: Epoch 53, batch 2, Loss 0.44416552782058716\n",
      "Validation: Epoch 53, batch 3, Loss 0.39272555708885193\n",
      "Validation: Epoch 53, batch 4, Loss 0.42438220977783203\n",
      "per epoch validation loss (batch average): 0.3850162625312805\n",
      "0\n",
      "Training: Epoch 54, batch 0, Loss 0.32016846537590027\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "per epoch training loss (batch average): 0.3847186803817749\n",
      "Validation: Epoch 54, batch 0, Loss 0.43532487750053406\n",
      "Validation: Epoch 54, batch 1, Loss 0.4547792077064514\n",
      "Validation: Epoch 54, batch 2, Loss 0.566550076007843\n",
      "Validation: Epoch 54, batch 3, Loss 0.1051480695605278\n",
      "Validation: Epoch 54, batch 4, Loss 0.47356081008911133\n",
      "per epoch validation loss (batch average): 0.40707260817289354\n",
      "0\n",
      "Training: Epoch 55, batch 0, Loss 0.551496684551239\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "per epoch training loss (batch average): 0.5471709370613098\n",
      "Validation: Epoch 55, batch 0, Loss 0.3662014603614807\n",
      "Validation: Epoch 55, batch 1, Loss 0.6143255233764648\n",
      "Validation: Epoch 55, batch 2, Loss 0.36416512727737427\n",
      "Validation: Epoch 55, batch 3, Loss 0.36204031109809875\n",
      "Validation: Epoch 55, batch 4, Loss 0.44869518280029297\n",
      "per epoch validation loss (batch average): 0.43108552098274233\n",
      "0\n",
      "Training: Epoch 56, batch 0, Loss 0.6627315878868103\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "per epoch training loss (batch average): 0.4978607416152954\n",
      "Validation: Epoch 56, batch 0, Loss 0.3206128776073456\n",
      "Validation: Epoch 56, batch 1, Loss 0.2934820353984833\n",
      "Validation: Epoch 56, batch 2, Loss 0.1573525220155716\n",
      "Validation: Epoch 56, batch 3, Loss 0.4910983145236969\n",
      "Validation: Epoch 56, batch 4, Loss 0.2972848117351532\n",
      "per epoch validation loss (batch average): 0.3119661122560501\n",
      "0\n",
      "Training: Epoch 57, batch 0, Loss 0.16499583423137665\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "per epoch training loss (batch average): 0.3562771111726761\n",
      "Validation: Epoch 57, batch 0, Loss 0.5955789685249329\n",
      "Validation: Epoch 57, batch 1, Loss 0.4678169786930084\n",
      "Validation: Epoch 57, batch 2, Loss 0.6424105763435364\n",
      "Validation: Epoch 57, batch 3, Loss 0.5226166844367981\n",
      "Validation: Epoch 57, batch 4, Loss 0.3617144823074341\n",
      "per epoch validation loss (batch average): 0.518027538061142\n",
      "0\n",
      "Training: Epoch 58, batch 0, Loss 0.6614148020744324\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "per epoch training loss (batch average): 0.5899562001228332\n",
      "Validation: Epoch 58, batch 0, Loss 0.5087127685546875\n",
      "Validation: Epoch 58, batch 1, Loss 0.4972511827945709\n",
      "Validation: Epoch 58, batch 2, Loss 0.3935697078704834\n",
      "Validation: Epoch 58, batch 3, Loss 0.7426008582115173\n",
      "Validation: Epoch 58, batch 4, Loss 0.4114786684513092\n",
      "per epoch validation loss (batch average): 0.5107226371765137\n",
      "0\n",
      "Training: Epoch 59, batch 0, Loss 0.37462255358695984\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "per epoch training loss (batch average): 0.38749647736549375\n",
      "Validation: Epoch 59, batch 0, Loss 0.4724007248878479\n",
      "Validation: Epoch 59, batch 1, Loss 0.41386356949806213\n",
      "Validation: Epoch 59, batch 2, Loss 0.4060819149017334\n",
      "Validation: Epoch 59, batch 3, Loss 0.4903419613838196\n",
      "Validation: Epoch 59, batch 4, Loss 0.6140840649604797\n",
      "per epoch validation loss (batch average): 0.4793544471263885\n",
      "0\n",
      "Training: Epoch 60, batch 0, Loss 0.33797675371170044\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "per epoch training loss (batch average): 0.4966651022434235\n",
      "Validation: Epoch 60, batch 0, Loss 0.5464755892753601\n",
      "Validation: Epoch 60, batch 1, Loss 0.24590197205543518\n",
      "Validation: Epoch 60, batch 2, Loss 0.5092884302139282\n",
      "Validation: Epoch 60, batch 3, Loss 0.28076809644699097\n",
      "Validation: Epoch 60, batch 4, Loss 0.2806709408760071\n",
      "per epoch validation loss (batch average): 0.3726210057735443\n",
      "0\n",
      "Training: Epoch 61, batch 0, Loss 0.5301052331924438\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "per epoch training loss (batch average): 0.7650956153869629\n",
      "Validation: Epoch 61, batch 0, Loss 0.6201483011245728\n",
      "Validation: Epoch 61, batch 1, Loss 0.3432650864124298\n",
      "Validation: Epoch 61, batch 2, Loss 0.32328563928604126\n",
      "Validation: Epoch 61, batch 3, Loss 0.2876989543437958\n",
      "Validation: Epoch 61, batch 4, Loss 0.5585795640945435\n",
      "per epoch validation loss (batch average): 0.4265955090522766\n",
      "0\n",
      "Training: Epoch 62, batch 0, Loss 0.18062500655651093\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "per epoch training loss (batch average): 0.319249564409256\n",
      "Validation: Epoch 62, batch 0, Loss 0.29025059938430786\n",
      "Validation: Epoch 62, batch 1, Loss 0.5545636415481567\n",
      "Validation: Epoch 62, batch 2, Loss 0.21600250899791718\n",
      "Validation: Epoch 62, batch 3, Loss 0.5448795557022095\n",
      "Validation: Epoch 62, batch 4, Loss 0.3223860263824463\n",
      "per epoch validation loss (batch average): 0.3856164664030075\n",
      "0\n",
      "Training: Epoch 63, batch 0, Loss 0.48203983902931213\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "per epoch training loss (batch average): 0.4153849005699158\n",
      "Validation: Epoch 63, batch 0, Loss 0.5059266686439514\n",
      "Validation: Epoch 63, batch 1, Loss 0.4646623134613037\n",
      "Validation: Epoch 63, batch 2, Loss 0.2470255345106125\n",
      "Validation: Epoch 63, batch 3, Loss 0.4142533242702484\n",
      "Validation: Epoch 63, batch 4, Loss 0.467374324798584\n",
      "per epoch validation loss (batch average): 0.41984843313694\n",
      "0\n",
      "Training: Epoch 64, batch 0, Loss 0.17478619515895844\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "per epoch training loss (batch average): 0.3351287394762039\n",
      "Validation: Epoch 64, batch 0, Loss 0.40585342049598694\n",
      "Validation: Epoch 64, batch 1, Loss 0.3314621150493622\n",
      "Validation: Epoch 64, batch 2, Loss 0.30124631524086\n",
      "Validation: Epoch 64, batch 3, Loss 0.190683051943779\n",
      "Validation: Epoch 64, batch 4, Loss 0.41761836409568787\n",
      "per epoch validation loss (batch average): 0.3293726533651352\n",
      "0\n",
      "Training: Epoch 65, batch 0, Loss 0.44619184732437134\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "per epoch training loss (batch average): 0.4260732889175415\n",
      "Validation: Epoch 65, batch 0, Loss 0.3714803159236908\n",
      "Validation: Epoch 65, batch 1, Loss 0.5379515886306763\n",
      "Validation: Epoch 65, batch 2, Loss 0.36022236943244934\n",
      "Validation: Epoch 65, batch 3, Loss 0.2782839834690094\n",
      "Validation: Epoch 65, batch 4, Loss 0.46222707629203796\n",
      "per epoch validation loss (batch average): 0.40203306674957273\n",
      "0\n",
      "Training: Epoch 66, batch 0, Loss 0.4378449618816376\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "per epoch training loss (batch average): 0.39255824983119963\n",
      "Validation: Epoch 66, batch 0, Loss 0.6153509020805359\n",
      "Validation: Epoch 66, batch 1, Loss 0.30322930216789246\n",
      "Validation: Epoch 66, batch 2, Loss 0.2386332005262375\n",
      "Validation: Epoch 66, batch 3, Loss 0.31755271553993225\n",
      "Validation: Epoch 66, batch 4, Loss 0.2134736031293869\n",
      "per epoch validation loss (batch average): 0.337647944688797\n",
      "0\n",
      "Training: Epoch 67, batch 0, Loss 0.43714505434036255\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "per epoch training loss (batch average): 0.3208040952682495\n",
      "Validation: Epoch 67, batch 0, Loss 0.2691166400909424\n",
      "Validation: Epoch 67, batch 1, Loss 0.36042433977127075\n",
      "Validation: Epoch 67, batch 2, Loss 0.46322304010391235\n",
      "Validation: Epoch 67, batch 3, Loss 0.6452283263206482\n",
      "Validation: Epoch 67, batch 4, Loss 0.40951862931251526\n",
      "per epoch validation loss (batch average): 0.42950219511985777\n",
      "0\n",
      "Training: Epoch 68, batch 0, Loss 0.4724394381046295\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "per epoch training loss (batch average): 0.35507543236017225\n",
      "Validation: Epoch 68, batch 0, Loss 0.7258790731430054\n",
      "Validation: Epoch 68, batch 1, Loss 0.285547137260437\n",
      "Validation: Epoch 68, batch 2, Loss 0.2692047357559204\n",
      "Validation: Epoch 68, batch 3, Loss 0.6784881949424744\n",
      "Validation: Epoch 68, batch 4, Loss 0.31265562772750854\n",
      "per epoch validation loss (batch average): 0.45435495376586915\n",
      "0\n",
      "Training: Epoch 69, batch 0, Loss 0.38218575716018677\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "per epoch training loss (batch average): 0.3830568343400955\n",
      "Validation: Epoch 69, batch 0, Loss 0.2756675183773041\n",
      "Validation: Epoch 69, batch 1, Loss 0.7071863412857056\n",
      "Validation: Epoch 69, batch 2, Loss 0.4218696653842926\n",
      "Validation: Epoch 69, batch 3, Loss 0.5636352300643921\n",
      "Validation: Epoch 69, batch 4, Loss 0.7026948928833008\n",
      "per epoch validation loss (batch average): 0.5342107295989991\n",
      "0\n",
      "Training: Epoch 70, batch 0, Loss 0.41633686423301697\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "per epoch training loss (batch average): 0.56202392578125\n",
      "Validation: Epoch 70, batch 0, Loss 0.5374600291252136\n",
      "Validation: Epoch 70, batch 1, Loss 0.5217490792274475\n",
      "Validation: Epoch 70, batch 2, Loss 0.3322756290435791\n",
      "Validation: Epoch 70, batch 3, Loss 0.2246100902557373\n",
      "Validation: Epoch 70, batch 4, Loss 0.49744024872779846\n",
      "per epoch validation loss (batch average): 0.4227070152759552\n",
      "0\n",
      "Training: Epoch 71, batch 0, Loss 0.4264313876628876\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "per epoch training loss (batch average): 0.472256475687027\n",
      "Validation: Epoch 71, batch 0, Loss 0.30576273798942566\n",
      "Validation: Epoch 71, batch 1, Loss 0.40150952339172363\n",
      "Validation: Epoch 71, batch 2, Loss 0.6890608668327332\n",
      "Validation: Epoch 71, batch 3, Loss 0.44403788447380066\n",
      "Validation: Epoch 71, batch 4, Loss 0.4326538145542145\n",
      "per epoch validation loss (batch average): 0.4546049654483795\n",
      "0\n",
      "Training: Epoch 72, batch 0, Loss 0.3420662581920624\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "per epoch training loss (batch average): 0.3812485754489899\n",
      "Validation: Epoch 72, batch 0, Loss 0.6445319056510925\n",
      "Validation: Epoch 72, batch 1, Loss 0.47565194964408875\n",
      "Validation: Epoch 72, batch 2, Loss 0.3359922468662262\n",
      "Validation: Epoch 72, batch 3, Loss 0.39446091651916504\n",
      "Validation: Epoch 72, batch 4, Loss 0.19072802364826202\n",
      "per epoch validation loss (batch average): 0.4082730084657669\n",
      "0\n",
      "Training: Epoch 73, batch 0, Loss 0.45070239901542664\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "per epoch training loss (batch average): 0.518461275100708\n",
      "Validation: Epoch 73, batch 0, Loss 0.3863171935081482\n",
      "Validation: Epoch 73, batch 1, Loss 0.5652085542678833\n",
      "Validation: Epoch 73, batch 2, Loss 0.46192288398742676\n",
      "Validation: Epoch 73, batch 3, Loss 0.5652050375938416\n",
      "Validation: Epoch 73, batch 4, Loss 0.99823397397995\n",
      "per epoch validation loss (batch average): 0.59537752866745\n",
      "0\n",
      "Training: Epoch 74, batch 0, Loss 0.37917572259902954\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "per epoch training loss (batch average): 0.4080266773700714\n",
      "Validation: Epoch 74, batch 0, Loss 0.28779521584510803\n",
      "Validation: Epoch 74, batch 1, Loss 0.31376829743385315\n",
      "Validation: Epoch 74, batch 2, Loss 0.5991718173027039\n",
      "Validation: Epoch 74, batch 3, Loss 0.8077318072319031\n",
      "Validation: Epoch 74, batch 4, Loss 0.23306261003017426\n",
      "per epoch validation loss (batch average): 0.4483059495687485\n",
      "0\n",
      "Training: Epoch 75, batch 0, Loss 0.5345211625099182\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "per epoch training loss (batch average): 0.413875424861908\n",
      "Validation: Epoch 75, batch 0, Loss 0.31292960047721863\n",
      "Validation: Epoch 75, batch 1, Loss 0.3716719150543213\n",
      "Validation: Epoch 75, batch 2, Loss 0.2978490889072418\n",
      "Validation: Epoch 75, batch 3, Loss 0.3032146692276001\n",
      "Validation: Epoch 75, batch 4, Loss 0.5398741960525513\n",
      "per epoch validation loss (batch average): 0.3651078939437866\n",
      "0\n",
      "Training: Epoch 76, batch 0, Loss 0.2250784933567047\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "per epoch training loss (batch average): 0.4389853894710541\n",
      "Validation: Epoch 76, batch 0, Loss 0.45579132437705994\n",
      "Validation: Epoch 76, batch 1, Loss 0.509824275970459\n",
      "Validation: Epoch 76, batch 2, Loss 0.33879971504211426\n",
      "Validation: Epoch 76, batch 3, Loss 0.2637963593006134\n",
      "Validation: Epoch 76, batch 4, Loss 0.496225506067276\n",
      "per epoch validation loss (batch average): 0.4128874361515045\n",
      "0\n",
      "Training: Epoch 77, batch 0, Loss 0.4251960813999176\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "per epoch training loss (batch average): 0.3695358097553253\n",
      "Validation: Epoch 77, batch 0, Loss 0.558576762676239\n",
      "Validation: Epoch 77, batch 1, Loss 0.8986091613769531\n",
      "Validation: Epoch 77, batch 2, Loss 0.6240421533584595\n",
      "Validation: Epoch 77, batch 3, Loss 0.40885162353515625\n",
      "Validation: Epoch 77, batch 4, Loss 0.4656142294406891\n",
      "per epoch validation loss (batch average): 0.5911387860774994\n",
      "0\n",
      "Training: Epoch 78, batch 0, Loss 0.2874982953071594\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "per epoch training loss (batch average): 0.3461298763751984\n",
      "Validation: Epoch 78, batch 0, Loss 0.20518498122692108\n",
      "Validation: Epoch 78, batch 1, Loss 0.4495907723903656\n",
      "Validation: Epoch 78, batch 2, Loss 0.3893694579601288\n",
      "Validation: Epoch 78, batch 3, Loss 0.4613162577152252\n",
      "Validation: Epoch 78, batch 4, Loss 0.705463171005249\n",
      "per epoch validation loss (batch average): 0.44218492805957793\n",
      "0\n",
      "Training: Epoch 79, batch 0, Loss 0.3264552652835846\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "per epoch training loss (batch average): 0.2919316664338112\n",
      "Validation: Epoch 79, batch 0, Loss 0.8001036047935486\n",
      "Validation: Epoch 79, batch 1, Loss 0.36699673533439636\n",
      "Validation: Epoch 79, batch 2, Loss 0.44449582695961\n",
      "Validation: Epoch 79, batch 3, Loss 0.20114043354988098\n",
      "Validation: Epoch 79, batch 4, Loss 0.328905314207077\n",
      "per epoch validation loss (batch average): 0.4283283829689026\n",
      "0\n",
      "Training: Epoch 80, batch 0, Loss 0.3005465567111969\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "per epoch training loss (batch average): 0.4122667670249939\n",
      "Validation: Epoch 80, batch 0, Loss 0.18027304112911224\n",
      "Validation: Epoch 80, batch 1, Loss 0.44012442231178284\n",
      "Validation: Epoch 80, batch 2, Loss 0.4707355499267578\n",
      "Validation: Epoch 80, batch 3, Loss 0.5647236704826355\n",
      "Validation: Epoch 80, batch 4, Loss 0.14009889960289001\n",
      "per epoch validation loss (batch average): 0.3591911166906357\n",
      "0\n",
      "Training: Epoch 81, batch 0, Loss 0.6910747289657593\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "per epoch training loss (batch average): 0.43150808215141295\n",
      "Validation: Epoch 81, batch 0, Loss 0.5234694480895996\n",
      "Validation: Epoch 81, batch 1, Loss 0.3259037435054779\n",
      "Validation: Epoch 81, batch 2, Loss 0.3991362750530243\n",
      "Validation: Epoch 81, batch 3, Loss 0.37160784006118774\n",
      "Validation: Epoch 81, batch 4, Loss 0.42462632060050964\n",
      "per epoch validation loss (batch average): 0.4089487254619598\n",
      "0\n",
      "Training: Epoch 82, batch 0, Loss 0.43482813239097595\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "per epoch training loss (batch average): 0.4876559913158417\n",
      "Validation: Epoch 82, batch 0, Loss 0.49050602316856384\n",
      "Validation: Epoch 82, batch 1, Loss 0.2637355923652649\n",
      "Validation: Epoch 82, batch 2, Loss 0.4642675518989563\n",
      "Validation: Epoch 82, batch 3, Loss 0.3166869580745697\n",
      "Validation: Epoch 82, batch 4, Loss 0.4315101206302643\n",
      "per epoch validation loss (batch average): 0.3933412492275238\n",
      "0\n",
      "Training: Epoch 83, batch 0, Loss 0.11635946482419968\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "per epoch training loss (batch average): 0.40591550022363665\n",
      "Validation: Epoch 83, batch 0, Loss 0.23243072628974915\n",
      "Validation: Epoch 83, batch 1, Loss 0.5189959406852722\n",
      "Validation: Epoch 83, batch 2, Loss 0.4718300402164459\n",
      "Validation: Epoch 83, batch 3, Loss 0.5154374837875366\n",
      "Validation: Epoch 83, batch 4, Loss 0.6085085868835449\n",
      "per epoch validation loss (batch average): 0.46944055557250974\n",
      "0\n",
      "Training: Epoch 84, batch 0, Loss 0.2707425057888031\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "per epoch training loss (batch average): 0.479004442691803\n",
      "Validation: Epoch 84, batch 0, Loss 0.6047011017799377\n",
      "Validation: Epoch 84, batch 1, Loss 0.3585364520549774\n",
      "Validation: Epoch 84, batch 2, Loss 0.4048982262611389\n",
      "Validation: Epoch 84, batch 3, Loss 0.24721990525722504\n",
      "Validation: Epoch 84, batch 4, Loss 0.4247431755065918\n",
      "per epoch validation loss (batch average): 0.40801977217197416\n",
      "0\n",
      "Training: Epoch 85, batch 0, Loss 0.2519552707672119\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "per epoch training loss (batch average): 0.26946471333503724\n",
      "Validation: Epoch 85, batch 0, Loss 0.34872761368751526\n",
      "Validation: Epoch 85, batch 1, Loss 0.5349483489990234\n",
      "Validation: Epoch 85, batch 2, Loss 0.2588340938091278\n",
      "Validation: Epoch 85, batch 3, Loss 0.4645450711250305\n",
      "Validation: Epoch 85, batch 4, Loss 0.4368394911289215\n",
      "per epoch validation loss (batch average): 0.4087789237499237\n",
      "0\n",
      "Training: Epoch 86, batch 0, Loss 0.2951577603816986\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "per epoch training loss (batch average): 0.33346658051013944\n",
      "Validation: Epoch 86, batch 0, Loss 0.45223942399024963\n",
      "Validation: Epoch 86, batch 1, Loss 0.28925177454948425\n",
      "Validation: Epoch 86, batch 2, Loss 0.53290855884552\n",
      "Validation: Epoch 86, batch 3, Loss 0.2657647132873535\n",
      "Validation: Epoch 86, batch 4, Loss 0.31446853280067444\n",
      "per epoch validation loss (batch average): 0.3709266006946564\n",
      "0\n",
      "Training: Epoch 87, batch 0, Loss 0.2758572995662689\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "per epoch training loss (batch average): 0.3913736045360565\n",
      "Validation: Epoch 87, batch 0, Loss 0.28351885080337524\n",
      "Validation: Epoch 87, batch 1, Loss 0.6286045908927917\n",
      "Validation: Epoch 87, batch 2, Loss 0.3961281478404999\n",
      "Validation: Epoch 87, batch 3, Loss 0.23019297420978546\n",
      "Validation: Epoch 87, batch 4, Loss 0.39215782284736633\n",
      "per epoch validation loss (batch average): 0.38612047731876376\n",
      "0\n",
      "Training: Epoch 88, batch 0, Loss 0.6804761290550232\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "per epoch training loss (batch average): 0.5506753623485565\n",
      "Validation: Epoch 88, batch 0, Loss 0.23594574630260468\n",
      "Validation: Epoch 88, batch 1, Loss 0.7853392958641052\n",
      "Validation: Epoch 88, batch 2, Loss 0.21530850231647491\n",
      "Validation: Epoch 88, batch 3, Loss 0.5947627425193787\n",
      "Validation: Epoch 88, batch 4, Loss 0.4646351933479309\n",
      "per epoch validation loss (batch average): 0.4591982960700989\n",
      "0\n",
      "Training: Epoch 89, batch 0, Loss 0.5233228802680969\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "per epoch training loss (batch average): 0.47603176534175873\n",
      "Validation: Epoch 89, batch 0, Loss 0.5432730913162231\n",
      "Validation: Epoch 89, batch 1, Loss 0.46840476989746094\n",
      "Validation: Epoch 89, batch 2, Loss 0.4388316571712494\n",
      "Validation: Epoch 89, batch 3, Loss 0.2746064364910126\n",
      "Validation: Epoch 89, batch 4, Loss 0.31644636392593384\n",
      "per epoch validation loss (batch average): 0.408312463760376\n",
      "0\n",
      "Training: Epoch 90, batch 0, Loss 0.3799602687358856\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "per epoch training loss (batch average): 0.46775158643722536\n",
      "Validation: Epoch 90, batch 0, Loss 0.5279955863952637\n",
      "Validation: Epoch 90, batch 1, Loss 0.7781599760055542\n",
      "Validation: Epoch 90, batch 2, Loss 0.3464588224887848\n",
      "Validation: Epoch 90, batch 3, Loss 0.38481876254081726\n",
      "Validation: Epoch 90, batch 4, Loss 0.3840426206588745\n",
      "per epoch validation loss (batch average): 0.4842951536178589\n",
      "0\n",
      "Training: Epoch 91, batch 0, Loss 0.4997287690639496\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "per epoch training loss (batch average): 0.46213520169258115\n",
      "Validation: Epoch 91, batch 0, Loss 0.4178147315979004\n",
      "Validation: Epoch 91, batch 1, Loss 0.4563928544521332\n",
      "Validation: Epoch 91, batch 2, Loss 0.29312437772750854\n",
      "Validation: Epoch 91, batch 3, Loss 0.4324955940246582\n",
      "Validation: Epoch 91, batch 4, Loss 0.31506583094596863\n",
      "per epoch validation loss (batch average): 0.3829786777496338\n",
      "0\n",
      "Training: Epoch 92, batch 0, Loss 0.31087109446525574\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "per epoch training loss (batch average): 0.4165112853050232\n",
      "Validation: Epoch 92, batch 0, Loss 0.3966340124607086\n",
      "Validation: Epoch 92, batch 1, Loss 0.35912951827049255\n",
      "Validation: Epoch 92, batch 2, Loss 0.30424055457115173\n",
      "Validation: Epoch 92, batch 3, Loss 0.10702357441186905\n",
      "Validation: Epoch 92, batch 4, Loss 0.2506098449230194\n",
      "per epoch validation loss (batch average): 0.2835275009274483\n",
      "0\n",
      "Training: Epoch 93, batch 0, Loss 0.4239634573459625\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "per epoch training loss (batch average): 0.5072000741958618\n",
      "Validation: Epoch 93, batch 0, Loss 0.716434895992279\n",
      "Validation: Epoch 93, batch 1, Loss 0.45328328013420105\n",
      "Validation: Epoch 93, batch 2, Loss 0.547081470489502\n",
      "Validation: Epoch 93, batch 3, Loss 0.36519238352775574\n",
      "Validation: Epoch 93, batch 4, Loss 0.2690387964248657\n",
      "per epoch validation loss (batch average): 0.4702061653137207\n",
      "0\n",
      "Training: Epoch 94, batch 0, Loss 0.45843058824539185\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "per epoch training loss (batch average): 0.5327169716358184\n",
      "Validation: Epoch 94, batch 0, Loss 0.434741735458374\n",
      "Validation: Epoch 94, batch 1, Loss 0.5421261787414551\n",
      "Validation: Epoch 94, batch 2, Loss 0.26175010204315186\n",
      "Validation: Epoch 94, batch 3, Loss 0.3247373700141907\n",
      "Validation: Epoch 94, batch 4, Loss 0.5619968771934509\n",
      "per epoch validation loss (batch average): 0.4250704526901245\n",
      "0\n",
      "Training: Epoch 95, batch 0, Loss 0.4982318580150604\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "per epoch training loss (batch average): 0.4053038120269775\n",
      "Validation: Epoch 95, batch 0, Loss 0.5351071953773499\n",
      "Validation: Epoch 95, batch 1, Loss 0.48201751708984375\n",
      "Validation: Epoch 95, batch 2, Loss 0.38221415877342224\n",
      "Validation: Epoch 95, batch 3, Loss 0.5249993205070496\n",
      "Validation: Epoch 95, batch 4, Loss 0.6086543798446655\n",
      "per epoch validation loss (batch average): 0.5065985143184661\n",
      "0\n",
      "Training: Epoch 96, batch 0, Loss 0.5163960456848145\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "per epoch training loss (batch average): 0.43782555460929873\n",
      "Validation: Epoch 96, batch 0, Loss 0.27722835540771484\n",
      "Validation: Epoch 96, batch 1, Loss 0.34746822714805603\n",
      "Validation: Epoch 96, batch 2, Loss 0.4943750500679016\n",
      "Validation: Epoch 96, batch 3, Loss 0.3160097301006317\n",
      "Validation: Epoch 96, batch 4, Loss 0.7140259146690369\n",
      "per epoch validation loss (batch average): 0.42982145547866824\n",
      "0\n",
      "Training: Epoch 97, batch 0, Loss 0.445296972990036\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "per epoch training loss (batch average): 0.5162881970405578\n",
      "Validation: Epoch 97, batch 0, Loss 0.2562664747238159\n",
      "Validation: Epoch 97, batch 1, Loss 1.0853806734085083\n",
      "Validation: Epoch 97, batch 2, Loss 0.33072972297668457\n",
      "Validation: Epoch 97, batch 3, Loss 0.4784310460090637\n",
      "Validation: Epoch 97, batch 4, Loss 0.6166713833808899\n",
      "per epoch validation loss (batch average): 0.5534958600997925\n",
      "0\n",
      "Training: Epoch 98, batch 0, Loss 0.316560834646225\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "per epoch training loss (batch average): 0.4023748517036438\n",
      "Validation: Epoch 98, batch 0, Loss 0.48554912209510803\n",
      "Validation: Epoch 98, batch 1, Loss 0.5405682921409607\n",
      "Validation: Epoch 98, batch 2, Loss 0.27444031834602356\n",
      "Validation: Epoch 98, batch 3, Loss 0.4996703267097473\n",
      "Validation: Epoch 98, batch 4, Loss 0.4513164460659027\n",
      "per epoch validation loss (batch average): 0.4503089010715485\n",
      "0\n",
      "Training: Epoch 99, batch 0, Loss 0.3506651520729065\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "per epoch training loss (batch average): 0.43108240962028505\n",
      "Validation: Epoch 99, batch 0, Loss 0.6456707715988159\n",
      "Validation: Epoch 99, batch 1, Loss 0.531877338886261\n",
      "Validation: Epoch 99, batch 2, Loss 0.5171559453010559\n",
      "Validation: Epoch 99, batch 3, Loss 0.5129546523094177\n",
      "Validation: Epoch 99, batch 4, Loss 0.422380268573761\n",
      "per epoch validation loss (batch average): 0.5260077953338623\n"
     ]
    }
   ],
   "source": [
    "def inner_training_loop(model, k, train_loader, optimizer, n_epochs, validation_loader=None, scheduler=None):\n",
    "    training_loss = []\n",
    "    validation_loss = []\n",
    "    for epoch in range(n_epochs):\n",
    "        for i, (X, Y, R) in enumerate(train_loader):\n",
    "            print(i)\n",
    "            X = X.to(model.device)\n",
    "            Y = Y.to(model.device)\n",
    "            R = R.to(model.device)\n",
    "            optimizer.zero_grad()\n",
    "            loss = model.chain[k].loss(X, Y, R, k)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            if scheduler is not None:\n",
    "                scheduler.step()\n",
    "            if i % 10 == 0:\n",
    "                print(f'Training: Epoch {epoch}, batch {i}, Loss {loss.item()}')\n",
    "            training_loss.append((epoch*len(train_loader) + i, loss.item()))\n",
    "        print(len(train_loader))\n",
    "        print(f'per epoch training loss (batch average): {sum([training_loss[epoch*len(train_loader) + x][1] for x in range(len(train_loader))])/len(train_loader)}')\n",
    "        if validation_loader is not None:\n",
    "            for i, (X, Y, R) in enumerate(validation_loader):\n",
    "                with torch.no_grad():\n",
    "                    X = X.to(model.device)\n",
    "                    Y = Y.to(model.device)\n",
    "                    R = R.to(model.device)\n",
    "                    loss = model.chain[k].loss(X, Y, R, k)\n",
    "                    validation_loss.append((epoch*len(validation_loader) + i, loss.item()))\n",
    "                    print(f'Validation: Epoch {epoch}, batch {i}, Loss {loss.item()}')\n",
    "            print(f'per epoch validation loss (batch average): {sum([validation_loss[epoch*len(validation_loader) + x][1] for x in range(len(validation_loader))])/len(validation_loader)}')\n",
    "    return training_loss, validation_loss\n",
    "            \n",
    "\n",
    "def outer_training_loop(model, train_loader, optimizer, n_epochs):\n",
    "    for k in range(model.n_Y_measurements - 1):\n",
    "        for epoch in range(n_epochs):\n",
    "            for i, (X, Y, R) in enumerate(train_loader):\n",
    "                \n",
    "                X = X.to(model.device)\n",
    "                Y = Y.to(model.device)\n",
    "                R = R.to(model.device)\n",
    "                optimizer.zero_grad()\n",
    "                loss = model.loss(X, Y, R, k)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                if i % 10 == 0:\n",
    "                    print(f'Epoch {epoch}, Loss {loss.item()}')\n",
    "\n",
    "optimizer = torch.optim.Adam(NN_chain.parameters(), lr=0.01)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=10000, gamma=1.)\n",
    "train_history, val_history = inner_training_loop(NN_chain, len(NN_chain.chain)-1, train_dataset, optimizer, 100, val_dataset, scheduler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[torch.Size([100, 102, 3]), torch.Size([100, 2, 1]), torch.Size([100, 102, 4])]"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[ x.shape for x in next(iter(val_dataset))]\n",
    "# plot the training and validation loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot the training and validation loss\n",
    "plt.plot([x[0] for x in train_history], [x[1] for x in train_history], label='Training Loss')\n",
    "# running average\n",
    "plt.plot([x[0] for x in train_history], [np.mean([train_history[i][1] for i in range(max(0, x-10), x+1)]) for x in [x[0] for x in train_history]], label='Training Loss (smoothed)', linestyle='--')    \n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "plt.plot([x[0] for x in val_history], [x[1] for x in val_history], label='Validation Loss')\n",
    "# running average\n",
    "plt.plot([x[0] for x in val_history], [np.mean([val_history[i][1] for i in range(max(0, x-10), x+1)]) for x in [x[0] for x in val_history]], label='Validation Loss (smoothed)', linestyle='--')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the history\n",
    "# # make lines thinner\n",
    "# plt.plot([i for i,x in enumerate(NN_chain.chain[-1].history[\"G\"])], [x.item() for x in (NN_chain.chain[-1].history[\"G\"])], label='G', linewidth=0.5)\n",
    "# plt.plot([i for i,x in enumerate(NN_chain.chain[-1].history[\"SI\"])], [x.item() for x in (NN_chain.chain[-1].history[\"SI\"])], label='SI')\n",
    "# plt.plot([i for i,x in enumerate(NN_chain.chain[-1].history[\"NN\"])], [x.item() for x in (NN_chain.chain[-1].history[\"NN\"])], label='NN')\n",
    "# plt.title(\"Sum\")\n",
    "# plt.legend()\n",
    "# plt.show()\n",
    "# # and the standard deviations\n",
    "# plt.plot([i for i,x in enumerate(NN_chain.chain[-1].history[\"Gs\"])], [x.item() for x in (NN_chain.chain[-1].history[\"Gs\"])], label='G', linewidth=0.5)\n",
    "# plt.plot([i for i,x in enumerate(NN_chain.chain[-1].history[\"SIs\"])], [x.item() for x in (NN_chain.chain[-1].history[\"SIs\"])], label='SI')\n",
    "# plt.plot([i for i,x in enumerate(NN_chain.chain[-1].history[\"NNs\"])], [x.item() for x in (NN_chain.chain[-1].history[\"NNs\"])], label='NN')\n",
    "# plt.legend()\n",
    "# plt.title(\"Standard deviations\")\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.1959, 0.1534]], device='cuda:0', grad_fn=<SoftplusBackward0>)"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out = NN_chain.chain[-1].forward(torch.tensor(0.).cuda(), torch.tensor([[0.,1.,0.]]).cuda(), torch.tensor([[[100000.]]]).cuda())\n",
    "out#/out.sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Other"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
